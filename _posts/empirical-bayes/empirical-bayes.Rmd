---
title: "Empirical Bayes"
description: |
  A Belated Follow-up
author:
  - name: Michael Clark
    url: https://m-clark.github.io
date: '`r format(Sys.Date(), "%B %d, %Y")`'
preview: ../../img/198R.png   # apparently no way to change the size displayed via css (ignored) or file (stretched)
output:
  distill::distill_article:
    self_contained: false
    toc: true
    css: ../../styles.css
draft: true
tags: [bayesian, empirical bayes, shrinkage, random effects, mixed models]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo=T, 
  message = F, 
  warning = F, 
  comment = NA,
  R.options=list(width=120), 
  cache.rebuild=F, 
  cache=T,
  fig.align='center', 
  fig.asp = .7,
  dev = 'svg', 
  dev.args=list(bg = 'transparent')
)

library(tidyverse); library(broom); library(kableExtra); library(visibly)

kable_df <- function(..., digits=3) {
  kable(..., digits=digits) %>% 
    kable_styling(full_width = F)
}

rnd = function(x, digits = 3) arm::fround(x, digits = digits)
```

## Introduction

A couple of folks I work with in different capacities independently came across an article by Data Camp's David Robinson [demonstrating <span class="emph">empirical bayes</span>](http://varianceexplained.org/r/empirical_bayes_baseball/).  It provides a nice simple example of how to create a prior from the observed data, allowing it to induce shrinkage in estimates, in this case career batting averages.  This would better allow one to compare someone that had only a relatively few at-bats to those that had longer careers.

It is a very simple and straightforward demo, and admits that it doesn't account for many other things that could be brought into the model, but that's also why it's effective at demonstrating the technique.  However, shrinkage of parameter estimates can be accomplished in other ways, so I thought I'd compare it to two of my preferred ways to do so - a fully Bayesian approach and a random effects/mixed-model approach.  

I demonstrate shrinkage in mixed models in more detail [here](http://m-clark.github.io/posts/2019-05-14-shrinkage-in-mixed-models/) and [here](https://m-clark.github.io/mixed-models-with-R/), and I'm not going to explain Bayesian analysis in general, [but see my doc on it](https://m-clark.github.io/bayesian-basics/).  This post is just for a quick  comparison of techniques.

## Data Setup

We'll start as we always do, with the data. The following just duplicates David's code from the article.  Nothing new here.

```{r data_setup}
library(dplyr)
library(tidyr)
library(Lahman)

career <- Batting %>%
  filter(AB > 0) %>%
  anti_join(Pitching, by = "playerID") %>%
  group_by(playerID) %>%
  summarize(H = sum(H), AB = sum(AB)) %>%
  mutate(average = H / AB)

# use names along with the player IDs
career <- Master %>%
  tbl_df() %>%
  select(playerID, nameFirst, nameLast) %>%
  unite(name, nameFirst, nameLast, sep = " ") %>%
  inner_join(career, by = "playerID") 

career_filtered <- career %>%
  filter(AB >= 500)

m <- MASS::fitdistr(career_filtered$average, 
                    dbeta,
                    start = list(shape1 = 1, shape2 = 10))

alpha0 <- m$estimate[1]
beta0 <- m$estimate[2]

career_eb <- career %>%
  mutate(eb_estimate = (H + alpha0) / (AB + alpha0 + beta0))

career_eb
```

## Models

As mentioned, I will compare the empirical bayes results to those of a couple of approaches.  They are:

- Bayesian mixed model on full data (using <span class="pack">brms</span>)
- standard mixed model on full data (using <span class="pack">lme4</span>)
- Bayesian mixed model on filtered data (at bats greater than 500)
- standard mixed model on filtered data


The advantages to these are that using a fully Bayesian approach allows us to not approximate the Bayesian and just do it. The standard mixed model essentially does the same thing with a penalized regression approach which also approximates the Bayesian, but doesn't require any double dipping of the data to get at a prior.  In both cases, we can accomplish the desired model with just a standard R modeling approach.

In this case the model is a standard binomial model for counts.  In base R <span class="func">glm</span>, we would do something like the following.

```{r glm_binomial, eval=FALSE}
glm(cbind(H, AB-H) ~ ..., data = career_eb, family = binomial)
```

The model is actually for the count of successes out of the total, which R has always oddly done in <span class="func">glm</span> as `cbind(success, not success)` rather than the more intuitive route (my opinion).  The brms package will make it more obvious, but glmer uses this same glm approach.  The key difference for both is that we add a per-observation random effect for `playerID`[^perobs].


## Full Bayes

We'll start with the full Bayesian approach using brms.  This model will struggle a bit[^problem_chain], and takes a while to run, as it's estimating `r nrow(career_eb) + 1` parameters.  But in the end we get what we want.

```{r bayes_full}
# in case anyone wants to use rstanarm I show it here
# library(rstanarm)
# bayes_full = stan_glmer(cbind(H, AB-H) ~ 1 + (1|playerID),
#                         data = career_eb,
#                         family = binomial)

library(brms)
bayes_full = brm(H|trials(AB) ~ 1 + (1|playerID), 
                 data = career_eb,
                 family = binomial,
                 seed = 1234,
                 iter = 1000,
                 cores = 4)
```

We can see right off the bat[^pun] that this approach estimates the data fairly well. Our posterior predictive distribution for the number of hits is hardly distinguishable from the observed data. 

```{r bayes_inspect, eval=FALSE}
pp_check(bayes_full)
```

Again, the binomial model is for counts, in this case, the number of hits. But if we wanted proportions, which in this case are the batting averages, we could just divide this result by the AB (at bats) column.  Here we can see a little more nuance, especially that the model shies away from the lower values more, but this would be a fantastic fit by any standards.

```{r bayes_inspect_average, echo=FALSE}
pp = pp_check(bayes_full)$data

pp = pp %>% 
  arrange(rep_id) %>% 
  mutate(avg = value/career_eb$AB) 

pp %>% 
  ggplot(aes(x = avg)) +
  geom_density(
    size = 1,
    color = 'gray50',
    data = pp %>% filter(is_y)) +
  geom_density(
    aes(group = rep_label),
    color = alpha('#ff5500', .25),
    size = .25, 
    data = pp %>% filter(!is_y)) + 
  theme_trueMinimal() + 
  theme(axis.text.y = element_blank(),
        axis.title.y = element_blank())
```


## Full Mixed

The lme4 model takes the glm approach as far as syntax goes `cbind(successes, non-successes)`.  Very straightforward, and fast, as it doesn't actually estimate the random effects, but instead *predicts* them.  The predictions are in fact akin to empirical bayes estimates[^ebblups].

```{r glmer_full}
glmer_full = lme4::glmer(cbind(H, AB-H) ~ 1 + (1|playerID), 
                         data = career_eb,
                         family = binomial)
```

## Filtered models

Since David's original 'prior' was based only on observations for those who had at least 500+ at bats (essentially a full season), the following just re-runs the models for just the filtered data set to see how those comparisons turn out.


```{r filtered_models}
bayes_filtered = brm(H|trials(AB) ~ 1 + (1|playerID), 
                     data = career_eb %>% filter(AB >= 500),
                     family = binomial,
                     iter = 1000,
                     seed = 1234,
                     cores = 4)

glmer_filtered = lme4::glmer(cbind(H, AB-H) ~ 1 + (1|playerID), 
                             data = career_eb %>% filter(AB >= 500),
                             family = binomial)
```



## Prediction comparison

Now we're ready to make some comparisons. We'll combine the fits from the models to the original data set.

```{r add_predictions}
career_other = career_eb %>% 
  mutate(
    bayes_estimate = fitted(bayes_full)[,1] / AB,
    glmer_estimate = fitted(glmer_full),
  )

career_other_filtered = career_filtered %>% 
  mutate(
    bayes_filtered_estimate = fitted(bayes_filtered)[,1] / AB,
    glmer_filtered_estimate = fitted(glmer_filtered),
  ) %>% 
  select(playerID, contains('filter'))

career_all = left_join(career_other, 
                       career_other_filtered)

```

```{r career_all_predictions, echo=FALSE}
career_all %>% 
  mutate_if(is.numeric, round, digits=3) %>% 
  DT::datatable(rownames = F, options = list(dom = 'ftp', scrollX = T))
```

We can see that the fully Bayesian an mixed models are essentially giving us the same values.  We start to see slight differences with the EB estimates, especially for those with fewer at-bats.  When there is less data, the EB estimates more sharply pull to the prior.

### Top and bottom predictions

If we just look at the top 10, we would not come to any different conclusions (only full data models shown).

```{r compare_top_predictions}
top_10_eb = career_all %>% 
  top_n(10, eb_estimate) %>% 
  select(playerID, eb_estimate)

top_10_bayes = career_all %>% 
  top_n(10, bayes_estimate) %>% 
  select(playerID, bayes_estimate)

top_10_mixed = career_all %>% 
  top_n(10, glmer_estimate) %>% 
  select(playerID, glmer_estimate)
```

```{r compare_top_predictions_table, echo=FALSE}
bind_cols(top_10_eb, top_10_bayes, top_10_mixed) %>% 
  kable_df()
```


Same for the bottom 10, although we see a little more waverying on the fitted values, as some of these are the ones who relatively fewer at bats, and would see more shrinkage as a result.

```{r compare_bot_predictions}
bottom_10_eb = career_all %>% 
  top_n(-10, eb_estimate) %>% 
  select(playerID, eb_estimate)

bottom_10_bayes = career_all %>% 
  top_n(-10, bayes_estimate) %>% 
  select(playerID, bayes_estimate)

bottom_10_mixed = career_all %>% 
  top_n(-10, glmer_estimate) %>% 
  select(playerID, glmer_estimate)
```

```{r compare_bot_predictions_table, echo=FALSE}
bind_cols(bottom_10_eb, bottom_10_bayes, bottom_10_mixed) %>% 
  kable_df()
```

### Extreme predictions

Now let's look at some more extreme predictions.  Those who averaged 0 or 1 for their lifetime batting average.  Note that none of these will have very many plate appearances, and will show the greatest shrinkage.  As a reminder, the filtered models did not include any of these individuals.

```{r other_comparisons_1avg, echo=FALSE}
career_all %>% 
  filter(average == 1) %>% 
  arrange(desc(AB)) %>% 
  mutate_if(is.numeric, round, digits=3) %>% 
  DT::datatable(., rownames = F, options = list(dom = 'ftp', scrollX = T))
```


```{r other_comparisons_0avg, echo=FALSE}
career_all %>% 
  filter(average == 0) %>% 
  arrange(desc(AB)) %>% 
  mutate_if(is.numeric, round, digits=3) %>% 
  DT::datatable(rownames = F, options = list(dom = 'ftp', scrollX = T))
```




## Visualize effects

The following reproduces David's plots.  I start with his [original image](http://varianceexplained.org/figs/2015-10-01-empirical_bayes_baseball/unnamed-chunk-11-1.png), altered only to be consistent with my visualization choices that use different color choices, add transparency, and allow size to reflect the number of at bats. Here is his explanation:

>The horizontal dashed red line marks $y=\alpha_0/\alpha_0+\beta_0=0.259$
- that’s what we would guess someone’s batting average was if we had no evidence at all. Notice that points above that line tend to move down towards it, while points below it move up.
The diagonal red line marks $x=y$. Points that lie close to it are the ones that didn’t get shrunk at all by empirical Bayes. Notice that they’re the ones with the highest number of at-bats (the brightest blue): they have enough evidence that we’re willing to believe the naive batting average estimate.

```{r robinson_plot_original, echo=FALSE}
rob_plot_theme =   
  theme(
    axis.title.y = element_text(angle = 0, size=8),
    axis.title.x = element_text(angle = 0, size=8),
    axis.text = element_text(size = 6),
    legend.key.size = unit(10, 'points'),
    legend.text =  element_text(size=8),
    legend.title =  element_text(size=8)
)

career_all %>% 
  ggplot(aes(x = average, y = eb_estimate)) +
  geom_hline(yintercept = mean(career_filtered$average), 
             color = '#ff5500',
             alpha = .5,
             size = 1) +
  geom_abline(color = '#00aaff', 
              alpha = .5,
              size = 1) +
  geom_point(aes(fill=AB, size=AB), color = 'gray50', pch=21, alpha=.1) +
  labs(y = 'EB Estimate', x='Batting Average') +
  scico::scale_fill_scico(direction = 1) + 
  scale_size_continuous(range = c(1, 4)) +
  scale_x_continuous(breaks = seq(0, 1, by=.1)) +
  scale_y_continuous(breaks = seq(.20, .35, by=.025)) +
  guides(color='none', fill = 'none', alpha = 'none') +
  visibly::theme_trueMinimal() +
  rob_plot_theme
```

Again, this is the same plot, but the size shows more clearly how observations don't exhibit as much shrinkage when there is enough information.


Here is the same plot against the full bayes estimates.  The original lines are kept, but I add lines representing the average of the whole data, and the intercept from the Bayesian analysis (which is essentially the same as with the mixed model).

```{r robinson_plot, echo=FALSE}
library(ggplot2)
averages = data.frame(
  average = c('filtered/eb', 'full data', 'bayes'),
  avg = c(mean(career_filtered$average),
          mean(career_eb$average),
          plogis(fixef(bayes_full)[1]))
)

library(ggnewscale)

career_all %>% 
  ggplot(aes(x = average, y = bayes_estimate)) +
  geom_hline(aes(yintercept = avg, color = average),
             data = averages %>% 
               mutate(average = fct_inorder(factor(average))),
             size = .5, 
             show.legend = F) +
  geom_text(aes(x = 1, y = c(avg[1]+.01, avg[2:3]-.0025), color = average, label=average), 
            hjust = .75,
            vjust = 1,
            size = 3,
            data = averages, 
            show.legend = F) + 
  scico::scale_color_scico_d(direction = -1, begin=.25, end=.75) +
  new_scale_color() +
  geom_abline(color = '#00aaff', alpha = .25, size = 1) +
  geom_point(aes(fill=AB, size=AB), color = 'gray50', pch=21, alpha=.1) +
  guides(size = F) + 
  labs(y = 'Full Bayes', x='Batting Average') +
  scico::scale_fill_scico(direction = 1) + 
  scale_size_continuous(range = c(1, 4)) +
  scale_x_continuous(breaks = seq(0, 1, by=.1)) +
  scale_y_continuous(breaks = seq(.20, .35, by=.025)) +
  visibly::theme_trueMinimal() +
  rob_plot_theme


# career_all %>% 
#   select(playerID, contains('estimate')) %>% 
#   gather(key = type, value = estimate, -playerID, -eb_estimate) %>% 
#   mutate(type = fct_inorder(type)) %>%   # to defy ggplot
#   drop_na() %>%                          # to get rid of warning
#   ggplot(aes(x = eb_estimate, y = estimate)) +
#   # geom_smooth(color = '#ff5500', se = F, size = .5, method = 'loess') +
#   geom_point(color = '#00aaff', alpha=.005, size = .5) +
#   facet_wrap(~type, ncol = 2) + 
#   theme_minimal()

```


```{r density_plot, echo=FALSE}
career_all %>% 
  select(contains('estimate')) %>% 
  gather(key = type, value = estimate) %>% 
  mutate(type = fct_inorder(type)) %>%   # to defy ggplot
  drop_na() %>%                          # to get rid of warning
  ggplot() +
  geom_density(aes(x = estimate, color = type)) +
  labs(y = '') +
  theme_minimal() +
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank())
```


```{r misc, eval=FALSE, echo=FALSE}
career_all %>% 
  select(contains('estimate')) %>% 
  cor(use='pair') %>% 
  round(2)
  
  
make_stancode(H|trials(AB) ~ 1 + (1|playerID), 
                  data = career,
                  family = binomial)

career_eb %>% pivot_longer
```

[^perobs]: This is possible for mixed models for counts like binomial and poisson (and other distributions) where we don't estimate the residual variance.  This allows us to deal with overdispersion in this model via the random effect variance.  See the [GLMM FAQ](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#fitting-models-with-overdispersion).

[^problem_chain]: One chain always struggled a bit with the brms defaults, but diagnostics were okay.

[^ebblups]: See [Bates' comment](https://stat.ethz.ch/pipermail/r-sig-mixed-models/2009q4/002984.html).

[^pun]: Not sorry!