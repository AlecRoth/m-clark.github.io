---
title: "Bayesian Analysis"
description: |
  Steps you can take in your Stan journey.
author:
  - name: Michael Clark
    url: https://m-clark.github.io
date: '`r format(Sys.Date(), "%B %d, %Y")`'
preview: ../../img/r_and_stan.png   # apparently no way to change the size displayed via css (ignored) or file (stretched)
output:
  distill::distill_article:
    self_contained: false
    toc: true
    css: [../../styles.css, 'https://use.fontawesome.com/releases/v5.14.0/css/all.css']
draft: true
tags: [tags, taggy]
categories:
  - bayesian
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo      = T, 
  eval      = F, 
  message   = F, 
  warning   = F, 
  comment   = NA,
  R.options = list(width = 120),
  cache.rebuild = F,
  cache = T,
  fig.align = 'center',
  fig.asp = .7,
  dev = 'svg',
  dev.args = list(bg = 'transparent')
)

library(tidyverse)
library(broom)
library(kableExtra)
library(visibly)

kable_df <- function(..., digits=3) {
  kable(..., digits=digits) %>% 
    kable_styling(full_width = F)
}

rnd = tidyext::rnd
```




## Overview

```{r rstan-img, echo=FALSE, out.width='50%', eval=F}
knitr::include_graphics('img/r_and_stan.png')  # this doesn't work without creating an img dir in the post dir
```

<img src='../../img/r_and_stan.png' style="display:block; margin: 0 auto; width: 50%">

<br>

Bayesian analysis takes some getting used to, but offers great advantages once you get into it.  While it can be difficult to get started, it typically should not take much to repeat an analysis one is already familiar with, say a standard regression with some (common) additional complexity like a binary outcome, interactions, random effects, etc.  What's great is that *Stan*, a programming language for Bayesian analysis, has come a long way and provides a means for (relatively) fast computation.  What's more, applied analysts do not need to know the Stan programming language to do common and even notably complicated models. Packages like rstanarm and brms, coupled with additional tools like bayesplot, tidybayes, and more, make getting and exploring results even easier than the R packages one already uses.

One of the advantages of doing Bayesian analysis with these tools is that there are many ways to diagnose model issues, problems, and failures.  This is great! Traditional analysis can often be notably more difficult in this regard, or at least typically requires more hands-on approaches, and often there can be serious model deficiencies without much notification or, if there is, there might be little one can do about it. On the other hand, current Bayesian packages are providing diagnostic information as a default part of results, have ready visualizations to explore issues, and more.



## The Stated Problem


<div style = 'text-align: center'>
<i class="fas fa-exclamation-triangle fa-5x" style="padding: 20px"></i>
</div>


This is all great in general.  However, in consulting I see a consistent issue.  Someone has a standard model, e.g. a basic regression or mixed model, but is getting problematic warnings, messages, or errors.  When they look up how to solve this problem, the documentation, forums, and other outlets, while great resources for the initiated, generally overwhelm newer and/or more applied users, as they are too technical, assume a great deal of background knowledge, are even underexplained (especially in the case of interpreting visualizations), and even suggest things that aren't typically possible (e.g. getting better data/model).  What seems straightforward to an advanced user or developer may not be even close to that for most applied analysts, and many of them (again, my experience) are left a bit deflated by the whole process.

In the beginning of Stan's ascension, the majority of people using Stan/rstan were more technically inclined, willing to code Stan directly, and, when problems arose, they were willing to do a lot of extra work to solve the problems (reading articles, developing pseudo-expertise with various techniques, and more). But tools like rstanarm and brms make running Bayesian models as easy to do as using base R functions and other non-Bayesian packages, and as Bayesian analysis has become more widely used, accepted, and presented, this has opened the Bayesian approach to practically anyone that wants to do it.

I personally don't think one needs to be an expert in Bayesian analysis to enjoy its benefits for common models, even if difficulties come up.  Nor do I think an exhaustive search through forums, articles, and more are necessary to have to do for issues that arise often.  So this is an attempt at a one-stop shop for dealing with the most common issues that arise, interpreting results of diagnostics, and summarizing the options one can take.


## Audience

<div style = 'text-align: center'>
<i class="fas fa-users fa-5x" style="padding: 20px"></i><i class="fas fa-users fa-5x"></i><i class="fas fa-users fa-5x"></i>
</div>

Here is the presumed audience for this post:

- Wants to bayes
- Is not going to code anything in Stan directly, nor knows how to
- Is not a statistician, nor desires technical insights about Bayesian analysis (at least not yet!)
- Is already having to learn a new tool/functions/possibly a whole system of inference
- Has probably never used the control argument for any model 


While you can do a Bayesian analysis just for the heck of it, you really need to understand a few key ideas to take advantage of what it offers. Some things you do need to know in order to use it on a basic level:

- The distributions: priors, likelihood, posterior, posterior predictive
- Iterative sampling to estimate  parameters. Even a cursory understanding of maximum likelihood would probably be enough.

You can obtain this basic info from my document [Bayesian Basics](m-clark.github.io/bayesian-basics/).



## Outline


Here is what we're going to do:

- Discuss the most common problems
- Provide quick solutions that should work most of the time
- Provide a practical approach for future endeavors




## Installation issues

Your first hurdle is installation, so let's start there.  Applied users will use rstanarm, brms, or other higher level interfaces to the Stan programming language. These tools use Stan, but Stan itself requires compilation to C++. This means that to run a basic regression with brms, you'll likely be depending on multiple languages and packages for it to work.

$\rightarrow$ package with basic R code  (brms, rstanarm)

$\rightarrow$ Translated to Stan (or use rstan with Stan code)
  
$\rightarrow$ Compiled to C++  (requires compiler)
  
  
In general, installing the package you want to use will install the appropriate dependencies, and that should be enough.  In some cases it may not be.


### Mac

Mac's Catalina OS has been problematic to say the least.  Since its release, there was a  period of time where Stan wasn't viable for many users without extensive workarounds, and even now, issues seemingly still arise with every update to Catalina. This is not specific to Stan, or R, by the way[^mac].  Then R 4.0 came along and helped some things, but also resulted in new problems. That said, the Stan community have been excellent at helping people here. My luck of late has been better, but I just assume at this point that if I update Catalina or update the packages, I may have to work through some problem.

Here are some discussions on the Stan forums for getting through installation problems.  Unfortunately a couple of these threads are now nearly a year old, and weren't closed.  This means that anything prior to R 4.0 can be ignored, and you probably only want to skip towards the end of those discussions for the relevant parts.

- [Dealing with Catalina I](https://discourse.mc-stan.org/t/dealing-with-catalina/11285/) First post Oct 2019
- [Dealing with Catalina II](https://discourse.mc-stan.org/t/dealing-with-catalina-ii/11802/) First post Nov 2019
- [Dealing with Catalina III](https://discourse.mc-stan.org/t/dealing-with-catalina-iii/12731/) First post Jan 2020
- [Dealing with Catalina IV](https://discourse.mc-stan.org/t/dealing-with-catalina-iv/13502/) First post Mar 2020


### Windows

I rarely had installation issues on Win 7 or Win 8 or Win 10, though haven't had to use it lately.  It does require [rtools](https://cran.r-project.org/bin/windows/Rtools/) to be installed, which is good to install for R with Windows anyway.  Again though, if you have issues, the Stan community will be very helpful.

[Search Windows Issues on Stan Forums](https://discourse.mc-stan.org/search?q=windows)




### Linux

I've only used Stan with Linux in a cluster computing environment. Applied users also need cluster computing on occasion, it's just that the problems may require IT support in that case.  I generally have not had issues, but it's not something I've done recently.  Also, if you're using Linux, you're probably an advanced user of these tools in general, and may not be the audience this post is mostly geared toward.  

[Search Linux Issues on Stan Forums](https://discourse.mc-stan.org/search?q=Linux)


### Other programming languages besides R

If you're using Stan with Python, Stata, Julia, etc., then you're coding Stan directly, and you're likely very familiar with the forums and dealing with a variety of issues.  That's not to say that you won't find something useful here, it's just that I have nothing to offer you regarding those platforms specifically.


## Example data


<div style = 'text-align: center'>
<i class="fas fa-database fa-5x" style="padding: 20px"></i>
</div>

<br>

To start out, I'm going to create some data for us to run some basic models with. To make things interesting, the true underlying model has categorical and continuous covariates, interactions, nonlinear relationships, random effects (observations are clustered in groups), and some variables are collinear.  You can skip these details if uninterested, but note that we will be purposely using under- and over-fitted models relative to this one to see what happens.

```{r create-data-setup}
library(tidyverse)

create_data <- function( N = 1000, ng = 100, seed = 1234) {
  
  set.seed(seed)
  
  X_mm = cbind(
    # a standard binary
    binary_1 = sample(0:1, N, replace = TRUE),                      
    # a relatively rare categorical
    binary_2 = sample(0:1, N, replace = TRUE, prob = c(.05, .95)),  
    # two partly collinear numeric
    mvtnorm::rmvnorm(N,
                     mean = rep(0, 3),
                     sigma = lazerhawk::create_corr(runif(3, max = .6)))
  )
  
  X_mm = cbind(
    # intercept
    1,
    X_mm,
    # a quadratic effect
    scale(poly(X_mm[,5], 3))[,2:3],
    # interaction of binary variables
    X_mm[,1]*X_mm[,2], 
    # interaction of binary 2 with numeric 1
    X_mm[,2]*X_mm[,3]
  )
  
  # add intercept
  colnames(X_mm) = c(
    'Intercept',
    'b1',
    'b2',
    'x1',
    'x2',
    'x3',
    'x3_sq',
    'x3_cub',
    'b1_b2',
    'b2_x1'
  )
  
  # coefficients
  beta = c(
    3.0,   # intercept
     .3,   # b1
    -.3,   # b2
     .5,   # x1
     .0,   # x2
     .3 ,  # x3 
     .3,   # x3_sq
    -.2,   # x3_cub
     .5,   # b1_b2 
    -.5    # b2_x1
  )
  
  # create target variable/linear predictor
  y = X_mm %*% beta
  
  # add random effect
  groups = sort(sample(1:ng, N, replace = T))
  
  re = rnorm(ng, sd = .5)[groups]  # re sd = .5
  
  # add re and residual noise
  y = y + re + rnorm(N)
  y = cbind(y, groups)
  colnames(y) = c('y', 'group')
  
  as_tibble(cbind(X_mm, y))
}


```

```{r save-true-parameters}
true_params = data.frame(
  parameter = c(
    'intercept',
    'b1',
    'b2',
    'x1',
    'x2',
    'x3 ',
    'x3_sq',
    'x3_cub',
    'b1_b2 ',
    'b2_x1',
    're sd',
    'sigma '
  ), 
    value = c(
    3.0,
     .3,
    -.3,
     .5,
     .0,
     .3,
     .3,
    -.2,
     .5,
    -.5,
     .5,
    1.0
  )
)
```


If you want to check that  the parameters are recovered, run the following.

```{r create-data-test, eval=FALSE}
dat = create_data(N = 10000)

mod = lme4::lmer(y ~ . -group + (1|group), data.frame(dat[,-1]))

mixedup::summarise_model(mod, ci = FALSE)
```

We do not need as much data for our purposes so we'll set the total sample size to 1000.  We'll also make our binary covariates explicit factors, which will make things easier when want to visualize grouped effects later.

```{r create-data}
# create the primary data frame

main_df = 
  create_data(N = 1000) %>% 
  as_tibble() %>% 
  select(group, b1:x3, y) %>% 
  mutate(
    b1 = factor(b1),   # will help with visuals
    b2 = factor(b2)
  )
```





## Basic Steps for Practical Modeling

<div style = 'text-align: center'>
<i class="fa fa-shoe-prints fa-5x" style="padding: 20px"></i>
</div>

<br>


Once data is in hand there are basic steps for practical modeling with Stan tools.


##### Use standard/default priors for the model

- For common regression models, normal or student t for the (fixed effect) coefficients. You will have to set this!
- (half) student-t for variances. Defaults are usually fine.
- Otherwise, look at the [recommendations](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations)

##### Build models in increasing complexity

- Never start with the 'final' model, if the fitting has issues with simpler models, things will only get worse with more complex models.
- Getting model settings squared away earlier (e.g. setting number of iterations, other options) will save time later.

##### Examine convergence via Rhat, ESS, visualization

- If you are okay here with no warnings or messages, you are okay to proceed for practical purposes. Any remaining issues, e.g. poor prediction, are likely to not have obvious solutions other than things like getting better data, revising theory, etc.

##### Examine fit visually  

- <span class="func" style = "">pp_check</span>, tidybayes, etc.

##### Compare models using loo, posterior probabilities of models

- For technical reasons, avoid using bayes factors

Assuming you have no problems in the above process, you have more or less fulfilled the basic requirements to do more standard analyses in Bayesian form.  Great! 



## Warnings, What They Are, and What to do About Them

Of course, if it was always that easy, we wouldn't be posting this. There are a few warnings that you're bound to come across at some point in modeling with the Stan ecosystem.  We'll cover these, as well as the most common solutions.  

Primary reference: [Brief Guide to Stan’s Warnings](https://mc-stan.org/misc/warnings.html)


### Rhat & Effective Sample Size

We will start with a simple standard regression model that we know is not adequate.  We will use default priors and run very few iterations. 

```{r model-start-100, message=TRUE}
# no priors, no complexity, all default settings, few iterations
library(brms)

model_start_100 = brm(
  y ~ b1 + b2 + x1 + x2, 
  data = main_df,
  iter = 100
)

summary(model_start_100)
```


The warnings about Rhat and effective sample size (ESS) are likely to pop up if you use default iterations for more complex models.  They mostly regard the efficiency of the sampling process, and whether you have enough samples to have stable parameter estimates.  The fix for these warnings is usually simple, just let the model run for more iterations.  The default is 2000 iterations with warmup half of that.  Warmup iterations are not used in calculation of parameter estimates, so you can just adjust iterations relative to the warmup.

As an example we can look at our short run model. If we plot the estimated values across each iteration for each chain, we can see that the chains could be mixing better, but only if you are used to looking at these things.  


```{r model-start-x1}
color_scheme_set(scico::scico(6, palette = 'batlow', begin = .1, end = .9, direction = -1))
# plot(model_start_100, par = 'x1') # examine specific parameters
mcmc_plot(model_start_100, type = 'combo')
mcmc_plot(model_start_100, type = 'areas')
```

To see what you would expect, just plot a series from a normal distribution.

```{r rnorm-trace-plot}
ggplot2::qplot(x = 1:1000,
               y = rnorm(1000),
               geom = 'line')
```

While things seem okay, what happens if we single out a particular chain, we may see otherwise.  In this case, the intercept and b2 coefficients may be problematic.

```{r trace-highlight}
mcmc_plot(model_start_100, highlight = 1, type = 'trace_highlight')
```



Some suggest to look at rank plots instead of traditional traceplots.  Really, all we've changed is looking for something 'fuzzy' to looking for 'approximately uniform', so my opinion is that it's not much of an improvement visually or intuitively.  In general, histograms, which are variants of bar charts, are rarely an improvement on any visualization I've come across.  If you do use it, you can use an overlay approach to see if the ranks are mixing, but this looks a lot like what I'd be looking for from a traceplot.

```{r rank-plots}
mcmc_plot(model_start_100, type = 'rank_hist')
mcmc_plot(model_start_100, type = 'rank_overlay')
```

#### Some technical details


##### Rhat

The $\hat{R}$ statistic measures  the ratio of the average variance of samples within each chain to the variance of the pooled samples across chains; if all chains are at equilibrium, these will be the same and Rhat will be one. If the chains have not converged to a common distribution, the Rhat statistic will be greater than one.

##### ESS

Effective sample size is an estimate of the effective number of independent draws from the posterior distribution of the estimand of interest. Because the draws within a chain are not independent if there is autocorrelation, the effective sample size will be smaller than the total number of iterations.

  - *Bulk ESS*: ESS for the mean
  - *Tail ESS*: ESS for the 5% and 95% quantiles. Tail-ESS can help diagnose problems due to different scales of the chains and slow mixing in the tails.
    
##### Trace plot

Shows the estimated parameter values at each iteration.  In general you would like a random bouncing around an average value.

##### Rank plot

From bayesplot helpfile: 

>Whereas traditional trace plots visualize how the chains mix over the course of sampling, rank histograms visualize how the values from the chains mix together in terms of ranking. An ideal plot would show the rankings mixing or overlapping in a uniform distribution.  

See Vehtari et al. (2019) for details.

##### ACF plot

The acf, or autocorrelation function plot, is exactly the same thing you'd visualize for any time series.  It is a plot of a series correlation with specific lags of itself.  Autocorrelation does not bias estimates, but increased autocorrelation may be more inefficient/slower. At lag zero, the series estimates are perfectly correlated with themselves, so that's where the plot usually starts.

##### Efficiency plots

I have seen these often been recommended but without an accompanying package function that actually does it.  I assume they mean something like the following, but honestly one shouldn't have to search forums and read the documentation of multiple functions to find this. Some just show you what you already saw in the summary, so don't really help. For the first two plots, darker is worse.

```{r eff-plots}
mcmc_plot(model_start_100, type = 'rhat')  # > 1.05 problematic
mcmc_plot(model_start_100, type = 'neff')  # < .1 problem
mcmc_plot(model_start_100, type = 'acf')
```

#### Solution for Rhat/ESS warnings

The solution to Rhat and ESS warnings is simply to do more iterations. To keep posterior samples and model objects from becoming unwieldy in size[^bigobjects], consider thinning also.

```{r increase-iter, eval=F}
brm(model)  # default with 4 chains, 1000 warmup 2000 total = 4*(2000 - 1000) = 4000 post warmup draws
brm(model, warmup = 2000, iter = 4000)  # 4*(4000 - 2000) = 8000 posterior draws
brm(model, warmup = 2000, iter = 4000, thin = 8)  # 4*(4000 - 2000)/8 = 1000 posterior draws
```



### BFMI low

You may see a warning that says some number of chains had an estimated Bayesian Fraction of Missing Information (BFMI) that was too low. This implies that the adaptation phase of the Markov Chains did not turn out well, and those chains likely did not explore the posterior distribution efficiently. For more details on this diagnostic, you can see [Betancourt's article](https://arxiv.org/abs/1604.00695), but this will almost surely be too technical for many applied and even more advanced users.

In this case, the problem here is usually remedied by just adding more iterations as before. I typically keep to 1000 posterior samples, which makes for nicer visualizations of distributions without creating relatively large model objects.

```{r model-start-checks}
model_start = update(model_start_100, warmup = 1750, iter = 2000) # 4 * (2000 - 1750) = 1000 posterior draws
summary(model_start)
plot(model_start, par = 'x1')
```


#### Solution for low BFMI

If there aren't other serious problems, add more iterations.  In some cases, switching from a heavy-tailed prior (e.g. student t) to normal would be helpful. Otherwise, you will approach it similarly to the problem of [divergent transitions][divergent transitions].


### Tree Depth

Tree depth is a more technical warning that has to do with the details of Hamiltonian Monte Carlo. Practically speaking:

> Lack of convergence and hitting the maximum number of leapfrog steps (equivalently maximum tree depth) are indicative of improper posteriors ~ Stan User Guide

Sometimes you'll get a warning about hitting maximum tree depth, and without getting overly technical, the fix is easy enough.  Just set the maximum higher.




#### Solution for max tree depth

Use the control argument to increase the value beyond 10.

```{r tree-depth, eval=FALSE}
model_start = update(model_start_100, warmup = 1750, iter = 2000, control = list(max_tree_depth = 15))
```


### Divergent Transitions

*Divergent transitions*  are a technical issue that indicates something may be notably wrong with the data or model ([technical details](https://mc-stan.org/docs/2_24/reference-manual/divergent-transitions.html)). They indicate that the sampling process has 'gone off the rails' and the iteration's results and anything based on them (i.e. subsequent draws, parameter estimates) can't be trusted.  

Why might this happen?

- insufficient data for the model's complexity
- poor model
- high collinearity
- improper priors
- separability (logistic regression)


As an example, I'll make a relatively complex model with only a small random sample of the data, and use very few warmups.

```{r divergent-model}
problem_model = brm(
  y ~ b1*b2*x1 + x2*b2 + x3*b2 + (1 + x1 + b2|group),
  data   = main_df %>% slice_sample(prop = .1),
  cores  = 4,
  warmup = 10,
  iter   = 1010,
  thin   = 4,
  seed   = 12
)

problem_model
```

So what do we do in this case? Well let's start with visual inspection.


#### Example: Pairs plot

A  diagnostic tool that is typically suggested to look at with divergent transitions is the <span class="emph" style = "">pairs plot</span>.  It is just a scatterplot matrix of the parameters estimates (and log posterior value), but it suffers from a few issues.  The plot is slow to render even with few parameters, and simply too unwieldy to use for many typical modeling situations. If you somehow knew in advance which parameters were causing issues, you could narrow it down by only looking at those parameters. But if you knew which parameters were the problem, you wouldn't need the pairs plot.  

Another issue is that it isn't what you think it is.  The upper diagonal is not just the flipped coordinates of the lower diagonal (like every other scatterplot matrix you've seen).  The chains are split such that half are used for the above diagonal plots, and the other for the lower.  

> In this case, plots below the diagonal will contain realizations that are below the median of the indicated variable (or are zero in the case of "divergent__"), and plots above the diagonal will contain realizations that are greater than or equal to the median of the indicated variable (or are one in the case of "divergent__"). ~ documentation for <span class="func" style = "">mcmc_pairs</span>

I suspect this won't mean much to applied users. Another issue is that you could change how the chains are split and it could potentially dramatically change the how the pattern of divergent transitions looks.  But the gist is, that if your red points show up on the upper diagonal, changing the `adapt_delta` argument may help, otherwise it likely won't.  In the cases I see for myself and clients, increasing `adapt_delta` rarely helps in either case, but it doesn't hurt to try.

Let's take a look anyway.  I'll use `hex` bins instead of standard points because the point plots have no transparency, don't take standard arguments to update, and generally require more effort to figure out how to manipulate than I want to spend on a diagnostic plot.  In addition, we'll use a density plot on the diagonal, instead of the poorer option of histogram.

```{r pairs-plot-model-start-complex}
mcmc_plot(
  model_problem,
  pars = c('b_b11', 'b_b21', 'sigma'),
  type = 'pairs',
  diag_fun = 'dens',
  off_diag_fun = 'hex',
  fixed = TRUE
)

# mcmc_pairs(
#   model_problem,
#   pars = c('b_b11', 'b_b21', 'sigma'),
#   # type = 'pairs',
#   diag_fun = 'dens',
#   off_diag_fun = 'hex',
#   np = nuts_params(model_problem),
#   condition = pairs_condition(nuts = 'divergent__')
# )
```

With problematic cases, what you might see on the off-diagonal plots is some sort of 'funneling', which would indicate where the sampler is getting stuck in the parameter space.  However, this visual notion isn't really defined, and it may be happening without being obvious, displaying just a bump[^funnelcloud].  But you'll also regularly see correlated parameters, including nonlinearly related ones, but it's unclear whether these might be a problem as well.

For the main model the pairs plot for all parameters took several seconds to produce, and even with the hex option still looks pretty poor.  It shows the intercept and `b2` parameters to be notably correlated, but there is no obvious reason for this.

```{r pairs-plot-model-start}
mcmc_plot(
  model_start,
  type = 'pairs',
  off_diag_fun = 'hex',
  diag_fun = 'dens'
)
```


#### Example: Parallel Coordinates Plot

It is also suggested to look at parallel coordinates[^parcoord].  The order of these is arbitrary, but can definitely be highly influential in your perception of them.  Also, unless everything is on similar scales, they aren't going to be very useful, but even if you scale your data in some fashion, the estimates given divergent transitions may be notably beyond a reasonable scale.  With our model, it was actually difficult to get divergent transitions, and sometimes, with a overly complex model with almost no warmup and only 15% of the data, I could get a result with no warnings whatsoever.

As near as I can tell, we'd be looking for knots where the highlighted lines converge to a point. Likewise in our pairs plot, we'd be looking for a pattern among the divergences.  If these aren't the case we're told that the divergences are probably false positives, which seems counter to the suggestion that even 1 divergent transition

```{r par-coord}
library(bayesplot)



mcmc_parcoord(
  model_start_complex,
  pars = vars(matches('^b')),
  size = .25, 
  alpha = .01,
  np = nuts_params(model_start_complex),  # without this div trans won't be highlighted
  np_style = parcoord_style_np(
    div_color = "#ff5500",
    div_size = 1,
    div_alpha = .1
  )
)
```

#### Solution for divergent transitions

Use the control argument to increase `adapt_delta` to .99[^adapt_delta] and let your model have more warmup/total iterations.

```{r adapt-delta, eval = F}
model = brm(..., control = list(adapt_delta = .99))
```

Aside from that you will have to look more deeply, including issues with priors, model specification, and more.  I find this typically comes from poor data (e.g. not scaled) combined with a complex model, and beyond that, the priors may need to be amended.


#### More

https://discourse.mc-stan.org/t/divergent-transitions-a-primer/17099


### shinystan

ADD SHINYSTAN IMG


Unfortunately the shinystan documentation in the browser doesn't really tell you what to look for in these plots.  The glossary contains information likely overly technical for applied users, and if there is a problem, there's not really a whole lot to go on.  As an example, consider the tree depth plots.  What would not be good here? The help tells you the value should be somewhere between 0 and whatever `max_treedepth` is set at.  If they are large, the documentation states it could be due to three different things, all of which suggest either reparameterization of the model (probably not unless using Stan directly), or just increasing the value.  None of the documentation tells you what those plots are supposed to look like, and unfortunately that renders them as not very useful.

The divergence and energy plots are similarly underexplained.  Many refer users to Betancourt's wonderful articles on the details, but these are far too technical for those not already steeped in the approach, and even then[^betanwhat].


### Other messages

Certain builds of rstan for certain settings (e.g. specific operating systems) will often have warnings or other messages.  Oftentimes it looks like a bunch of gobbledygook, which is typically something happening at the C++ level. If your model runs and produces output, you can typically ignore most cases. However, even then you should look it up on the forums just to be sure.

#### Parser warnings

Parser warnings are either a deprecation warning or another more serious kind (Jacobian). The latter will not happen if you're using higher level interfaces rather than programming in Stan directly. The other kind, deprecation warnings, are not something you can do anything about, but the developer of the package will likely need to make minor changes to the code to avoid them.  I've never seen parser warnings from using rstanarm or brms.


#### Compilation warnings

Compiler warnings happen regularly and indicate something going on at the compiler level, typically that something in Stan is being compiled but not used.  You can ignore these.



#### Solutions for other messages

If you are using a package to interface with Stan and not having an issue with the model (i.e. it runs), these messages can largely be ignored.



## Other Problems

### But it's slow!

The main other problem people seem to be concerned with is the speed of the analysis.  If you have a lot of data, or more the point, a lot of parameters, your model can be very slow.  For standard models with rstanarm and brms, there may be no real benefit if you have millions of data points and simpler models.  If your model is only taking a couple minutes, then you really have nothing to complain about- watch some youtube or something while it runs. If your model takes on the order of hours, work with less data or simpler models, until you have your modeling code, plots, etc. squared away

#### Solutions for a slow model

- Scale the data: for any model, bayesian or otherwise, scaling the data will usually result in a more manageable estimation process.
- Use more informative priors: why explore areas of the posterior that aren't going to lead to plausible results?
- Possibly use less iterations if no other issues.
- If possible, work with less data or simpler models until ready for the real deal.
- If possible, work with a different version of the model that can still answer the question of interest. For example, if an ordinal model is causing a lot of issues, and you're not interested in category specific probabilities, just treat it as numeric[^reparameterize].

## Summary: The Practical Approach to Diagnostics

For applied analysts, I would suggest primarily focusing on the density plots and trace plots for parameters.  For the density plots of regression coefficients, these should be roughly normal looking, for variance parameters you may see skewness, especially if the estimate is relatively near zero with smaller data sets. Trace plots in general should look 'grassy' or like a 'fuzzy caterpillar', which isn't very helpful, but deviations are usually striking and obvious in my experience.  If you see chains that look like they are getting stuck around certain estimates, or separating from one another, this would indicate a problem.  If the chains are not converging with one another, you probably already were getting warnings and messages.





## Better Bayesian Approaches

We've talked about the basics one can do to run a bayesian model, and what to do if there is a problem, which is the primary goal of this post.  But it might be nice if we could avoid the problems in the first place, and our model might still be inadequate without any warnings.  So let's do some 'best practical practices' to outline an approach you can use every time to help things run more smoothly.


## Suggested Steps

<div style = 'text-align: center'>
<i class="fas fa-list-ol fa-5x" style="padding: 20px"></i>
</div>

<br>

<div style = 'text-align: center'>
<i class="fas fa-hat-wizard fas-5x" style="padding: 20px"></i>
</div>

<br>

- First generate 'fake data' to assess prior viability
- With viable priors, start with a simple, but viable model
- For simple models you do not need many iterations
    - If you are doing standard GLM or simpler versions of common extensions, even the defaults are likely overkill.  For example, a basic linear regression should converge almost immediately.
- Problems
  - If Rhat is issue, run more iterations
  - if max_tree_depth is issue, increase
  - if divergent transitions
      - Check data, has it been scaled?
      - Use pairs plot
      - Use parcoord plot
      <!-- - reparameterize model -->
      - get more/better data
      - get a better model
- Use pp_check
    - Nice, but what if it doesn't fit?
        - get better data
        - get a better model
- Explore a more viable model
- Compare and/or average models

In this case, let's say we know we should at least do a linear mixed model.  You could even start with just a linear regression, but we'll say we already know we need to at least account for the heterogeneity.




## Simulate from priors

A first step is to produce some viable priors.  But the obvious question is, what priors should we choose?  Thankfully, for standard models there is not much guesswork involved. Bayesian analysis has been around a long time, so the bulk of the work has been done for you.  Even default settings should not affect things much, especially for rstanarm, which has some basic defaults that are informed by the data.  However, due to the flexibility of the brms modeling functions, some priors are unspecified and left flat (i.e. uniform), which is something we definitely don't want, and while others like rstanarm's defaults might be somewhat better, they could still cause problems for more complex situations.  So how might we choose better ones?


The basic idea here is to generate parameters (e.g. regression coefficients) based on the prior distributions, predict data based on those parameters, and then compare the predictions to our observed target variable.  The brms package makes this very easy to do.  We will check the following:

- Prior set 0  
  - similar to brms
  - regression coefficients: uniform
  - Intercept: default
  - Variances: default
- Prior set 1
  - similar to rstanarm
  - regression coefficients: normal
  - Intercept: default
  - Variances: default
- Prior set 2
  - diffuse, larger sd than rstanrm
  - regression coefficients: Normal with mean 0, standard deviation 10
  - Intercept: default
  - Variances: default
- Prior set 3
  - If data is standardized, this would be very reasonable
  - regression coefficients: Normal with mean 0, standard deviation 1
  - Intercept: default
  - Variances: default
- Prior set 4
  - Go further to restrict range of intercept to more plausible values
  - regression coefficients: Normal with mean 0, standard deviation 1
  - Intercept: mean of `y` (~3)
  - Variances: default
- Prior set 5
  - Go further to restrict range of sigma to more plausible values
  - regression coefficients: Normal with mean 0, standard deviation 1
  - Intercept: based on mean of `y` (~3)
  - Variances: based on sd of `y` (~1)

We can use pp_check to examine the prior-generated data versus the observed target `y`, but I wait to show them all together at the end.

```{r sample-prior, eval=FALSE}
# prior_summary(model_start) # examine the defaults from a previous model.

pr_uniform = prior(uniform(-100, 100), lb = -100, ub = 100, 'b')

model_default_prior = brm(
  y ~ b1 + b2 + x1 + x2 + x3, 
  data = main_df,
  iter = 1000,
  sample_prior = 'only',
  prior =  
)

# pp_check(model_default_prior, nsamples = 50)

pr_auto = sjstats::auto_prior(
  y ~ b1 + b2 + x1 + x2 + x3,
  data = main_df,
  gaussian = TRUE
)

model_auto_prior = brm(
  y ~ b1 + b2 + x1 + x2 + x3, 
  data = main_df,
  iter = 1000,
  sample_prior = 'only',
  prior = pr_auto
)

# pp_check(model_auto_prior, nsamples = 50)


pr_norm_b_0_10 = prior(normal(0, 10), 'b')

model_0_norm_b_0_10 = brm(
  y ~ b1 + b2 + x1 + x2 + x3, 
  data = main_df,
  iter = 1000,
  sample_prior = 'only',
  prior = pr_norm_b_0_10
)

# pp_check(model_0_norm_b_0_10, nsamples = 50)

pr_norm_b_0_1 = prior(normal(0, 1), 'b')

model_0_norm_b_0_1 = brm(
  y ~ b1 + b2 + x1 + x2 + x3, 
  data = main_df,
  iter = 1000,
  sample_prior = 'only',
  prior = pr_norm_b_0_1
)

# pp_check(model_0_norm_b_0_1, nsamples = 50)

pr_norm_b_norm_int = c(
  prior(normal(0, 1), class = 'b'),
  prior(normal(3, 1), class = 'Intercept')#,
)

model_0_norm_b_0_1_norm_Int = brm(
  y ~ b1 + b2 + x1 + x2 + x3, 
  data = main_df,
  iter = 1000,
  sample_prior = 'only',
  prior = pr_norm_b_norm_int
)

# pp_check(model_0_norm_b_0_1_norm_Int, nsamples = 50)


pr_norm_b_norm_int_t_sigma = c(
  prior(normal(0, 1), class = 'b'),
  prior(normal(3, 1), class = 'Intercept'),
  prior(student_t(10, 1, 1), class = 'sigma')
)

model_0_norm_b_0_1_norm_Int_sigma = brm(
  y ~ b1 + b2 + x1 + x2 + x3, 
  data = main_df,
  iter = 1000,
  sample_prior = 'only',
  prior = pr_norm_b_norm_int_t_sigma
)

# pp_check(model_0_norm_b_0_1_norm_Int, nsamples = 50)
```

The following plot shows the model predictions based on priors only. We restrict the range of values for display purposes, so note that some of the priors would generate more extreme values.  For example the defaults could generate values into the $\pm$ 500 and beyond. I also mark the boundaries of the observed target variable. 

```{r proposed-priors-plot, echo=FALSE}
# can't do alpha here
color_scheme_set(scico::scico(6, palette = 'batlow', begin = .1, end = .9, direction = -1)) 

{
  {
    pp_check(model_default_prior, nsamples = 50, alpha = .2) +
      labs(title = 'Defaults')
  }/
    # theme(axis.text.x = element_text(size= 6)) +
    {
      pp_check(model_auto_prior, nsamples = 50, alpha = .2) +
        labs(title = 'Auto prior')
    }/
    {
      pp_check(model_0_norm_b_0_10, nsamples = 50, alpha = .2) +
        labs(title = 'Normal b, with defaults')
    }
} * 
  geom_vline(xintercept = c(min(main_df$y), max(main_df$y)), color = '#00aaff80') * 
  scale_x_continuous(breaks = seq(-25, 25, by = 10), limits = c(-50, 50)) |
  
{
  {
    pp_check(model_0_norm_b_0_1, nsamples = 50, alpha = .2) +
    labs(title = 'Normal b tighter')
  } /
{
  pp_check(model_0_norm_b_0_1_norm_Int, nsamples = 50, alpha = .2) +
    labs(title = 'Add Int prior') 
  } /
    pp_check(model_0_norm_b_0_1_norm_Int_sigma, nsamples = 50, alpha = .2) +
    labs(title = 'Add sigma prior')
} * 
  geom_vline(xintercept = c(min(main_df$y), max(main_df$y)), color = '#00aaff80') * 
  scale_x_continuous(breaks = seq(-25, 25, by = 10), limits = c(-50, 50))
```


So given that our target variable is between `r 'round(min(main_df$y))'` and `r 'round(max(main_df$y))'`, it seems that just adding some basic, data-informed information to our priors resulted in more plausible results.  This will generally help our models be more efficient and better behaved. Note that if all else fails, for brms you can use a convenience function like auto_prior demonstrated above.


## Run and Summarize the Baseline Model

Now let's run a baseline model, one that's simple but plausible.  Given that there will eventually be additional complexities, I'll go ahead and add some iterations, and increase max tree depth and adapt delta now to make the code reusable.

```{r model-baseline}
library(brms)

pr = c(
  prior(normal(0, 1), class = 'b'),
  prior(student_t(10, 1, 1), class = 'sigma'),
  prior(student_t(10, 1, 1), class = 'sd')  # prior for random intercept std dev
)

model_baseline = brm(
  y ~ b1 + b2 + x1 + x2 + x3 + (1 | group), 
  data   = main_df,
  warmup = 5000,
  iter   = 6000,
  thin  = 4,
  prior  = pr, 
  cores  = 4,
  control = list(
    adapt_delta = .95,
    max_treedepth = 15
  ),
  save_all_pars = TRUE
)
```

```{r model-baseline-summary}
summary(model_baseline)
```



### Check priors


Were our priors informative? The following will do a simple check of whether the posterior standard deviation is greater than 10% of the prior standard deviation[^lakeland].

https://statmodeling.stat.columbia.edu/2019/08/10/

```{r informative-priors}
prior_summary(model_baseline)
bayestestR::check_prior(model_baseline)
```



### Explore and visualize effects

We can plot effects easily with brms. The <span class="func" style = "">conditional_effects</span> function is what we want here.  Without interactions or other things going, on they aren't very interesting, but it's a useful tool nonetheless.


```{r model-start-explore}
conditional_effects(model_start, 'b2')
```

We can also use the hypothesis function to test for specific types of effects.  By default they provide a one-sided probability and uncertainty interval.  For starters,  we can just duplicate what we saw in the previous summary for the `b2` effect.  The only benefit is to easily obtain the one-sided p-value (that `b2` is less than zero) and the corresponding *evidence ratio*, which is just `p/(1-p)`.

```{r model-start-hype}
hypothesis(model_start, 'b21 < 0')
```

But we can really try anything. The following tests whether the combined effect of our categorical covariates is greater than zero.

```{r model-start-hype2}
hypothesis(model_start, 'b11 + b21 > 0')
```


### Model fit

#### Posterior predictive checks

Posterior predictive checks are a key component of bayesian analysis.  The prior checks we did before are just a special case of this.  Here we actually use the posterior distributions of parameters to generate the data to compare what the model 

```{r model-baseline-ppcheck}
pp_check(model_baseline, nsamples = 100)
pp_check(model_baseline, nsamples = 10, type ='error_scatter_avg')

q10 = function(y) quantile(y, 0.1)
q90 = function(y) quantile(y, 0.9)

pp_check(model_baseline, nsamples = 100, type ='stat', stat='median')
pp_check(model_baseline, nsamples = 100, type ='stat', stat = 'q90')
pp_check(model_baseline, nsamples = 100, type ='stat_2d', stat = c('q10', 'q90'))
pp_check(model_baseline, nsamples = 100, type ='stat', stat = 'max')

# library(patchwork)
# {pp_check(model_baseline, nsamples = 100, type ='stat', stat = 'min') +
#     pp_check(model_baseline, nsamples = 100, type ='stat', stat = 'max')}  /
#   {pp_check(model_baseline, nsamples = 100, type ='stat_grouped', stat = 'min', group = 'b2') + 
#       guides(fill = 'none') +
#     pp_check(model_baseline, nsamples = 100, type ='stat_grouped', stat = 'max', group = 'b2') + 
#       guides(fill = 'none')}
```

#### Bayes R-squared

In this scenario, we can examine the amount of variance accounted for in the target variable by the covariates. I don't really recommend this beyond linear models assuming a normal distribution for the target, but people like to report it. Conceptually, it is simply a (squared) correlation of fitted values to the observed, so can be seen as descriptive statistic.  Since we are Bayesians, we also get a ready-made interval for it.  But to stress the complexity in trying to assess this, in this mixed model we can obtain the result with the random effect included (conditional) or without (unconditional).  Both are reasonable ways to express the statistic, but the one including the group effect naturally will be superior, assuming it is notable in the first place.

https://avehtari.github.io/bayes_R2/bayes_R2.html

Andrew Gelman, Ben Goodrich, Jonah Gabry, and Aki Vehtari (2018). R-squared for Bayesian regression models. The American Statistician, doi:10.1080/00031305.2018.1549100. [Online Preprint](http://www.stat.columbia.edu/~gelman/research/unpublished/bayes_R2_v3.pdf).

```{r model-baseline-r2}
bayes_R2(model_baseline)
bayes_R2(model_baseline, re_formula = NA)  # random effects not included
# performance::r2_bayes(model_baseline)  # alternative provides both
```

To show the limitation of R^2^, I rerun the model using a restrictive prior on the intercept. Intercepts for the resulting models are different but the other fixed effects are basically the same.  The R^2^ suggests equal fit.

```{r r2-not-pp, echo=FALSE}
model_r2_vs_pp = update(
  model_baseline, 
  cores = 4,
  prior = c(
    prior(normal(0, 1), class = 'b'),
    prior(normal(5, .1), class = 'Intercept'),
    prior(student_t(10, 1, 1), class = 'sigma'),
    prior(student_t(10, 1, 1), class = 'sd')  # prior for random intercept
  ),
  save_all_pars = TRUE
)

tibble(
  model = c('baseline', 'modified'),
  rbind(as_tibble(bayes_R2(model_baseline, re_formula = NA)),
        as_tibble(bayes_R2(model_r2_vs_pp, re_formula = NA)))
) %>% 
  kable_df()
```

However, a posterior predictive check shows clearly the failure of the modified model to capture the data.

```{r r2-not-pp-check}
{pp_check(model_baseline, re_formula = NA) + labs(subtitle = 'Baseline Model')} /
  {pp_check(model_r2_vs_pp, re_formula = NA) + labs(subtitle = 'Modified Model')}
```

A variant of R^2^, the 'LOO' R^2^, is also available via the loo_R2 function. LOO stands for *leave-one-out*, as in leave-one-out cross-validation.  It's based on the residuals from the leave one out predictions.  The results suggests that the LOO R^2^ actually picks up the difference in models, and would be lower for the modified model, even if we included the random effects.

```{r}
tibble(
  model = c('baseline', 'modified'),
  rbind(as_tibble(loo_R2(model_baseline, re_formula = NA)),
        as_tibble(loo_R2(model_r2_vs_pp, re_formula = NA)))
) %>% 
  kable_df()
```



<!-- ### Model comparison -->

<!-- We will use estimates like WAIC and loo for model comparison later.  They are essentially used as you would AIC, or root mean squared error to compare models in a predictive fashion.  In addition We only really have our  -->

<!-- Even now, we can use loo as a diagnostic to possibly discover problematic observations, -->

<!-- ```{r} -->
<!-- loo(model_start) -->
<!-- WAIC(model_start) -->
<!-- post_prob(model_baseline, model_r2_vs_pp) -->
<!-- ``` -->

<!-- The interesting thing here is that we have a grossly inefficient model, yet none we have nothing that notes any issues -->









## Comparing Models

If we're doing what we should, we should have a couple models to compare against one another.  In the Bayesian approach, we will use estimates like WAIC and loo for model comparison.  They are essentially used as you would AIC, or root mean squared error, to compare models in a predictive fashion.  The values themselves don't tell us much, but in comparing models, lower means less predictive error, which is what we want.  And since we're bayesian, we even have estimates of uncertainty for these values as well.

WAIC vs. LOO.  LOO has better diagnostics for noting whether there are potential issues using it.  But in practice, how much would it differ?

https://mc-stan.org/loo/articles/loo2-weights.html


```{r model-complex}
model_interact = update(
  model_baseline,
  . ~ . + b1:b2 + b2:x1,
  cores = 4
)

model_interact_nonlin = update(
  model_interact,
  . ~ . + s(x3),
  cores = 4
)
```


```{r}
model_baseline = add_criterion(model_baseline,  'loo')
model_interact = add_criterion(model_interact, 'loo')
model_interact_nonlin = add_criterion(model_interact_nonlin, 'loo')

loo_compare(
  model_baseline, 
  model_interact,
  model_interact_nonlin
)

map_df(list(model_baseline,
            model_interact,
            model_interact_nonlin), function(x)
              as.data.frame(bayes_R2(x)))
```


```{r}
post_prob(
  model_baseline, 
  model_interact
)

post_prob(
  model_baseline, 
  model_interact_nonlin
)

post_prob(
  model_baseline, 
  model_interact,
  model_interact_nonlin
)

model_weights(
  model_baseline, 
  model_interact,
  model_interact_nonlin, 
  method = 'loo'
) 
```


```{r}
pp_check(model_baseline)
pp_check(model_interact_nonlin)
pp_check(model_baseline, nsamples = 100, type ='stat', stat = 'min')
pp_check(model_interact_nonlin, nsamples = 100, type ='stat', stat = 'min')
```



### Problems at the loo

After the previous warnings discussed, the next most common point of confusion I see with clients is with model comparison.  Part of the reason is that this is an area of ongoing research and development, and most of the tools and documentation are notably technical.  Another reason is that these are not perfect tools.  They can fail to show notable problems for models that are definitely misspecified, and flag models that are essentially okay.  Sometimes they flag models that other indicators may suggest are better models relatively speaking, which actually isn't a contradiction, but which may indicate an overfit situation.  So what should we do here?

As an example, let's start with our complex model and get the leave-one-out criterion measure.  I will avoid as much technical jargon as possible so that the applied modeler can get on with things.  The first part are the stats that are used in the previous model comparison and weighting, particularly the elpd_loo[^looic]

The following provides influence statistics, which are a way of saying which observations in the data are not capture well by the model. Larger values are bad, but you don't even have to worry much here, the function provides the breakdown for you. 

```{r loo-basic}
loo(model_baseline)
loo(model_interact_nonlin)
```


```{r pareto-compare, echo=FALSE}
pareto_probs = loo(model_interact_nonlin)

str(pareto_probs)

loo_pit_interact_nonlin = pp_check(model_interact_nonlin, type = 'loo_pit')
loo_pit_baseline = pp_check(model_baseline, type = 'loo_pit')

# bind_rows(
#   loo_pit_baseline %>% layer_data(),
#   loo_pit_interact_nonlin %>% layer_data(),
#   .id = 'model'
# ) %>% 
#   mutate(
#     model = factor(model, labels = c('baseline', 'complex')),
#     alpha = rep(c(1, .5), e = 1000)
#     ) %>% 
#   rename(Uniform = x, `LOO-PIT` = y) %>% 
#   ggplot(aes(Uniform, `LOO-PIT`)) +
#   geom_line(aes(color = model, alpha = I(alpha)), size = 2) +
#   geom_line(aes(y = theoretical), color = '#990021', alpha = .5) +
#   scico::scale_color_scico_d(begin = .25, end = .75) +
#   theme_clean()

# main_df %>% 
#   slice(which(pareto_probs$diagnostics$pareto_k > .5)) %>% 
#   bind_cols(pareto_probs$diagnostics %>%
#               as_tibble() %>%
#               slice(which(pareto_probs$diagnostics$pareto_k > .5))) %>% 
#   bind_cols(pareto_probs$pointwise %>%
#               as_tibble() %>%
#               slice(which(pareto_probs$diagnostics$pareto_k > .5)))
```


## Prediction


### Averaging models

Rather than selecting one model, if our goal is prediction, we can use model averaging, which provides weighted estimates of several models. 

```{r}
prediction_data = crossing(
  b1 = 0:1,
  b2 = 0:1,
  x1 = 0,
  x2 = 0,
  x3 = 0
  # x1 = c(-1, 0, 1),
  # x2 = c(-1, 0, 1),
  # x3 = c(-1, 0, 1)
)

average_predictions = pp_average(
  model_baseline,
  model_interact,
  model_interact_nonlin,
  newdata = prediction_data,
  re_formula = NA
) 

average_predictions
```

Vehtari, A., Gelman, A., and Gabry, J. (2017a). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing. 27(5), 1413–1432. doi:10.1007/s11222-016-9696-4 ([journal version](http://link.springer.com/article/10.1007%2Fs11222-016-9696-4), [preprint arXiv:1507.04544](https://arxiv.org/abs/1507.04544)).

Yao, Y., Vehtari, A., Simpson, D., and Gelman, A. (2018) Using stacking to average Bayesian predictive distributions. Bayesian Analysis, advance publication, doi:10.1214/17-BA1091. ([online](https://projecteuclid.org/euclid.ba/1516093227)).

Vehtari, A., Simpson, D., Gelman, A., Yao, Y., and Gabry, J. (2019). Pareto smoothed importance sampling. [preprint arXiv:1507.02646](https://arxiv.org/abs/1507.02646/)

Gabry, J. , Simpson, D. , Vehtari, A. , Betancourt, M. and Gelman, A. (2019), Visualization in Bayesian workflow. J. R. Stat. Soc. A, 182: 389-402. doi:10.1111/rssa.12378. (journal version, arXiv preprint, code on GitHub)


### Cross-validation


```{r}
library(future)
plan(multiprocess)
model_interact_nonlin_cv = kfold(model_interact_nonlin, K = 5, chains = 1, save_fits = TRUE)
str(model_interact_nonlin_cv, 0)
test = kfold_predict(model_interact_nonlin_cv, newdata = prediction_data)

rmse <- function(y, yrep) {
  yrep_mean <- colMeans(yrep)
  sqrt(mean((yrep_mean - y)^2))
}
rmse(y = test$y, yrep = test$yrep)
```




The issues with recommendations that I see are problematic for applied users.  I'm at U of Michigan and see Stan family users from psychology, environmental sciences, engineering, and many other disciplines, and I thought it might be useful to pass along some recurring themes.

Many suggestions, while great if you have time to read several technical articles and discussions and have the ability to understand them, are simply not very helpful for applied users.  These folks probably  don't have the time (and generally no inclination) to delve into things like the details regarding divergent transitions, pareto values, etc.



https://mc-stan.org/docs/2_24/stan-users-guide/problematic-posteriors-chapter.html


## Solutions and their problems

It seems that the 'usual' approach is adding a level of complexity that is going well beyond what should be expected for new/applied users.  While some might actually enjoy such complexity (like me!), this turns applied users off, especially if there is no obvious gain over traditional approaches.

Increasing adapt_delta seems to often does not solve the problem for which it is suggested (based on personal experience it's actually rare). I usually start at .99 anyway just to make it a non-issue. For applied users, having to deal with 'control' options usually shouldn't be a first step, especially if it's not likely to improve things. For example, would trying a simpler model for diagnostic purposes, or rescaling the data, be just as likely to improve things?


Reparameterization/Change of variables
    - Only those with notably advanced statistical knowledge would even know where to begin here.

'Finding a better model' isn't usually helpful even if 100% accurate for every modeling situation.  Unless the advice is specific (e.g. to add an additional specific covariate, change a specific distribution, add some other complexity), it's likely not going to help an applied user.

'Try different priors' Which and by how does one determine 'different'?  For many regression situations, normal(0, 1) vs. normal(0, 10) vs. student(df = X) would result in basically wasted time.  Is there a way to make a reasonable guess as to what should be tested to demonstrate sensitivity/improvement?

The model comparison situation has grown so complex there is a 20+ item FAQ that still may not help in many cases.  What is someone to do if there are 'pareto issues' and they have no other obvious modeling options? Abandon model comparison?

### pareto stuff

https://avehtari.github.io/modelselection/CV-FAQ.html#16_What_to_do_if_I_have_many_high_Pareto_(k)%E2%80%99s


The usual cases are

- misspecified models MC: all models are misspecified, so how will knowing this help us finish the project?
- models with parameters which see the information only from one observation each (e.g. 'random' effect models)  MC: sorry, but this is reality for many such models.  Assuming we can't increase this, which is practically every situation, what are we to do about it?
- otherwise flexible models  MC: how is 'flexible' defined? Overly vague priors?
- [A quick note what I infer from p_loo and Pareto km values](https://discourse.mc-stan.org/t/a-quick-note-what-i-infer-from-p-loo-and-pareto-k-values/3446)
- [Recommendations for what to do when k exceeds 0.5 in the loo package?](https://discourse.mc-stan.org/t/recommendations-for-what-to-do-when-k-exceeds-0-5-in-the-loo-package/3417)
- [Improve model with some observations Pareto >0.7](https://discourse.mc-stan.org/t/improve-model-with-some-observations-pareto-0-7/17500)
- [Bayesian data analysis - roaches cross-validation demo](https://rawgit.com/avehtari/modelselection_tutorial/master/roaches.html#22_cross-validation_checking)
- [16 What to do if I have many high Pareto k’s?](https://avehtari.github.io/modelselection/CV-FAQ.html#16_What_to_do_if_I_have_many_high_Pareto_(k)%E2%80%99s)
- [Pareto K for outlier detection 1](https://discourse.mc-stan.org/t/pareto-k-for-outlier-detection/12177/9)





> These measures are not independent. If there are many high Pareto k values as in case of model 4, then elpd_loo (or looic) can’t be trusted. Even if there would be no high Pareto k values, R^2 can’t be trusted if p_loo is relatively high compared to the total number of parameters or the number of observations as in case of model 2-4. So there is no contradiction here, but you need to take into account if diagnostics tell you that some other measures can’t be used. ~ [Aki Vehtari](https://discourse.mc-stan.org/t/good-pp-check-and-r-square-but-large-pareto-k-values/17678)





Diagnostics in general are notably overexplained for applied users.  Applied users need rules of thumb, which will always be problematic, but we also can't expect a user of rstanarm to become half expert in LOO model comparison issues, causes of divergent transitions, and what amount of missing .

If the Stan group isn't high on using Bayes factors, then functions should give a warning and reference. I'm not sure if you are aware, but Krushke and others are leading many in applied disciplines to use it.


Suggesting to examine a pairs plot is essentially useless.  For typical models it is far too large (not to mention extremely slow), and furthermore, what are people supposed to look for that would actually be something they can act on?


Common problems


## Easy solutions for common messages and warnings
- Rhat too high
- bulk/tail ESS too low
- Slow





## Resources


https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations

https://mc-stan.org/docs/2_24/reference-manual/divergent-transitions.html

https://discourse.mc-stan.org/t/divergent-transitions-a-primer/17099

https://mc-stan.org/docs/2_24/reference-manual/effective-sample-size-section.html

https://mc-stan.org/bayesplot/articles/visual-mcmc-diagnostics.html

http://mc-stan.org/cmdstanr/articles/cmdstanr.html Use CmdStan to save memory


[^betanwhat]: Betancourt, whose work I admire greatly, typically makes my head spin.

[^parcoord]: I swear sometimes we're taking a step back in time with some of these plots.  All of the diagnostic plots defaults appear to be styles you'd find in Tukey's EDA from 1950.  You don't necessarily need to get fancy, but surely the defaults could be better.

[^funnelcloud]: And I say this as someone with  experience detecting funnel clouds.

[^adapt_delta]: In my experience, there isn't a need to guess between .80 and .99 as the time differences are typically negligble.  If it doesn't work at .99, it won't at .9999 either.

[^bigobjects]: With more posterior samples comes slower visualizations and possibly other computations.

[^mac]: I've had so many issues with 'just works' Macs I will almost certainly abandon them completely for my next round of computing.

[^lakeland]: In the same post, as a comment, Daniel Lakeland proposes an alternative approach is whether the posterior estimate falls within the 95% highest density interval of the prior.  This is available via the method argument:  `bayestestR::check_prior(model_baseline, method = 'lakeland')`.

[^looic]: The `looic` is just -2*`elpd_loo`, as we often use -2*log likelihood (a.k.a. deviance) in standard approaches for AIC.  In this case, `elpd_loo`, is a leave-one-out density.  `p_loo` is the 'effective number of parameters', which users of penalized regression and mixed models will have some familiarity with.

[^reparameterize]: It is often suggested in the Stan world to reparameterize models.  However, this advice doesn't really apply in the case of using rstanarm or brms (i.e. where you aren't writing Stan code directly), and it assumes a level of statistical expertise many would not have, or even if they do, route to respecifying the model may not be obvious.