---
title: "Bayesian Analysis"
description: |
  Steps you can take in your Stan journey.
author:
  - name: Michael Clark
    url: https://m-clark.github.io
date: '`r format(Sys.Date(), "%B %d, %Y")`'
preview: ../../img/r_and_stan.png   # apparently no way to change the size displayed via css (ignored) or file (stretched)
output:
  distill::distill_article:
    self_contained: false
    toc: true
    css: [../../styles.css, 'https://use.fontawesome.com/releases/v5.14.0/css/all.css']
draft: true
tags: [tags, taggy]
categories:
  - bayesian
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo      = T, 
  eval      = F, 
  message   = F, 
  warning   = F, 
  comment   = NA,
  R.options = list(width = 120),
  cache.rebuild = F,
  cache = T,
  fig.align = 'center',
  fig.asp = .7,
  dev = 'svg',
  dev.args = list(bg = 'transparent')
)

library(tidyverse)
library(broom)
library(kableExtra)
library(visibly)

kable_df <- function(..., digits=3) {
  kable(..., digits=digits) %>% 
    kable_styling(full_width = F)
}

rnd = tidyext::rnd
```




## Overview

```{r rstan-img, echo=FALSE, out.width='50%', eval=F}
knitr::include_graphics('img/r_and_stan.png')  # this doesn't work without creating an img dir in the post dir
```

<img src='../../img/r_and_stan.png' style="display:block; margin: 0 auto; width: 50%">

<br>

Bayesian analysis takes some getting used to, but offers great advantages once you get into it.  While it can be difficult to get started, it typically should not take much to repeat an analysis one is already familiar with, say a standard regression with some (common) additional complexity like a binary outcome, interactions, random effects, etc.  What's great is that *Stan*, a programming language for Bayesian analysis, has come a long way and provides a means for (relatively) fast computation.  What's more, applied analysts do not need to know the Stan programming language to do common and even notably complicated models. Packages like rstanarm and brms, coupled with additional tools like bayesplot, tidybayes, and more, make getting and exploring results even easier than the R packages one already uses.

One of the advantages of doing Bayesian analysis with these tools is that there are many ways to diagnose model issues, problems, and failures.  This is great! Traditional analysis can often be notably more difficult in this regard, or at least typically requires more hands-on approaches, and often there can be serious model deficiencies without much notification or, if there is, there might be little one can do about it. On the other hand, current Bayesian packages are providing diagnostic information as a default part of results, have ready visualizations to explore issues, and more.



## The Stated Problem


<div style = 'text-align: center'>
<i class="fas fa-exclamation-triangle fa-5x" style="padding: 20px"></i>
</div>


This is all great in general.  However, in consulting I see a consistent issue.  Someone has a standard model, e.g. a basic regression or mixed model, but is getting problematic warnings, messages, or errors.  When they look up how to solve this problem, the documentation, forums, and other outlets, while great resources for the initiated, generally overwhelm newer and/or more applied users, as they are too technical, assume a great deal of background knowledge, are even underexplained (especially in the case of interpreting visualizations), and even suggest things that aren't typically possible (e.g. getting better data/model).  What seems straightforward to an advanced user or developer may not be even close to that for most applied analysts, and many of them (again, my experience) are left a bit deflated by the whole process.

In the beginning of Stan's ascension, the vast majority of people using Stan/rstan were more technically inclined, willing to code Stan directly, and, when problems arose, they were willing to do a lot of extra work to solve the problems (reading articles, developing pseudo-expertise with various techniques, and more). But tools like rstanarm and brms make running Bayesian models as easy to do as using base R functions and other non-Bayesian packages, and as Bayesian analysis has become more widely used, accepted, and presented, this has opened the Bayesian approach to practically anyone that wants to do it.


I personally don't think one needs to be an expert in Bayesian analysis to enjoy its benefits for common models, even if difficulties come up.  So this is an attempt at a one-stop shop for dealing with the most common issues that arise, interpreting results of diagnostics, and summarizing the options one can take.


## Audience

<div style = 'text-align: center'>
<i class="fas fa-users fa-5x" style="padding: 20px"></i><i class="fas fa-users fa-5x"></i><i class="fas fa-users fa-5x"></i>
</div>

Here is the presumed audience for this post:

- Wants to bayes
- Is not going to code anything in Stan directly, nor knows how to
- Is not a statistician, nor desires technical insights about Bayesian analysis (at least not yet!)
- Is already having to learn a new tool/functions/possibly a whole system of inference
- Has probably never used the control argument for any model 


While you can do a Bayesian analysis just for the heck of it, you really need to understand a few key ideas to take advantage of what it offers. Some things you do need to know in order to use it on a basic level:

- The distributions: priors, likelihood, posterior, posterior predictive
- Iterative sampling to estimate  parameters. Even a cursory understanding of maximum likelihood would probably be enough.

You can obtain this basic info from my document [Bayesian Basics](m-clark.github.io/bayesian-basics/).



## Outline


Here is what we're going to do:

- Discuss the most common problems
- Provide quick solutions that should work most of the time
- Provide a practical approach for future endeavors




## Installation issues

Your first hurdle is installation, so let's start there.  Applied users will use rstanarm, brms, or other higher level interfaces to the Stan programming language. These tools use Stan, but Stan itself requires compilation to C++. This means that to run a basic regression with brms, you'll likely be depending on multiple languages and packages for it to work.

$\rightarrow$ package with basic R code  (brms, rstanarm)

$\rightarrow$ Translated to Stan (or use rstan with Stan code)
  
$\rightarrow$ Compiled to C++  (requires compiler)
  
  
In general, installing the package you want to use will install the appropriate dependencies, and that should be enough.  In some cases it may not be.


### Mac

Mac's Catalina OS has been problematic to say the least.  Since its release, there was a  period of time where Stan wasn't viable for many users without extensive workarounds, and even now, issues seemingly still arise with every update to Catalina. This is not specific to Stan, or R, by the way[^mac].  Then R 4.0 came along and helped some things, but also resulted in new problems. That said, the Stan community have been excellent at helping people here. My luck of late has been better, but I just assume at this point that if I update Catalina or update the packages, I may have to work through some problem.

Here are some discussions on the Stan forums for getting through installation problems.  Unfortunately a couple of these threads are now nearly a year old, and weren't closed.  This means that anything prior to R 4.0 can be ignored, and you probably only want to skip towards the end of those discussions for the relevant parts.

- [Dealing with Catalina I](https://discourse.mc-stan.org/t/dealing-with-catalina/11285/) First post Oct 2019
- [Dealing with Catalina II](https://discourse.mc-stan.org/t/dealing-with-catalina-ii/11802/) First post Nov 2019
- [Dealing with Catalina III](https://discourse.mc-stan.org/t/dealing-with-catalina-iii/12731/) First post Jan 2020
- [Dealing with Catalina IV](https://discourse.mc-stan.org/t/dealing-with-catalina-iv/13502/) First post Mar 2020


### Windows

I rarely had installation issues on Win 7 or Win 8 or Win 10, though haven't had to use it lately.  It does require [rtools](https://cran.r-project.org/bin/windows/Rtools/) to be installed, which is good to install for R with Windows anyway.  Again though, if you have issues, the Stan community will be very helpful.

[Search Windows Issues on Stan Forums](https://discourse.mc-stan.org/search?q=windows)




### Linux

I've only used Stan with Linux in a cluster computing environment. Applied users also need cluster computing on occasion, it's just that the problems may require IT support in that case.  I generally have not had issues, but it's not something I've done recently.  Also, if you're using Linux, you're probably an advanced user of these tools in general, and may not be the audience this post is mostly geared toward.  

[Search Linux Issues on Stan Forums](https://discourse.mc-stan.org/search?q=Linux)


### Other programming languages besides R

If you're using Stan with Python, Stata, Julia, etc., then you're coding Stan directly, and you're likely very familiar with the forums and dealing with a variety of issues.  That's not to say that you might not find something useful here, it's just that I have nothing to offer you regarding those platforms specifically.


## Example data

To start out, I'm going to create some data for us to run some basic models with.

```{r create-data-setup}
library(tidyverse)

create_data <- function( N = 1000, ng = 100, seed = 1234) {
  
  set.seed(seed)
  
  X_mm = cbind(
    # a standard binary
    binary_1 = sample(0:1, N, replace = TRUE),                      
    # a relatively rare categorical
    binary_2 = sample(0:1, N, replace = TRUE, prob = c(.05, .95)),  
    # two partly collinear numeric
    mvtnorm::rmvnorm(N, mean = rep(0,3), sigma = lazerhawk::create_corr(runif(3, max = .6)))
  )
  
  X_mm = cbind(
    # intercept
    1,
    X_mm,
    # a quadratic effect
    scale(poly(X_mm[,5], 3))[,2:3],
    # interaction of binary variables
    X_mm[,1]*X_mm[,2], 
    # interaction of binary 2 with numeric 1
    X_mm[,2]*X_mm[,3]
  )
  
  # add intercept
  colnames(X_mm) = c(
    'Intercept',
    'b1',
    'b2',
    'x1',
    'x2',
    'x3',
    'x3_sq',
    'x3_cub',
    'b1_b2',
    'b2_x1'
  )
  
  # coefficients
  beta = c(
    3.0,   # intercept
     .3,   # b1
    -.3,   # b2
     .5,   # x1
     .0,   # x2
     .3 ,  # x3 
     .3,   # x3_sq
    -.2,   # x3_cub
     .5,   # b1_b2 
    -.5    # b2_x1
  )
  
  # create target variable/linear predictor
  
  y = X_mm %*% beta
  
  # add random effect
  
  groups = sort(sample(1:ng, N, replace = T))
  re = rnorm(ng, sd = .5)[groups]
  
  # add re and residual noise
  y = y + re + rnorm(N)
  y = cbind(y, groups)
  colnames(y) = c('y', 'group')
  
  as_tibble(cbind(X_mm, y))
}
```


```{r create-data-test}
dat = create_data(N = 10000)

glimpse(dat)

mod = lme4::lmer(y ~ . -group + (1|group), data.frame(dat[,-1]))

mixedup::summarise_model(mod, ci = FALSE)
```


```{r create-data}
# create a realistic data frame

main_df = 
  create_data(N = 1000) %>% 
  as_tibble() %>% 
  select(group, b1:x3, y) %>% 
  mutate(
    b1 = factor(b1),   # will help with visuals
    b2 = factor(b2)
  )
```





## Traditional steps

- Use standard/default priors for the model
    - for standard models, normal or student t for the (fixed effect) coefficients
    - (half) student-t for variances
    - Otherwise, look at the [recommendations](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations)
    
    
- Build models in increasing complexity
    - Never start with the final model

- Examine convergence via Rhat, ESS, visualization
    - If you are okay here with no warnings or messages, you are okay for practical purposes. Any remaining issues, e.g. poor prediction, are likely to not have obvious solutions other than things like getting better data, revising theory, etc.

- Examine fit visually with pp_check, tidybayes, etc.
- Compare models using loo, posterior probabilities of models
  - For technical reasons, avoid using bayes factors

```{r model-start-100}
# no priors, no complexity, all default settings
library(brms)

model_start_100 = brm(
  y ~ b1 + b2 + x1 + x2 + x3, 
  data = main_df,
  iter = 100
)

summary(model_start_100)
```


## Warnings, What They Are, and What to do About Them

https://mc-stan.org/misc/warnings.html


### Rhat & Effective Sample Size


The warnings about Rhat and effective sample size (ESS) are likely to pop up if you use default iterations for more complex models.  The fix for these is usually simple, just let it run longer.  The default is 2000 iterations with warmup half of that.

As an example we can look at our short run model. If we plot the estimated values across each iteration for each chain, we can see that the chains could be mixing better, but only if you are used to looking at these things.  


```{r model-start-x1}
plot(model_start_100, par = 'x1')
mcmc_plot(model_start_100, type = 'trace')
```

To see what you would expect, just plot a series from a normal distribution.

```{r rnorm-trace-plot}
ggplot2::qplot(x = 1:1000,
               y = rnorm(1000),
               geom = 'line')
```


Some suggest to look at rank plots instead of traditional traceplots.  Really, all we've changed is looking for something 'fuzzy' to looking for 'approximately uniform', so my opinion is that it's not much of an improvement visually or intuitively.  In general, histograms, which are variants of bar charts, are pretty much never an improvement on any visualization I've come across.  If you do use it, you can use an overlay approach to see if the ranks are mixing.  Looks a lot like what I'd be looking for from a traceplot.



```{r rank-plots}
mcmc_plot(model_start_100, type = 'rank_hist')
mcmc_plot(model_start_100, type = 'rank_overlay')
```


- **Rhat**: The Rhat statistic measures  the ratio of the average variance of samples within each chain to the variance of the pooled samples across chains; if all chains are at equilibrium, these will be the same and Rhat will be one. If the chains have not converged to a common distribution, the Rhat statistic will be greater than one. 

- **ESS** is an estimate of the effective number of independent draws from the posterior distribution of the estimand of interest. Because the draws within a chain are not independent if there is autocorrelation, the effective sample size will be smaller than the total number of iterations. 
    - Bulk ESS: ESS for the mean
    - Tail ESS: ESS for the 5% and 95% quantiles. Tail-ESS can help diagnosing problems due to different scales of the chains and slow mixing in the tails.

- **Trace plot**: Shows the estimated parameter values at each iteration.  In general you would like a random bouncing around an average value.  

- **Rank plot**: (from bayesplot helpfile) Whereas traditional trace plots visualize how the chains mix over the course of sampling, rank histograms visualize how the values from the chains mix together in terms of ranking. An ideal plot would show the rankings mixing or overlapping in a uniform distribution.  See Vehtari et al. (2019) for details.

- **Efficiency plots**: This is often recommended but never with a function that actually does it.  I assume they mean something like the following, but honestly one shouldn't have to search forums and read the documentation of multiple functions to find this. Some just show you what you already saw in the summary

```{r}
mcmc_plot(model_start, type='rhat')
mcmc_plot(model_start, type='neff')
mcmc_plot(model_start, type='acf')
mcmc_neff(neff_ratio(model_start))
```

#### Solution for Rhat/ESS warnings

The solution to Rhat and ESS warnings is simply to do more iterations. To keep posterior samples and model objects from becoming unwieldy in size[^bigobjects], consider thinning also.

```{r increase-iter, eval=F}
brm(model)  # default with 4 chains, 1000 warmup 2000 total = 4*(2000 - 1000) = 4000 post warmup draws
brm(model, warmup = 2000, iter = 4000)  # 4*(4000 - 2000) = 8000 posterior draws
brm(model, warmup = 2000, iter = 4000, thin = 8)  # 4*(4000 - 2000)/8 = 1000 posterior draws
```


### BFMI low

You may see a warning that says some number of chains had an estimated Bayesian Fraction of Missing Information (BFMI) that was too low. This implies that the adaptation phase of the Markov Chains did not turn out well and those chains likely did not explore the posterior distribution efficiently. For more details on this diagnostic, see https://arxiv.org/abs/1604.00695.


In this case, these issues are easily remedied by just adding more iterations. I typically keep to 1000 posterior samples, which makes for nicer visualizations of distributions without creating relatively large model objects.

```{r model-start-checks}
model_start = update(model_start_100, warmup = 1750, iter = 2000)
summary(model_start)
plot(model_start, par = 'x1')
```

ADD SHINYSTAN IMG

#### Solution for low BFMI

If there aren't other serious problems, add more iterations.


### Tree Depth


> Lack of convergence and hitting the maximum number of leapfrog steps (equivalently maximum tree depth) are indicative of improper posteriors ~ Stan User Guide

Sometimes you'll get a warning about hitting maximum tree depth, and without getting overly technical, the fix is easy enough.  Just set the maximum higher.

```{r tree-depth, eval=FALSE}
model_start = update(model_start_100, warmup = 1750, iter = 2000, control = list(max_tree_depth = 15) )
```


Unfortunately the shinystan documentation in the browser doesn't really tell you what to look for in these plots.  The glossary contains information likely overly technical for applied users, and if there is a problem, there's not really a whole lot to go on.  As an example, consider the tree depth plots.  What would not be good here? The help tells you the value should be somewhere between 0 and whatever `max_treedepth` is set at.  If they are large, the documentation states it could be due to three different things, all of which suggest either reparameterization of the model (probably not unless using Stan directly), or just increasing the value.  None of the documentation tells you what those plots are supposed to look like, and unfortunately that renders them as not very useful.

The divergence and energy plots are similarly underexplained.  Many refer users to Betancourt's wonderful articles on the details, but these are far too technical for those not already steeped in the approach, and even then[^betanwhat].

#### Solution for low max tree depth

Use the control argument to increase the value beyond 10.

```{r max-tree, eval = F}
model = brm(..., control = list(max_treedepth = `a number bigger than 10`))
```


### Divergent Transitions

*Divergent transitions*  are a technical issue that indicates something may be notably wrong with the data or model ([technical details](https://mc-stan.org/docs/2_24/reference-manual/divergent-transitions.html)). They indicate that the sampling process has 'gone off the rails' and the iteration's results and anything based on them (i.e. subsequent draws, parameter estimates) can't be trusted.  

Why might this happen?

- insufficient data for the model's complexity
- poor model
- high collinearity
- improper priors
- separability (logistic regression)


As an example, I'll make a relatively complex model with only a small random sample of the data, and use very few warmups.

```{r divergent-model}
problem_model = brm(
  y ~ b1*b2 + x1*b2 + x2*b2 + x3*b2 + (1|group),
  data = main_df %>% slice_sample(prop = .15),
  cores = 4,
  warmup = 10,
  iter = 260,
  seed = 12
)

problem_model
```

So what do we do in this case? Well let's start with visual inspection.


#### Example: Pairs plot

A  diagnostic tool that is typically suggested to look at with divergent transitions is the <span class="emph" style = "">pairs plot</span>.  It is just a scatterplot matrix of the parameters estimates (and log posterior value), but it suffers from a few issues.  The plot is slow to render even with few parameters, and simply too unwieldy to use for many typical modeling situations. If you somehow knew in advance which parameters were causing issues, you could narrow it down by only looking at those parameters. But if you knew which parameters were the problem, you wouldn't need the pairs plot.  

Another issue is that it isn't what you think it is.  The upper diagonal is not just the flipped coordinates of the lower diagonal (like every other scatterplot matrix you've seen).  The chains are split such that half are used for the above diagonal plots, and the other for the lower.  In addition, for diagonstic purposes:

> In this case, plots below the diagonal will contain realizations that are below the median of the indicated variable (or are zero in the case of "divergent__"), and plots above the diagonal will contain realizations that are greater than or equal to the median of the indicated variable (or are one in the case of "divergent__"). ~ documentation for <span class="func" style = "">mcmc_pairs</span>

I suspect this won't mean much to applied users. Another issue is that you could potentially change how the chains are split and it could dramatically change the how the pattern of divergent transitions looks.  But the gist is, that if your red points show up on the upper diagonal, changing the `adapt_delta` argument will probably help, otherwise it likely won't.  In the cases I see for myself and clients, increasing `adapt_delta` rarely helps in any scenario.

Let's take a look anyway.  I'll use `hex` bins instead of standard points because the point plots have no transparency, don't take standard arguments to update, and generally require more effort than I want to spend on a diagnostic plot.  In addition, we'll use a density plot on the diagonal, instead of the poorer option of histogram.

```{r pairs-plot-model-start-complex}
mcmc_plot(
  model_start_complex,
  pars = c('b_x1', 'sigma'),
  type = 'pairs',
  diag_fun = 'dens',
  off_diag_fun = 'hex'
)
```

We can see on the diagonal that the posterior distribution for `x1` is not even approaching normal. What you might see on the off-diagonal if there were a problem is some sort of 'funneling', which would indicate where the sampler is getting stuck in the parameter space.  However, this visual notion isn't really defined, and it may be happening without being obvious, displaying just a bump[^funnelcloud].  But you'll also regularly see correlated parameters, including nonlinearly related ones, but it's unclear whether these might be a problem as well.

For the main model the pairs plot for all parameters took several seconds to produce, and even with the hex option still looks pretty poor.  It shows the intercept and `b2` parameters to be notably correlated, but there is no obvious reason for this.

```{r pairs-plot-model-start}
mcmc_plot(
  model_start,
  type = 'pairs',
  off_diag_fun = 'hex',
  diag_fun = 'dens'
)
```


#### Example: Parallel Coordinates Plot

It is also suggested to look at parallel coordinates[^parcoord].  The order of these is arbitrary, but can definitely be highly influential in your perception of them.  Also, unless everything is on similar scales, they aren't going to be very useful, but even if you scale your data in some fashion, the estimates given divergent transitions may be notably beyond a reasonable scale.  With our model, it was actually difficult to get divergent transitions, and sometimes, with a overly complex model with almost no warmup and only 15% of the data, I could get a result with no warnings whatsoever.

As near as I can tell, we'd be looking for knots where the highlighted lines converge to a point. Likewise in our pairs plot, we'd be looking for a pattern among the divergences.  If these aren't the case we're told that the divergences are probably false positives, which seems counter to the suggestion that even 1 divergent transition

```{r par-coord}
library(bayesplot)



mcmc_parcoord(
  model_start_complex,
  pars = vars(matches('^b')),
  size = .25, 
  alpha = .01,
  np = nuts_params(model_start_complex),  # without this div trans won't be highlighted
  np_style = parcoord_style_np(
    div_color = "#ff5500",
    div_size = 1,
    div_alpha = .1
  )
)
```

#### Solution for divergent transitions

Use the control argument to increase `adapt_delta` to .99[^adapt_delta] and let your model have more warmup/total iterations.

```{r adapt-delta, eval = F}
model = brm(..., control = list(adapt_delta = .99))
```

Aside from that you will have to look more deeply, including issues with priors, model specification, and more.


#### More

https://discourse.mc-stan.org/t/divergent-transitions-a-primer/17099



## The practical approach to diagnostics

For applied analysts, I would suggest primarily focusing on the density plots and trace plots for parameters.  For the density plots of regression coefficients, these should be roughly normal looking, for variance parameters you likely will see skewness, especially if the estimate is relatively near zero with smaller data sets. Trace plots in general should look 'grassy' or like a 'fuzzy caterpillar', which isn't very helpful, but deviations are usually striking and obvious in my experience.  If you see chains that look like they are getting stuck around certain estimates, or separating from one another, this would indicate a problem.  If the chains are not converging to one another, you probably already were getting warnings and messages.


So now let's explore the actual effects. The <span class="func" style = "">conditional_effects</span> function is what we want here, as this is what you'd likely report visually.  Without interactions or other things going, on they aren't very interesting, but it's a useful tool nonetheless.

```{r model-start-explore}
conditional_effects(model_start, 'b2')
```

```{r}
hypothesis(model_start, 'b11 + b21 > 0')
```


https://avehtari.github.io/bayes_R2/bayes_R2.html

```{r model-start-ppcheck}
bayes_R2(model_start)
pp_check(model_start, nsamples = 100)
pp_check(model_start, nsamples = 10, type ='error_scatter_avg')

q10 = function(y) quantile(y, 0.1)
q90 = function(y) quantile(y, 0.9)
pp_check(model_start, nsamples = 100, type ='stat', stat='median')
pp_check(model_start, nsamples = 100, type ='stat', stat = 'q90')
pp_check(model_start, nsamples = 100, type ='stat_2d', stat = c('q10', 'q90'))
pp_check(model_start, nsamples = 100, type ='stat', stat = 'max')

library(patchwork)
{pp_check(model_start, nsamples = 100, type ='stat', stat = 'min') +
    pp_check(model_start, nsamples = 100, type ='stat', stat = 'max')}  /
  {pp_check(model_start, nsamples = 100, type ='stat_grouped', stat = 'min', group = 'b2') + 
      guides(fill = 'none') +
    pp_check(model_start, nsamples = 100, type ='stat_grouped', stat = 'max', group = 'b2') + 
      guides(fill = 'none')}

```


We will use estimates like WAIC and loo for model comparison later.  We can use loo as a diagnostic to possibly discover

```{r}
loo(model_start)
WAIC(model_start)
```

The interesting thing here is that we have a grossly inefficient model, yet none we have nothing that notes any issues


```{r model-re}
model_re = brm(
  y ~ b1 + b2 + x1 + x2 + x3 + (1|group), 
  data = main_df
)
```





## Simulate from priors


```{r sample-prior, eval=FALSE}
pr = prior(normal(0, 10), 'b')

model_0_norm_b = brm(
  y ~ b1 + b2 + x1 + x2 + x3, 
  data = main_df,
  iter = 1000,
  sample_prior = 'only',
  prior = pr
)

pp_check(model_0_norm_b, nsamples = 50)

pr = prior(normal(0, 1), 'b')

model_0_norm_b_0_1 = brm(
  y ~ b1 + b2 + x1 + x2 + x3, 
  data = main_df,
  iter = 1000,
  sample_prior = 'only',
  prior = pr
)

pp_check(model_0_norm_b_0_1, nsamples = 50)

prior_summary(model_0)


pr = c(
  prior(normal(0, 1), class = 'b'),
  prior(normal(3, 1), class = 'Intercept')#,
)

model_0_norm_b_0_1_norm_Int = brm(
  y ~ b1 + b2 + x1 + x2 + x3, 
  data = main_df,
  iter = 1000,
  sample_prior = 'only',
  prior = pr
)

pp_check(model_0_norm_b_0_1_norm_Int, nsamples = 50)


pr = c(
  prior(normal(0, 1), class = 'b'),
  prior(normal(3, 1), class = 'Intercept'),
  prior(student_t(10, 1, 1), class = 'sigma'),
)

model_0_norm_b_0_1_norm_Int_sigma = brm(
  y ~ b1 + b2 + x1 + x2 + x3, 
  data = main_df,
  iter = 1000,
  sample_prior = 'only',
  prior = pr
)

pp_check(model_0_norm_b_0_1_norm_Int, nsamples = 50)


library(patchwork)
{
  pp_check(model_0_norm_b, nsamples = 50) +
    scale_x_continuous(breaks = seq(-50, 50, by = 25), limits = c(-125, 125)) +
    labs(title = 'Normal b, with defaults') +
    pp_check(model_0_norm_b_0_1, nsamples = 50) +
    scale_x_continuous(breaks = seq(-25, 25, by = 10), limits = c(-50, 50)) +
    labs(title = 'Normal b tighter')
  } /
{
  pp_check(model_0_norm_b_0_1_norm_Int, nsamples = 50) +
    scale_x_continuous(breaks = seq(-25, 25, by = 10), limits = c(-50, 50)) +
    labs(title = 'Add Int prior') +
    pp_check(model_0_norm_b_0_1_norm_Int_sigma, nsamples = 50) +
    scale_x_continuous(breaks = seq(-25, 25, by = 10), limits = c(-50, 50)) +
    labs(title = 'Add sigma prior')
  }
  
```


## Suggested steps

- First generate 'fake data' to assess prior viability
- If Rhat is issue, run more iterations
- if max_tree_depth is issue, increase
- if divergent transitions
    - Use pairs plot
    - Use parcoord plot
    - reparameterize model
    - get better data
    - get a better model
- Use pp_check
    - Nice, but what if it doesn't fit?
        - get better data
        - get a better model
-

## Get the model to run

- Let it run 'long enough'
- No need to save more than necessary

## Explore the model

pp_check, hypothesis etc.

## Compare and/or average models

WAIC vs. LOO.  LOO has better diagnostics for noting whether there are potential issues using it.  But in practice, how much would it differ?

https://mc-stan.org/loo/articles/loo2-weights.html




The issues with recommendations that I see are problematic for applied users.  I'm at U of Michigan and see Stan family users from psychology, environmental sciences, engineering, and many other disciplines, and I thought it might be useful to pass along some recurring themes.

Many suggestions, while great if you have time to read several technical articles and discussions and have the ability to understand them, are simply not very helpful for applied users.  These folks probably  don't have the time (and generally no inclination) to delve into things like the details regarding divergent transitions, pareto values, etc.



https://mc-stan.org/docs/2_24/stan-users-guide/problematic-posteriors-chapter.html


## Solutions and their problems

It seems that the 'usual' approach is adding a level of complexity that is going well beyond what should be expected for new/applied users.  While some might actually enjoy such complexity (like me!), this turns applied users off, especially if there is no obvious gain over traditional approaches.

Increasing adapt_delta seems to often does not solve the problem for which it is suggested (based on personal experience it's actually rare). I usually start at .99 anyway just to make it a non-issue. For applied users, having to deal with 'control' options usually shouldn't be a first step, especially if it's not likely to improve things. For example, would trying a simpler model for diagnostic purposes, or rescaling the data, be just as likely to improve things?


Reparameterization/Change of variables
    - Only those with notably advanced statistical knowledge would even know where to begin here.

'Finding a better model' isn't usually helpful even if 100% accurate for every modeling situation.  Unless the advice is specific (e.g. to add an additional specific covariate, change a specific distribution, add some other complexity), it's likely not going to help an applied user.

'Try different priors' Which and by how does one determine 'different'?  For many regression situations, normal(0, 1) vs. normal(0, 10) vs. student(df = X) would result in basically wasted time.  Is there a way to make a reasonable guess as to what should be tested to demonstrate sensitivity/improvement?

The model comparison situation has grown so complex there is a 20+ item FAQ that still may not help in many cases.  What is someone to do if there are 'pareto issues' and they have no other obvious modeling options? Abandon model comparison?

### pareto stuff

https://avehtari.github.io/modelselection/CV-FAQ.html#16_What_to_do_if_I_have_many_high_Pareto_(k)%E2%80%99s


The usual cases are

- misspecified models MC: all models are misspecified, so how will knowing this help us finish the project?
- models with parameters which see the information only from one observation each (e.g. 'random' effect models)  MC: sorry, but this is reality for many such models.  Assuming we can't increase this, which is practically every situation, what are we to do about it?
- otherwise flexible models  MC: how is 'flexible' defined? Overly vague priors?
- [A quick note what I infer from p_loo and Pareto km values](https://discourse.mc-stan.org/t/a-quick-note-what-i-infer-from-p-loo-and-pareto-k-values/3446)
- [Recommendations for what to do when k exceeds 0.5 in the loo package?](https://discourse.mc-stan.org/t/recommendations-for-what-to-do-when-k-exceeds-0-5-in-the-loo-package/3417)
- [Improve model with some observations Pareto >0.7](https://discourse.mc-stan.org/t/improve-model-with-some-observations-pareto-0-7/17500)
- [Bayesian data analysis - roaches cross-validation demo](https://rawgit.com/avehtari/modelselection_tutorial/master/roaches.html#22_cross-validation_checking)
- [16 What to do if I have many high Pareto k’s?](https://avehtari.github.io/modelselection/CV-FAQ.html#16_What_to_do_if_I_have_many_high_Pareto_(k)%E2%80%99s)
- [Pareto K for outlier detection 1](https://discourse.mc-stan.org/t/pareto-k-for-outlier-detection/12177/9)





> These measures are not independent. If there are many high Pareto k values as in case of model 4, then elpd_loo (or looic) can’t be trusted. Even if there would be no high Pareto k values, R^2 can’t be trusted if p_loo is relatively high compared to the total number of parameters or the number of observations as in case of model 2-4. So there is no contradiction here, but you need to take into account if diagnostics tell you that some other measures can’t be used. ~ [Aki Vehtari](https://discourse.mc-stan.org/t/good-pp-check-and-r-square-but-large-pareto-k-values/17678)





Diagnostics in general are notably overexplained for applied users.  Applied users need rules of thumb, which will always be problematic, but we also can't expect a user of rstanarm to become half expert in LOO model comparison issues, causes of divergent transitions, and what amount of missing .

If the Stan group isn't high on using Bayes factors, then functions should give a warning and reference. I'm not sure if you are aware, but Krushke and others are leading many in applied disciplines to use it.


Suggesting to examine a pairs plot is essentially useless.  For typical models it is far too large (not to mention extremely slow), and furthermore, what are people supposed to look for that would actually be something they can act on?


Common problems


## Easy solutions for common messages and warnings
- Rhat too high
- bulk/tail ESS too low
- Slow





## Resources


https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations

https://mc-stan.org/docs/2_24/reference-manual/divergent-transitions.html

https://discourse.mc-stan.org/t/divergent-transitions-a-primer/17099

https://mc-stan.org/docs/2_24/reference-manual/effective-sample-size-section.html

https://mc-stan.org/bayesplot/articles/visual-mcmc-diagnostics.html

http://mc-stan.org/cmdstanr/articles/cmdstanr.html Use CmdStan to save memory


[^betanwhat]: Betancourt, whose work I admire greatly, typically makes my head spin.
[^parcoord]: I swear sometimes we're taking a step back in time with some of these plots.  All of the diagnostic plots defaults appear to be styles you'd find in Tukey's EDA from 1950.  You don't necessarily need to get fancy, but surely the defaults could be better.
[^funnelcloud]: And I say this as someone with  experience detecting funnel clouds.

[^adapt_delta]: In my experience, there isn't a need to guess between .80 and .99 as the time differences are typically negligble.  If it doesn't work at .99, it won't at .9999 either.

[^bigobjects]: With more posterior samples comes slower visualizations and possibly other computations.

[^mac]: I've had so many issues with 'just works' Macs I will almost certainly abandon them completely for my next round of computing.