---
title: "Bayesian Analysis"
description: |
  Steps you can take in your Stan journey.
author:
  - name: Michael Clark
    url: https://m-clark.github.io
date: '`r format(Sys.Date(), "%B %d, %Y")`'
preview: ../../img/r_and_stan.png   # apparently no way to change the size displayed via css (ignored) or file (stretched)
output:
  distill::distill_article:
    self_contained: false
    toc: true
    css: [../../styles.css, 'https://use.fontawesome.com/releases/v5.14.0/css/all.css']
draft: true
tags: [tags, taggy]
categories:
  - bayesian
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo      = T, 
  eval      = F, 
  message   = F, 
  warning   = F, 
  comment   = NA,
  R.options = list(width = 120),
  cache.rebuild = F,
  cache = T,
  fig.align = 'center',
  fig.asp = .7,
  dev = 'svg',
  dev.args = list(bg = 'transparent')
)

library(tidyverse)
library(broom)
library(kableExtra)
library(visibly)

kable_df <- function(..., digits=3) {
  kable(..., digits=digits) %>% 
    kable_styling(full_width = F)
}

rnd = tidyext::rnd
```




## Overview

```{r rstan-img, echo=FALSE, out.width='50%', eval=F}
knitr::include_graphics('img/r_and_stan.png')  # this doesn't work without creating an img dir in the post dir
```

<img src='../../img/r_and_stan.png' style="display:block; margin: 0 auto; width: 50%">

<br>

Bayesian analysis takes some getting used to, but offers great advantages once you get into it.  While it can be difficult to get started, it typically should not take much to repeat an analysis one is already familiar with, say a standard regression with some (common) additional complexity like a binary outcome, interactions, random effects, etc.  What's great is that *Stan*, a programming language for Bayesian analysis, has come a long way and provides a means for (relatively) fast computation.  What's more, applied analysts do not need to know the Stan programming language to do common and even notably complicated models. Packages like rstanarm and brms, coupled with additional tools like bayesplot, tidybayes, and more, make getting and exploring results even easier than the R packages one already uses.

One of the advantages of doing Bayesian analysis with these tools is that there are many ways to diagnose model issues, problems, and failures.  This is great! Traditional analysis can often be notably more difficult in this regard, or at least typically requires more hands-on approaches, and often there can be serious model deficiencies without much notification or, if there is, there might be little one can do about it. On the other hand, current Bayesian packages are providing diagnostic information as a default part of results, have ready visualizations to explore issues, and more.



## The Stated Problem


<div style = 'text-align: center'>
<i class="fas fa-exclamation-triangle fa-5x" style="padding: 20px"></i>
</div>


This is all great in general.  However, in consulting I see a consistent issue.  Someone has a standard model, e.g. a basic regression or mixed model, but is getting problematic warnings, messages, or errors.  When they look up how to solve this problem, the documentation, forums, and other outlets, while great resources for the initiated, generally overwhelm newer and/or more applied users, as they are too technical, assume a great deal of background knowledge, are even underexplained (especially in the case of interpreting visualizations), and even suggest things that aren't typically possible (e.g. getting better data/model).  What seems straightforward to an advanced user or developer may not be even close to that for most applied analysts, and many of them (again, my experience) are left a bit deflated by the whole process.

In the beginning of Stan's ascension, the vast majority of people using Stan/rstan were more technically inclined, willing to code Stan directly, and, when problems arose, they were willing to do a lot of extra work to solve the problems (reading articles, developing pseudo-expertise with various techniques, and more). But tools like rstanarm and brms make running Bayesian models as easy to do as using base R functions and other non-Bayesian packages, and as Bayesian analysis has become more widely used, accepted, and presented, this has opened the Bayesian approach to practically anyone that wants to do it.


I personally don't think one needs to be an expert in Bayesian analysis to enjoy its benefits for common models, even if difficulties come up.  So this is an attempt at a one-stop shop for dealing with the most common issues that arise, interpreting results of diagnostics, and summarizing the options one can take.


## Audience

<div style = 'text-align: center'>
<i class="fas fa-users fa-5x" style="padding: 20px"></i><i class="fas fa-users fa-5x"></i><i class="fas fa-users fa-5x"></i>
</div>

Here is the presumed audience for this post:

- Wants to bayes
- Is not going to code anything in Stan directly, nor knows how to
- Is not a statistician, nor desires technical insights about Bayesian analysis (at least not yet!)
- Is already having to learn a new tool/functions/possibly a whole system of inference
- Has probably never used the control argument for any model 


While you can do a Bayesian analysis just for the heck of it, you really need to understand a few key ideas to take advantage of what it offers. Some things you do need to know in order to use it on a basic level:

- The distributions: priors, likelihood, posterior, posterior predictive
- Iterative sampling to estimate  parameters. Even a cursory understanding of maximum likelihood would probably be enough.

You can obtain this basic info from my document [Bayesian Basics](m-clark.github.io/bayesian-basics/).



## Outline


Here is what we're going to do:

- Discuss the most common problems
- Provide quick solutions that should work most of the time
- Provide a practical approach for future endeavors




## Installation issues

Your first hurdle is installation, so let's start there.  Applied users will use rstanarm, brms, or other higher level interfaces to the Stan programming language. These tools use Stan, but Stan itself requires compilation to C++. This means that to run a basic regression with brms, you'll likely be depending on multiple languages and packages for it to work.

$\rightarrow$ package with basic R code  (brms, rstanarm)

$\rightarrow$ Translated to Stan (or use rstan with Stan code)
  
$\rightarrow$ Compiled to C++  (requires compiler)
  
  
In general, installing the package you want to use will install the appropriate dependencies, and that should be enough.  In some cases it may not be.


### Mac

Mac's Catalina OS has been problematic to say the least.  Since its release, there was a  period of time where Stan wasn't viable for many users without extensive workarounds, and even now, issues seemingly still arise with every update to Catalina. This is not specific to Stan, or R, by the way[^mac].  Then R 4.0 came along and helped some things, but also resulted in new problems. That said, the Stan community have been excellent at helping people here. My luck of late has been better, but I just assume at this point that if I update Catalina or update the packages, I may have to work through some problem.

Here are some discussions on the Stan forums for getting through installation problems.  Unfortunately a couple of these threads are now nearly a year old, and weren't closed.  This means that anything prior to R 4.0 can be ignored, and you probably only want to skip towards the end of those discussions for the relevant parts.

- [Dealing with Catalina I](https://discourse.mc-stan.org/t/dealing-with-catalina/11285/) First post Oct 2019
- [Dealing with Catalina II](https://discourse.mc-stan.org/t/dealing-with-catalina-ii/11802/) First post Nov 2019
- [Dealing with Catalina III](https://discourse.mc-stan.org/t/dealing-with-catalina-iii/12731/) First post Jan 2020
- [Dealing with Catalina IV](https://discourse.mc-stan.org/t/dealing-with-catalina-iv/13502/) First post Mar 2020


### Windows

I rarely had installation issues on Win 7 or Win 8 or Win 10, though haven't had to use it lately.  It does require [rtools](https://cran.r-project.org/bin/windows/Rtools/) to be installed, which is good to install for R with Windows anyway.  Again though, if you have issues, the Stan community will be very helpful.

[Search Windows Issues on Stan Forums](https://discourse.mc-stan.org/search?q=windows)




### Linux

I've only used Stan with Linux in a cluster computing environment. Applied users also need cluster computing on occasion, it's just that the problems may require IT support in that case.  I generally have not had issues, but it's not something I've done recently.  Also, if you're using Linux, you're probably an advanced user of these tools in general, and may not be the audience this post is mostly geared toward.  

[Search Linux Issues on Stan Forums](https://discourse.mc-stan.org/search?q=Linux)


### Other programming languages besides R

If you're using Stan with Python, Stata, Julia, etc., then you're coding Stan directly, and you're likely very familiar with the forums and dealing with a variety of issues.  That's not to say that you won't find something useful here, it's just that I have nothing to offer you regarding those platforms specifically.


## Example data

To start out, I'm going to create some data for us to run some basic models with. To make things interesting, the true underlying model has categorical and continuous covariates, interactions, nonlinear relationships, random effects (observations are clustered in groups), and some variables are collinear.  You can skip these details if uninterested, but note that we will be purposely using under- and over-fitted models relative to this one to see what happens.

```{r create-data-setup}
library(tidyverse)

create_data <- function( N = 1000, ng = 100, seed = 1234) {
  
  set.seed(seed)
  
  X_mm = cbind(
    # a standard binary
    binary_1 = sample(0:1, N, replace = TRUE),                      
    # a relatively rare categorical
    binary_2 = sample(0:1, N, replace = TRUE, prob = c(.05, .95)),  
    # two partly collinear numeric
    mvtnorm::rmvnorm(N,
                     mean = rep(0, 3),
                     sigma = lazerhawk::create_corr(runif(3, max = .6)))
  )
  
  X_mm = cbind(
    # intercept
    1,
    X_mm,
    # a quadratic effect
    scale(poly(X_mm[,5], 3))[,2:3],
    # interaction of binary variables
    X_mm[,1]*X_mm[,2], 
    # interaction of binary 2 with numeric 1
    X_mm[,2]*X_mm[,3]
  )
  
  # add intercept
  colnames(X_mm) = c(
    'Intercept',
    'b1',
    'b2',
    'x1',
    'x2',
    'x3',
    'x3_sq',
    'x3_cub',
    'b1_b2',
    'b2_x1'
  )
  
  # coefficients
  beta = c(
    3.0,   # intercept
     .3,   # b1
    -.3,   # b2
     .5,   # x1
     .0,   # x2
     .3 ,  # x3 
     .3,   # x3_sq
    -.2,   # x3_cub
     .5,   # b1_b2 
    -.5    # b2_x1
  )
  
  # create target variable/linear predictor
  y = X_mm %*% beta
  
  # add random effect
  groups = sort(sample(1:ng, N, replace = T))
  
  re = rnorm(ng, sd = .5)[groups]  # re sd = .5
  
  # add re and residual noise
  y = y + re + rnorm(N)
  y = cbind(y, groups)
  colnames(y) = c('y', 'group')
  
  as_tibble(cbind(X_mm, y))
}
```

If you want to check that  the parameters are recovered, run the following.

```{r create-data-test}
dat = create_data(N = 10000)

mod = lme4::lmer(y ~ . -group + (1|group), data.frame(dat[,-1]))

mixedup::summarise_model(mod, ci = FALSE)
```

We do not need as much data for our purposes so we'll set the total sample size to 1000.  We'll also make our binary covariates explicit factors, which will make things easier when want to visualize grouped effects later.

```{r create-data}
# create the primary data frame

main_df = 
  create_data(N = 1000) %>% 
  as_tibble() %>% 
  select(group, b1:x3, y) %>% 
  mutate(
    b1 = factor(b1),   # will help with visuals
    b2 = factor(b2)
  )
```





## Basic Steps for Practical Modeling

Once data is in hand there are basic steps for practical modeling with Stan tools.


##### Use standard/default priors for the model

- For common regression models, normal or student t for the (fixed effect) coefficients. You will have to set this!
- (half) student-t for variances. Defaults are usually fine.
- Otherwise, look at the [recommendations](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations)

##### Build models in increasing complexity

- Never start with the 'final' model, if the fitting has issues with simpler models, things will only get worse with more complex models.
- Getting model settings squared away earlier (e.g. setting number of iterations, other options) will save time later.

##### Examine convergence via Rhat, ESS, visualization

- If you are okay here with no warnings or messages, you are okay to proceed for practical purposes. Any remaining issues, e.g. poor prediction, are likely to not have obvious solutions other than things like getting better data, revising theory, etc.

##### Examine fit visually  

- <span class="func" style = "">pp_check</span>, tidybayes, etc.

##### Compare models using loo, posterior probabilities of models

- For technical reasons, avoid using bayes factors

Assuming you have no problems in the above process, you have more or less fulfilled the basic requirements to do more standard analyses in Bayesian form.  Great! 



## Warnings, What They Are, and What to do About Them

Of course, if it was always that easy, we wouldn't be posting this. There are a few warnings that you're bound to come across at some point in modeling with the Stan ecosystem.  We'll cover these, as well as the most common solutions.  

Primary reference: [Brief Guide to Stan’s Warnings](https://mc-stan.org/misc/warnings.html)


### Rhat & Effective Sample Size

We will start with a simple standard regression model that we know is not adequate.  We will use default priors and run very few iterations. 

```{r model-start-100, message=TRUE}
# no priors, no complexity, all default settings, few iterations
library(brms)

model_start_100 = brm(
  y ~ b1 + b2 + x1 + x2, 
  data = main_df,
  iter = 100
)

summary(model_start_100)
```


The warnings about Rhat and effective sample size (ESS) are likely to pop up if you use default iterations for more complex models.  They mostly regard the efficiency of the sampling process, and whether you have enough samples to have stable parameter estimates.  The fix for these warnings is usually simple, just let the model run for more iterations.  The default is 2000 iterations with warmup half of that.  Warmup iterations are not used in calculation of parameter estimates, so you can just adjust iterations relative to the warmup.

As an example we can look at our short run model. If we plot the estimated values across each iteration for each chain, we can see that the chains could be mixing better, but only if you are used to looking at these things.  


```{r model-start-x1}
color_scheme_set(scico::scico(6, palette = 'batlow', begin = .1, end = .9, direction = -1))
# plot(model_start_100, par = 'x1') # examine specific parameters
mcmc_plot(model_start_100, type = 'combo')
mcmc_plot(model_start_100, type = 'areas')
```

To see what you would expect, just plot a series from a normal distribution.

```{r rnorm-trace-plot}
ggplot2::qplot(x = 1:1000,
               y = rnorm(1000),
               geom = 'line')
```

While things seem okay, what happens if we single out a particular chain, we may see otherwise.  In this case, the intercept and b2 coefficients may be problematic.

```{r trace-highlight}
mcmc_plot(model_start_100, highlight = 1, type = 'trace_highlight')
```



Some suggest to look at rank plots instead of traditional traceplots.  Really, all we've changed is looking for something 'fuzzy' to looking for 'approximately uniform', so my opinion is that it's not much of an improvement visually or intuitively.  In general, histograms, which are variants of bar charts, are rarely an improvement on any visualization I've come across.  If you do use it, you can use an overlay approach to see if the ranks are mixing, but this looks a lot like what I'd be looking for from a traceplot.

```{r rank-plots}
mcmc_plot(model_start_100, type = 'rank_hist')
mcmc_plot(model_start_100, type = 'rank_overlay')
```

#### Some technical details


##### Rhat

The $\hat{R}$ statistic measures  the ratio of the average variance of samples within each chain to the variance of the pooled samples across chains; if all chains are at equilibrium, these will be the same and Rhat will be one. If the chains have not converged to a common distribution, the Rhat statistic will be greater than one.

##### ESS

Effective sample size is an estimate of the effective number of independent draws from the posterior distribution of the estimand of interest. Because the draws within a chain are not independent if there is autocorrelation, the effective sample size will be smaller than the total number of iterations.

  - *Bulk ESS*: ESS for the mean
  - *Tail ESS*: ESS for the 5% and 95% quantiles. Tail-ESS can help diagnose problems due to different scales of the chains and slow mixing in the tails.
    
##### Trace plot

Shows the estimated parameter values at each iteration.  In general you would like a random bouncing around an average value.

##### Rank plot

From bayesplot helpfile: 

>Whereas traditional trace plots visualize how the chains mix over the course of sampling, rank histograms visualize how the values from the chains mix together in terms of ranking. An ideal plot would show the rankings mixing or overlapping in a uniform distribution.  

See Vehtari et al. (2019) for details.

##### ACF plot

The acf, or autocorrelation function plot, is exactly the same thing you'd visualize for any time series.  It is a plot of a series correlation with specific lags of itself.  Autocorrelation does not bias estimates, but increased autocorrelation may be more inefficient/slower. At lag zero, the series estimates are perfectly correlated with themselves, so that's where the plot usually starts.

##### Efficiency plots

I have seen these often been recommended but without an accompanying package function that actually does it.  I assume they mean something like the following, but honestly one shouldn't have to search forums and read the documentation of multiple functions to find this. Some just show you what you already saw in the summary, so don't really help. For the first two plots, darker is worse.

```{r eff-plots}
mcmc_plot(model_start_100, type = 'rhat')  # > 1.05 problematic
mcmc_plot(model_start_100, type = 'neff')  # < .1 problem
mcmc_plot(model_start_100, type = 'acf')
```

#### Solution for Rhat/ESS warnings

The solution to Rhat and ESS warnings is simply to do more iterations. To keep posterior samples and model objects from becoming unwieldy in size[^bigobjects], consider thinning also.

```{r increase-iter, eval=F}
brm(model)  # default with 4 chains, 1000 warmup 2000 total = 4*(2000 - 1000) = 4000 post warmup draws
brm(model, warmup = 2000, iter = 4000)  # 4*(4000 - 2000) = 8000 posterior draws
brm(model, warmup = 2000, iter = 4000, thin = 8)  # 4*(4000 - 2000)/8 = 1000 posterior draws
```



### BFMI low

You may see a warning that says some number of chains had an estimated Bayesian Fraction of Missing Information (BFMI) that was too low. This implies that the adaptation phase of the Markov Chains did not turn out well, and those chains likely did not explore the posterior distribution efficiently. For more details on this diagnostic, you can see [Betancourt's article](https://arxiv.org/abs/1604.00695), but this will almost surely be too technical for many applied and even more advanced users.

In this case, the problem here is usually remedied by just adding more iterations as before. I typically keep to 1000 posterior samples, which makes for nicer visualizations of distributions without creating relatively large model objects.

```{r model-start-checks}
model_start = update(model_start_100, warmup = 1750, iter = 2000) # 4 * (2000 - 1750) = 1000 posterior draws
summary(model_start)
plot(model_start, par = 'x1')
```


#### Solution for low BFMI

If there aren't other serious problems, add more iterations.  In some cases, switching from a heavy-tailed prior (e.g. student t) to normal would be helpful. Otherwise, you will approach it similarly to the problem of [divergent transitions][divergent transitions].


### Tree Depth

Tree depth is a more technical warning that has to do with the details of Hamiltonian Monte Carlo. Practically speaking:

> Lack of convergence and hitting the maximum number of leapfrog steps (equivalently maximum tree depth) are indicative of improper posteriors ~ Stan User Guide

Sometimes you'll get a warning about hitting maximum tree depth, and without getting overly technical, the fix is easy enough.  Just set the maximum higher.




#### Solution for max tree depth

Use the control argument to increase the value beyond 10.

```{r tree-depth, eval=FALSE}
model_start = update(model_start_100, warmup = 1750, iter = 2000, control = list(max_tree_depth = 15))
```


### Divergent Transitions

*Divergent transitions*  are a technical issue that indicates something may be notably wrong with the data or model ([technical details](https://mc-stan.org/docs/2_24/reference-manual/divergent-transitions.html)). They indicate that the sampling process has 'gone off the rails' and the iteration's results and anything based on them (i.e. subsequent draws, parameter estimates) can't be trusted.  

Why might this happen?

- insufficient data for the model's complexity
- poor model
- high collinearity
- improper priors
- separability (logistic regression)


As an example, I'll make a relatively complex model with only a small random sample of the data, and use very few warmups.

```{r divergent-model}
problem_model = brm(
  y ~ b1*b2*x1 + x2*b2 + x3*b2 + (1 + x1 + b2|group),
  data   = main_df %>% slice_sample(prop = .1),
  cores  = 4,
  warmup = 10,
  iter   = 1010,
  thin   = 4,
  seed   = 12
)

problem_model
```

So what do we do in this case? Well let's start with visual inspection.


#### Example: Pairs plot

A  diagnostic tool that is typically suggested to look at with divergent transitions is the <span class="emph" style = "">pairs plot</span>.  It is just a scatterplot matrix of the parameters estimates (and log posterior value), but it suffers from a few issues.  The plot is slow to render even with few parameters, and simply too unwieldy to use for many typical modeling situations. If you somehow knew in advance which parameters were causing issues, you could narrow it down by only looking at those parameters. But if you knew which parameters were the problem, you wouldn't need the pairs plot.  

Another issue is that it isn't what you think it is.  The upper diagonal is not just the flipped coordinates of the lower diagonal (like every other scatterplot matrix you've seen).  The chains are split such that half are used for the above diagonal plots, and the other for the lower.  

> In this case, plots below the diagonal will contain realizations that are below the median of the indicated variable (or are zero in the case of "divergent__"), and plots above the diagonal will contain realizations that are greater than or equal to the median of the indicated variable (or are one in the case of "divergent__"). ~ documentation for <span class="func" style = "">mcmc_pairs</span>

I suspect this won't mean much to applied users. Another issue is that you could change how the chains are split and it could potentially dramatically change the how the pattern of divergent transitions looks.  But the gist is, that if your red points show up on the upper diagonal, changing the `adapt_delta` argument may help, otherwise it likely won't.  In the cases I see for myself and clients, increasing `adapt_delta` rarely helps in either case, but it doesn't hurt to try.

Let's take a look anyway.  I'll use `hex` bins instead of standard points because the point plots have no transparency, don't take standard arguments to update, and generally require more effort to figure out how to manipulate than I want to spend on a diagnostic plot.  In addition, we'll use a density plot on the diagonal, instead of the poorer option of histogram.

```{r pairs-plot-model-start-complex}
mcmc_plot(
  model_problem,
  pars = c('b_b11', 'b_b21', 'sigma'),
  type = 'pairs',
  diag_fun = 'dens',
  off_diag_fun = 'hex',
  fixed = TRUE
)

# mcmc_pairs(
#   model_problem,
#   pars = c('b_b11', 'b_b21', 'sigma'),
#   # type = 'pairs',
#   diag_fun = 'dens',
#   off_diag_fun = 'hex',
#   np = nuts_params(model_problem),
#   condition = pairs_condition(nuts = 'divergent__')
# )
```

With problematic cases, what you might see on the off-diagonal plots is some sort of 'funneling', which would indicate where the sampler is getting stuck in the parameter space.  However, this visual notion isn't really defined, and it may be happening without being obvious, displaying just a bump[^funnelcloud].  But you'll also regularly see correlated parameters, including nonlinearly related ones, but it's unclear whether these might be a problem as well.

For the main model the pairs plot for all parameters took several seconds to produce, and even with the hex option still looks pretty poor.  It shows the intercept and `b2` parameters to be notably correlated, but there is no obvious reason for this.

```{r pairs-plot-model-start}
mcmc_plot(
  model_start,
  type = 'pairs',
  off_diag_fun = 'hex',
  diag_fun = 'dens'
)
```


#### Example: Parallel Coordinates Plot

It is also suggested to look at parallel coordinates[^parcoord].  The order of these is arbitrary, but can definitely be highly influential in your perception of them.  Also, unless everything is on similar scales, they aren't going to be very useful, but even if you scale your data in some fashion, the estimates given divergent transitions may be notably beyond a reasonable scale.  With our model, it was actually difficult to get divergent transitions, and sometimes, with a overly complex model with almost no warmup and only 15% of the data, I could get a result with no warnings whatsoever.

As near as I can tell, we'd be looking for knots where the highlighted lines converge to a point. Likewise in our pairs plot, we'd be looking for a pattern among the divergences.  If these aren't the case we're told that the divergences are probably false positives, which seems counter to the suggestion that even 1 divergent transition

```{r par-coord}
library(bayesplot)



mcmc_parcoord(
  model_start_complex,
  pars = vars(matches('^b')),
  size = .25, 
  alpha = .01,
  np = nuts_params(model_start_complex),  # without this div trans won't be highlighted
  np_style = parcoord_style_np(
    div_color = "#ff5500",
    div_size = 1,
    div_alpha = .1
  )
)
```

#### Solution for divergent transitions

Use the control argument to increase `adapt_delta` to .99[^adapt_delta] and let your model have more warmup/total iterations.

```{r adapt-delta, eval = F}
model = brm(..., control = list(adapt_delta = .99))
```

Aside from that you will have to look more deeply, including issues with priors, model specification, and more.  I find this typically comes from poor data (e.g. not scaled) combined with a complex model, and beyond that, the priors may need to be amended.


#### More

https://discourse.mc-stan.org/t/divergent-transitions-a-primer/17099


### shinystan

ADD SHINYSTAN IMG


Unfortunately the shinystan documentation in the browser doesn't really tell you what to look for in these plots.  The glossary contains information likely overly technical for applied users, and if there is a problem, there's not really a whole lot to go on.  As an example, consider the tree depth plots.  What would not be good here? The help tells you the value should be somewhere between 0 and whatever `max_treedepth` is set at.  If they are large, the documentation states it could be due to three different things, all of which suggest either reparameterization of the model (probably not unless using Stan directly), or just increasing the value.  None of the documentation tells you what those plots are supposed to look like, and unfortunately that renders them as not very useful.

The divergence and energy plots are similarly underexplained.  Many refer users to Betancourt's wonderful articles on the details, but these are far too technical for those not already steeped in the approach, and even then[^betanwhat].


### Other messages

Certain builds of rstan for certain settings (e.g. specific operating systems) will often have warnings or other messages.  Oftentimes it looks like a bunch of gobbledygook, which is typically something happening at the C++ level. If your model runs and produces output, you can typically ignore most cases. However, even then you should look it up on the forums just to be sure.

#### Parser warnings

Parser warnings are either a deprecation warning or another more serious kind (Jacobian). The latter will not happen if you're using higher level interfaces rather than programming in Stan directly. The other kind, deprecation warnings, are not something you can do anything about, but the developer of the package will likely need to make minor changes to the code to avoid them.  I've never seen parser warnings from using rstanarm or brms.


#### Compilation warnings

Compiler warnings happen regularly and indicate something going on at the compiler level, typically that something in Stan is being compiled but not used.  You can ignore these.



#### Solutions for other messages

If you are using a package to interface with Stan and not having an issue with the model (i.e. it runs), these messages can largely be ignored.


## Summary: The Practical Approach to Diagnostics

For applied analysts, I would suggest primarily focusing on the density plots and trace plots for parameters.  For the density plots of regression coefficients, these should be roughly normal looking, for variance parameters you may see skewness, especially if the estimate is relatively near zero with smaller data sets. Trace plots in general should look 'grassy' or like a 'fuzzy caterpillar', which isn't very helpful, but deviations are usually striking and obvious in my experience.  If you see chains that look like they are getting stuck around certain estimates, or separating from one another, this would indicate a problem.  If the chains are not converging with one another, you probably already were getting warnings and messages.





## Better Bayesian Approaches

We've talked about the basics one can do to run a bayesian model, and what to do if there is a problem, which is the primary goal of this post.  But it might be nice if we could avoid the problems in the first place, and our model might still be inadequate without any warnings.  So let's do some 'best practical practices' to see what we come up with


## Suggested steps

- First generate 'fake data' to assess prior viability
- With viable priors, start with a simple, but viable model
- For simple models you do not need many iterations
    - If you are doing standard GLM or simpler versions of common extensions, even the defaults are likely overkill.  For example, a basic linear regression should converge almost immediately.
- Problems
  - If Rhat is issue, run more iterations
  - if max_tree_depth is issue, increase
  - if divergent transitions
      - Check data, has it been scaled?
      - Use pairs plot
      - Use parcoord plot
      <!-- - reparameterize model -->
      - get more/better data
      - get a better model
- Use pp_check
    - Nice, but what if it doesn't fit?
        - get better data
        - get a better model
- Explore a more viable model
- Compare and/or average models

In this case, let's say we know we should at least do a linear mixed model.  You could even start with just a linear regression, but we'll say we already know we need to at least account for the heterogeneity.




## Simulate from priors

A first step is to produce some viable priors.  The basic idea here is to generate coefficients based on the prior distributions, predict data based on those coefficients, and then compare the predictions to our observed target variable.  The brms package makes this very easy to do.  We will check the following:

- Prior set 1
  - regression coefficients: Normal with mean 0, standard deviation 10 
  - Intercept: default
  - Variances: default
- Prior set 2
  - regression coefficients: Normal with mean 0, standard deviation 1
  - Intercept: default
  - Variances: default
- Prior set 3
  - regression coefficients: Normal with mean 0, standard deviation 1
  - Intercept: mean of `y`
  - Variances: default
- Prior set 4
  - regression coefficients: Normal with mean 0, standard deviation 1
  - Intercept: based on mean of `y` (~3)
  - Variances: based on sd of `y` (~1)

We'll use pp_check to examine the prior-generated data versus the observed target `y`.  I show them all together at the end.

```{r sample-prior, eval=FALSE}
# prior_summary(model_start) # examine the defaults from a previous model.

pr_norm_b_0_10 = prior(normal(0, 10), 'b')

model_0_norm_b_0_10 = brm(
  y ~ b1 + b2 + x1 + x2 + x3, 
  data = main_df,
  iter = 1000,
  sample_prior = 'only',
  prior = pr_norm_b_0_10
)

# pp_check(model_0_norm_b_0_10, nsamples = 50)

pr_norm_b_0_1 = prior(normal(0, 1), 'b')

model_0_norm_b_0_1 = brm(
  y ~ b1 + b2 + x1 + x2 + x3, 
  data = main_df,
  iter = 1000,
  sample_prior = 'only',
  prior = pr_norm_b_0_1
)

# pp_check(model_0_norm_b_0_1, nsamples = 50)



pr_norm_b_norm_int = c(
  prior(normal(0, 1), class = 'b'),
  prior(normal(3, 1), class = 'Intercept')#,
)

model_0_norm_b_0_1_norm_Int = brm(
  y ~ b1 + b2 + x1 + x2 + x3, 
  data = main_df,
  iter = 1000,
  sample_prior = 'only',
  prior = pr_norm_b_norm_int
)

# pp_check(model_0_norm_b_0_1_norm_Int, nsamples = 50)


pr_norm_b_norm_int_t_sigma = c(
  prior(normal(0, 1), class = 'b'),
  prior(normal(3, 1), class = 'Intercept'),
  prior(student_t(10, 1, 1), class = 'sigma')
)

model_0_norm_b_0_1_norm_Int_sigma = brm(
  y ~ b1 + b2 + x1 + x2 + x3, 
  data = main_df,
  iter = 1000,
  sample_prior = 'only',
  prior = pr_norm_b_norm_int_t_sigma
)

# pp_check(model_0_norm_b_0_1_norm_Int, nsamples = 50)
```


```{r sample-prior-show, eval=FALSE}
library(patchwork)
{
  pp_check(model_0_norm_b_0_10, nsamples = 50) +
    scale_x_continuous(breaks = seq(-25, 25, by = 10), limits = c(-50, 50)) +
    labs(title = 'Normal b, with defaults') +
    # theme(axis.text.x = element_text(size= 6)) +
    pp_check(model_0_norm_b_0_1, nsamples = 50) +
    scale_x_continuous(breaks = seq(-25, 25, by = 10), limits = c(-50, 50)) +
    labs(title = 'Normal b tighter')
  } /
{
  pp_check(model_0_norm_b_0_1_norm_Int, nsamples = 50) +
    scale_x_continuous(breaks = seq(-25, 25, by = 10), limits = c(-50, 50)) +
    labs(title = 'Add Int prior') +
    pp_check(model_0_norm_b_0_1_norm_Int_sigma, nsamples = 50) +
    scale_x_continuous(breaks = seq(-25, 25, by = 10), limits = c(-50, 50)) +
    labs(title = 'Add sigma prior')
  }
```

So given that our target variable is between `r round(min(main_df$y))` and `r round(max(main_df$y))`, it seems that just adding some basic, data-informed information to our priors resulted in more plausible results.  This will help our models be more efficient and better behaved.

Note that if all else fails you can use a convenience function like auto_prior demonstrated below.  From the help file:

> auto_prior() is a small, convenient function to create some default priors for brms-models with automatically adjusted prior scales, in a similar way like rstanarm does. The default scale for the intercept is 10, for coefficients 2.5. If the outcome is gaussian, both scales are multiplied with sd(y). Then, for categorical variables, nothing more is changed. For numeric variables, the scales are divided by the standard deviation of the related variable. 

Let's compare it to our previous worst case.

```{r auto-prior}
pr_auto = sjstats::auto_prior(
  y ~ b1 + b2 + x1 + x2 + x3,
  data = main_df,
  gaussian = T
)

model_auto_prior = brm(
  y ~ b1 + b2 + x1 + x2 + x3, 
  data = main_df,
  iter = 1000,
  sample_prior = 'only',
  prior = pr_auto
)

pp_check(model_auto_prior, nsamples = 50)


```

```{r default-prior, echo=FALSE}
model_default_prior = brm(
  y ~ b1 + b2 + x1 + x2 + x3, 
  data = main_df,
  iter = 1000,
  sample_prior = 'only',
  prior =  prior(uniform(-10, 10), 'b')
)

# pp_check(model_default_prior, nsamples = 50)
```


```{r proposed-priors-plot}
{
  pp_check(model_default_prior, nsamples = 50) +
    scale_x_continuous(breaks = seq(-25, 25, by = 10), limits = c(-50, 50)) +
    labs(title = 'Defaults') +
    # theme(axis.text.x = element_text(size= 6)) +
    pp_check(model_auto_prior, nsamples = 50) +
    scale_x_continuous(breaks = seq(-25, 25, by = 10), limits = c(-50, 50)) +
    labs(title = 'Auto prior')
  } /
{
  pp_check(model_0_norm_b_0_10, nsamples = 50) +
    scale_x_continuous(breaks = seq(-25, 25, by = 10), limits = c(-50, 50)) +
    labs(title = 'Normal b, with defaults') +
    # theme(axis.text.x = element_text(size= 6)) +
    pp_check(model_0_norm_b_0_1, nsamples = 50) +
    scale_x_continuous(breaks = seq(-25, 25, by = 10), limits = c(-50, 50)) +
    labs(title = 'Normal b tighter')
  } /
{
  pp_check(model_0_norm_b_0_1_norm_Int, nsamples = 50) +
    scale_x_continuous(breaks = seq(-25, 25, by = 10), limits = c(-50, 50)) +
    labs(title = 'Add Int prior') +
    pp_check(model_0_norm_b_0_1_norm_Int_sigma, nsamples = 50) +
    scale_x_continuous(breaks = seq(-25, 25, by = 10), limits = c(-50, 50)) +
    labs(title = 'Add sigma prior')
  }
```



So now let's explore the actual effects. The <span class="func" style = "">conditional_effects</span> function is what we want here, as this is what you'd likely report visually.  Without interactions or other things going, on they aren't very interesting, but it's a useful tool nonetheless.


```{r model-start-explore}
conditional_effects(model_start, 'b2')
```

```{r}
hypothesis(model_start, 'b11 + b21 > 0')
```


https://avehtari.github.io/bayes_R2/bayes_R2.html

```{r model-start-ppcheck}
bayes_R2(model_start)
pp_check(model_start, nsamples = 100)
pp_check(model_start, nsamples = 10, type ='error_scatter_avg')

q10 = function(y) quantile(y, 0.1)
q90 = function(y) quantile(y, 0.9)
pp_check(model_start, nsamples = 100, type ='stat', stat='median')
pp_check(model_start, nsamples = 100, type ='stat', stat = 'q90')
pp_check(model_start, nsamples = 100, type ='stat_2d', stat = c('q10', 'q90'))
pp_check(model_start, nsamples = 100, type ='stat', stat = 'max')

library(patchwork)
{pp_check(model_start, nsamples = 100, type ='stat', stat = 'min') +
    pp_check(model_start, nsamples = 100, type ='stat', stat = 'max')}  /
  {pp_check(model_start, nsamples = 100, type ='stat_grouped', stat = 'min', group = 'b2') + 
      guides(fill = 'none') +
    pp_check(model_start, nsamples = 100, type ='stat_grouped', stat = 'max', group = 'b2') + 
      guides(fill = 'none')}

```


We will use estimates like WAIC and loo for model comparison later.  We can use loo as a diagnostic to possibly discover

```{r}
loo(model_start)
WAIC(model_start)
```

The interesting thing here is that we have a grossly inefficient model, yet none we have nothing that notes any issues


```{r model-re}
model_re = brm(
  y ~ b1 + b2 + x1 + x2 + x3 + (1|group), 
  data = main_df
)
```









## Get the model to run

- Let it run 'long enough'
- No need to save more than necessary

## Explore the model

pp_check, hypothesis etc.

## Compare and/or average models

WAIC vs. LOO.  LOO has better diagnostics for noting whether there are potential issues using it.  But in practice, how much would it differ?

https://mc-stan.org/loo/articles/loo2-weights.html




The issues with recommendations that I see are problematic for applied users.  I'm at U of Michigan and see Stan family users from psychology, environmental sciences, engineering, and many other disciplines, and I thought it might be useful to pass along some recurring themes.

Many suggestions, while great if you have time to read several technical articles and discussions and have the ability to understand them, are simply not very helpful for applied users.  These folks probably  don't have the time (and generally no inclination) to delve into things like the details regarding divergent transitions, pareto values, etc.



https://mc-stan.org/docs/2_24/stan-users-guide/problematic-posteriors-chapter.html


## Solutions and their problems

It seems that the 'usual' approach is adding a level of complexity that is going well beyond what should be expected for new/applied users.  While some might actually enjoy such complexity (like me!), this turns applied users off, especially if there is no obvious gain over traditional approaches.

Increasing adapt_delta seems to often does not solve the problem for which it is suggested (based on personal experience it's actually rare). I usually start at .99 anyway just to make it a non-issue. For applied users, having to deal with 'control' options usually shouldn't be a first step, especially if it's not likely to improve things. For example, would trying a simpler model for diagnostic purposes, or rescaling the data, be just as likely to improve things?


Reparameterization/Change of variables
    - Only those with notably advanced statistical knowledge would even know where to begin here.

'Finding a better model' isn't usually helpful even if 100% accurate for every modeling situation.  Unless the advice is specific (e.g. to add an additional specific covariate, change a specific distribution, add some other complexity), it's likely not going to help an applied user.

'Try different priors' Which and by how does one determine 'different'?  For many regression situations, normal(0, 1) vs. normal(0, 10) vs. student(df = X) would result in basically wasted time.  Is there a way to make a reasonable guess as to what should be tested to demonstrate sensitivity/improvement?

The model comparison situation has grown so complex there is a 20+ item FAQ that still may not help in many cases.  What is someone to do if there are 'pareto issues' and they have no other obvious modeling options? Abandon model comparison?

### pareto stuff

https://avehtari.github.io/modelselection/CV-FAQ.html#16_What_to_do_if_I_have_many_high_Pareto_(k)%E2%80%99s


The usual cases are

- misspecified models MC: all models are misspecified, so how will knowing this help us finish the project?
- models with parameters which see the information only from one observation each (e.g. 'random' effect models)  MC: sorry, but this is reality for many such models.  Assuming we can't increase this, which is practically every situation, what are we to do about it?
- otherwise flexible models  MC: how is 'flexible' defined? Overly vague priors?
- [A quick note what I infer from p_loo and Pareto km values](https://discourse.mc-stan.org/t/a-quick-note-what-i-infer-from-p-loo-and-pareto-k-values/3446)
- [Recommendations for what to do when k exceeds 0.5 in the loo package?](https://discourse.mc-stan.org/t/recommendations-for-what-to-do-when-k-exceeds-0-5-in-the-loo-package/3417)
- [Improve model with some observations Pareto >0.7](https://discourse.mc-stan.org/t/improve-model-with-some-observations-pareto-0-7/17500)
- [Bayesian data analysis - roaches cross-validation demo](https://rawgit.com/avehtari/modelselection_tutorial/master/roaches.html#22_cross-validation_checking)
- [16 What to do if I have many high Pareto k’s?](https://avehtari.github.io/modelselection/CV-FAQ.html#16_What_to_do_if_I_have_many_high_Pareto_(k)%E2%80%99s)
- [Pareto K for outlier detection 1](https://discourse.mc-stan.org/t/pareto-k-for-outlier-detection/12177/9)





> These measures are not independent. If there are many high Pareto k values as in case of model 4, then elpd_loo (or looic) can’t be trusted. Even if there would be no high Pareto k values, R^2 can’t be trusted if p_loo is relatively high compared to the total number of parameters or the number of observations as in case of model 2-4. So there is no contradiction here, but you need to take into account if diagnostics tell you that some other measures can’t be used. ~ [Aki Vehtari](https://discourse.mc-stan.org/t/good-pp-check-and-r-square-but-large-pareto-k-values/17678)





Diagnostics in general are notably overexplained for applied users.  Applied users need rules of thumb, which will always be problematic, but we also can't expect a user of rstanarm to become half expert in LOO model comparison issues, causes of divergent transitions, and what amount of missing .

If the Stan group isn't high on using Bayes factors, then functions should give a warning and reference. I'm not sure if you are aware, but Krushke and others are leading many in applied disciplines to use it.


Suggesting to examine a pairs plot is essentially useless.  For typical models it is far too large (not to mention extremely slow), and furthermore, what are people supposed to look for that would actually be something they can act on?


Common problems


## Easy solutions for common messages and warnings
- Rhat too high
- bulk/tail ESS too low
- Slow





## Resources


https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations

https://mc-stan.org/docs/2_24/reference-manual/divergent-transitions.html

https://discourse.mc-stan.org/t/divergent-transitions-a-primer/17099

https://mc-stan.org/docs/2_24/reference-manual/effective-sample-size-section.html

https://mc-stan.org/bayesplot/articles/visual-mcmc-diagnostics.html

http://mc-stan.org/cmdstanr/articles/cmdstanr.html Use CmdStan to save memory


[^betanwhat]: Betancourt, whose work I admire greatly, typically makes my head spin.

[^parcoord]: I swear sometimes we're taking a step back in time with some of these plots.  All of the diagnostic plots defaults appear to be styles you'd find in Tukey's EDA from 1950.  You don't necessarily need to get fancy, but surely the defaults could be better.

[^funnelcloud]: And I say this as someone with  experience detecting funnel clouds.

[^adapt_delta]: In my experience, there isn't a need to guess between .80 and .99 as the time differences are typically negligble.  If it doesn't work at .99, it won't at .9999 either.

[^bigobjects]: With more posterior samples comes slower visualizations and possibly other computations.

[^mac]: I've had so many issues with 'just works' Macs I will almost certainly abandon them completely for my next round of computing.