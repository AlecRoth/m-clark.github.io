<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
  <title>Mixed Models for Big Data</title>
  
  <meta property="description" itemprop="description" content="Explorations of a fast penalized regression approach with bam in mgcv"/>
  
  
  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2019-10-13"/>
  <meta property="article:created" itemprop="dateCreated" content="2019-10-13"/>
  <meta name="article:author" content="Michael Clark"/>
  
  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Mixed Models for Big Data"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="Explorations of a fast penalized regression approach with bam in mgcv"/>
  <meta property="og:locale" content="en_US"/>
  
  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Mixed Models for Big Data"/>
  <meta property="twitter:description" content="Explorations of a fast penalized regression approach with bam in mgcv"/>
  
  <!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Generalized additive models : An introduction with r, second edition;citation_publication_date=2017;citation_publisher=Chapman; Hall/CRC;citation_doi=10.1201/9781315370279;citation_author=Simon N. Wood"/>
  <meta name="citation_reference" content="citation_title=Faster model matrix crossproducts for large generalized linear models with discretized covariates;citation_publication_date=2019;citation_doi=10.1007/s11222-019-09864-2;citation_issn=1573-1375;citation_author=Zheyuan Li;citation_author=Simon N. Wood"/>
  <meta name="citation_reference" content="citation_title=Generalized additive models for large data sets;citation_publication_date=2015;citation_volume=64;citation_doi=10.1111/rssc.12068;citation_issn=1467-9876;citation_author=Simon N. Wood;citation_author=Yannig Goude;citation_author=Simon Shaw"/>
  <meta name="citation_reference" content="citation_title=Generalized additive models for gigadata: Modeling the u.k. Black smoke network daily data;citation_publication_date=2017;citation_volume=112;citation_doi=10.1080/01621459.2016.1195744;citation_issn=0162-1459;citation_author=Simon N. Wood;citation_author=Zheyuan Li;citation_author=Gavin Shaddick;citation_author=Nicole H. Augustin"/>
  <meta name="citation_reference" content="citation_title=GlmmTMB balances speed and flexibility among packages for zero-inflated generalized linear mixed modeling;citation_publication_date=2017;citation_volume=9;citation_issn=2073-4859;citation_author=Mollie E. Brooks;citation_author=Kasper Kristensen;citation_author=Koen J. van Benthem;citation_author=Arni Magnusson;citation_author=Casper W. Berg;citation_author=Anders Nielsen;citation_author=Hans J. Skaug;citation_author=Martin MÃ¤chler;citation_author=Benjamin M. Bolker"/>
  <!--radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","date","preview","output","draft","bibliography","nocite","tags"]}},"value":[{"type":"character","attributes":{},"value":["Mixed Models for Big Data"]},{"type":"character","attributes":{},"value":["Explorations of a fast penalized regression approach with bam in mgcv\n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url"]}},"value":[{"type":"character","attributes":{},"value":["Michael Clark"]},{"type":"character","attributes":{},"value":["https://m-clark.github.io"]}]}]},{"type":"character","attributes":{},"value":["October 13, 2019"]},{"type":"character","attributes":{},"value":["../../img/gam_sim.png"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained","toc","css"]}},"value":[{"type":"logical","attributes":{},"value":[false]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["../../styles.css"]}]}]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["gam_references.bib"]},{"type":"character","attributes":{},"value":["@li_faster_2019, @wood_mgcv:_2012, @wood_generalized_2015, @wood_generalized_2015-1, @wood_generalized_2017-1\n"]},{"type":"character","attributes":{},"value":["bayesian","empirical bayes","shrinkage","random effects","mixed models"]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["big-mixed-models_files/bowser-1.9.3/bowser.min.js","big-mixed-models_files/distill-2.2.21/template.v2.js","big-mixed-models_files/jquery-1.11.3/jquery.min.js","big-mixed-models_files/kePrint-0.0.1/kePrint.js","big-mixed-models_files/webcomponents-2.0.0/webcomponents.js","gam_references.bib"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->
  
  <style type="text/css">
  
  body {
    background-color: white;
  }
  
  .pandoc-table {
    width: 100%;
  }
  
  .pandoc-table>caption {
    margin-bottom: 10px;
  }
  
  .pandoc-table th:not([align]) {
    text-align: left;
  }
  
  .pagedtable-footer {
    font-size: 15px;
  }
  
  .html-widget {
    margin-bottom: 2.0em;
  }
  
  .l-screen-inset {
    padding-right: 16px;
  }
  
  .l-screen .caption {
    margin-left: 10px;
  }
  
  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .shaded-content {
    background: white;
  }
  
  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }
  
  .hidden {
    display: none !important;
  }
  
  d-article {
    padding-bottom: 30px;
  }
  
  d-appendix {
    padding-top: 30px;
  }
  
  d-article>p>img {
    width: 100%;
  }
  
  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }
  
  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  /* CSS for table of contents */
  
  .d-toc {
    color: rgba(0,0,0,0.8);
    font-size: 0.8em;
    line-height: 1em;
  }
  
  .d-toc-header {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    text-transform: uppercase;
    margin-top: 0;
    margin-bottom: 1.3em;
  }
  
  .d-toc a {
    border-bottom: none;
  }
  
  .d-toc ul {
    padding-left: 0;
  }
  
  .d-toc li>ul {
    padding-top: 0.8em;
    padding-left: 16px;
    margin-bottom: 0.6em;
  }
  
  .d-toc ul,
  .d-toc li {
    list-style-type: none;
  }
  
  .d-toc li {
    margin-bottom: 0.9em;
  }
  
  .d-toc-separator {
    margin-top: 20px;
    margin-bottom: 2em;
  }
  
  .d-article-with-toc {
    border-top: none;
    padding-top: 0;
  }
  
  
  
  /* Tweak code blocks (note that this CSS is repeated above in an injection
     into the d-code shadow dom) */
  
  d-code {
    overflow-x: auto !important;
  }
  
  pre.d-code code.d-code {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }
  
  pre.text-output {
  
    font-size: 12px;
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  @media(min-width: 768px) {
  
  d-code {
    overflow-x: visible !important;
  }
  
  pre.d-code code.d-code  {
      padding-left: 18px;
      font-size: 14px;
  }
  pre.text-output {
    font-size: 14px;
  }
  }
  
  /* Figure */
  
  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }
  
  .figure img {
    width: 100%;
  }
  
  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }
  
  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }
  
  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }
  
  
  
  /* Tweak 1000px media break to show more text */
  
  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }
  
    .grid {
      grid-column-gap: 16px;
    }
  
    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }
  
  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }
  
    .grid {
      grid-column-gap: 32px;
    }
  }
  
  
  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */
  
  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }
  
  
  /* Social footer */
  
  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }
  
  .disqus-comments {
    margin-right: 30px;
  }
  
  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }
  
  #disqus_thread {
    margin-top: 30px;
  }
  
  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }
  
  .article-sharing a:hover {
    border-bottom: none;
  }
  
  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }
  
  .subscribe p {
    margin-bottom: 0.5em;
  }
  
  
  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }
  
  
  /* Improve display for browsers without grid (IE/Edge <= 15) */
  
  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }
  
  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }
  
  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }
  
  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }
  
  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }
  
  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }
  
  
  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }
  
  .downlevel .footnotes ol {
    padding-left: 13px;
  }
  
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }
  
  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }
  
  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }
  
  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  </style>
  
  <script type="application/javascript">
  
  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }
  
  // show body when load is complete
  function on_load_complete() {
  
    // set body to visible
    document.body.style.visibility = 'visible';
  
    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }
  
    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }
  
  function init_distill() {
  
    init_common();
  
    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);
  
    // create d-title
    $('.d-title').changeElementType('d-title');
  
    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);
  
    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();
  
    // move posts container into article
    $('.posts-container').appendTo($('d-article'));
  
    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');
  
    // create d-bibliography
    var bibliography = $('<d-bibliography></d-bibliography>');
    $('#distill-bibliography').wrap(bibliography);
  
    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;
  
    // replace citations with <d-cite>
    $('.citation').each(function(i, val) {
      appendix = true;
      var cites = $(this).attr('data-cites').split(" ");
      var dt_cite = $('<d-cite></d-cite>');
      dt_cite.attr('key', cites.join());
      $(this).replaceWith(dt_cite);
    });
    // remove refs
    $('#refs').remove();
  
    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();
  
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-toc a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });
  
    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');
  
    // replace code blocks with d-code
    $('pre>code').each(function(i, val) {
      var code = $(this);
      var pre = code.parent();
      var clz = "";
      var language = pre.attr('class');
      if (language) {
        // map unknown languages to "clike" (without this they just dissapear)
        if ($.inArray(language, ["bash", "clike", "css", "go", "html",
                                 "javascript", "js", "julia", "lua", "markdown",
                                 "markup", "mathml", "python", "svg", "xml"]) == -1)
          language = "clike";
        language = ' language="' + language + '"';
        var dt_code = $('<d-code block' + language + clz + '></d-code>');
        dt_code.text(code.text());
        pre.replaceWith(dt_code);
      } else {
        code.addClass('text-output').unwrap().changeElementType('pre');
      }
    });
  
    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {
  
      // capture layout
      var layout = $(this).attr('data-layout');
  
      // apply layout to markdown level block elements
      var elements = $(this).children().not('d-code, pre.text-output, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });
  
  
      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });
  
    // load distill framework
    load_distill_framework();
  
    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {
  
      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;
  
      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');
  
      // table of contents
      if (have_authors) // adjust border if we are in authors
        $('.d-toc').parent().addClass('d-article-with-toc');
  
      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');
  
      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }
  
      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');
  
      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");
  
      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }
  
       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }
  
      // inject pre code styles (can't do this with a global stylesheet b/c a shadow root is used)
      $('d-code').each(function(i, val) {
        var style = document.createElement('style');
        style.innerHTML = 'pre code { padding-left: 10px; font-size: 12px; border-left: 2px solid rgba(0,0,0,0.1); } ' +
                          '@media(min-width: 768px) { pre code { padding-left: 18px; font-size: 14px; } }';
        if (this.shadowRoot)
          this.shadowRoot.appendChild(style);
      });
  
      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();
  
      // clear polling timer
      clearInterval(tid);
  
      // show body now that everything is ready
      on_load_complete();
    }
  
    var tid = setInterval(distill_post_process, 50);
    distill_post_process();
  
  }
  
  function init_downlevel() {
  
    init_common();
  
     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));
  
    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
  
    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();
  
    // remove toc
    $('.d-toc-header').remove();
    $('.d-toc').remove();
    $('.d-toc-separator').remove();
  
    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });
  
  
    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);
  
    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);
  
    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();
  
    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();
  
    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));
  
    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });
  
    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));
  
    $('body').addClass('downlevel');
  
    on_load_complete();
  }
  
  
  function init_common() {
  
    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};
  
        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });
  
        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);
  
    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});
  
    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });
  
      }
    });
  
    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });
  
    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.distill-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }
  
    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');
  
    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");
  
    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();
  
    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }
  
  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });
  
  </script>
  
  <!--/radix_placeholder_distill-->
  <script src="big-mixed-models_files/kePrint-0.0.1/kePrint.js"></script>
  <script src="big-mixed-models_files/jquery-1.11.3/jquery.min.js"></script>
  <script src="big-mixed-models_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="big-mixed-models_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="big-mixed-models_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->

  <link rel="stylesheet" href="../../styles.css" type="text/css"/>

</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Mixed Models for Big Data","description":"Explorations of a fast penalized regression approach with bam in mgcv","authors":[{"author":"Michael Clark","authorURL":"https://m-clark.github.io","affiliation":"&nbsp;","affiliationURL":"#"}],"publishedDate":"2019-10-13T00:00:00.000-04:00","citationText":"Clark, 2019"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Mixed Models for Big Data</h1>
<p>Explorations of a fast penalized regression approach with bam in mgcv</p>
</div>

<div class="d-byline">
  Michael Clark <a href="https://m-clark.github.io" class="uri">https://m-clark.github.io</a> 
  
<br/>October 13, 2019
</div>

<div class="d-article">
<h3 class="d-toc-header">Table of Contents</h3>
<nav class="d-toc" id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#r-packages-for-mixed-models-with-large-data">R Packages for Mixed Models with Large Data</a></li>
<li><a href="#additive-models-as-mixed-models">Additive Models as Mixed Models</a><ul>
<li><a href="#comparison-of-gam-to-the-mixed-model">Comparison of GAM to the Mixed Model</a></li>
<li><a href="#the-bam-approach">The bam approach</a></li>
<li><a href="#fixed-effects-comparison">Fixed effects comparison</a></li>
<li><a href="#variance-components-comparison">Variance components comparison</a></li>
<li><a href="#estimated-random-effects">Estimated random effects</a></li>
<li><a href="#comparisons-to-bayesian-estimates">Comparisons to Bayesian Estimates</a></li>
</ul></li>
<li><a href="#back-to-the-initial-problem">Back to the initial problem</a></li>
<li><a href="#when-to-use-bam">When to use bam</a><ul>
<li><a href="#linear-mixed-models">Linear Mixed Models</a></li>
<li><a href="#generalized-linear-mixed-models">Generalized Linear Mixed Models</a></li>
<li><a href="#limitations">Limitations</a></li>
</ul></li>
<li><a href="#other-options">Other options</a></li>
<li><a href="#summary">Summary</a></li>
</ul>
</nav>
<hr class="d-toc-separator"/>
<h2 id="introduction">Introduction</h2>
<p>With mixed models, it is easy to run into data that is larger in size than some more typical data scenarios. Consider a cross-sectional data set with 200 individuals. This is fairly small data. Now, if we observe them each five times, as in a longitudinal setting, we suddenly have 1000 observations. There may be less than 200 countries in the world, but if we survey 100s or 1000s of people in many of them, we suddenly have a notable data set size, and still would potentially like to model a country-level random effect. What are our options when dealing with possibly gigabytes of data?</p>
<p>Background required:</p>
<p>For the following you should have familiarity with <a href="https://m-clark.github.io/mixed-models-with-R/">mixed models</a>. Knowledge of the lme4 package would be useful but isnât required. Likewise, knowledge of <a href="https://m-clark.github.io/generalized-additive-models/">generalized additive models</a> and mgcv would be helpful, but I donât think itâs required to follow the demonstration.</p>
<h2 id="r-packages-for-mixed-models-with-large-data">R Packages for Mixed Models with Large Data</h2>
<p>While many tools abound to conduct mixed models for larger data sizes, their limitations can be found pretty quickly. Râs lme4 is a standard, but powerful mixed model tool. More to the point, it is very computationally efficient, such that it can handle very large sample sizes for simpler mixed models. For linear mixed models this can include hundreds of thousands of observations with possibly multiple random effects, still running on a basic laptop. For such models, itâs still largely the tool of choice, and its approach has even been copied/ported into other statistical packages.</p>
<p>Weâll first create some data to model. This is just a simple random intercepts setting.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
set.seed(12358)
N = 1e6
n_groups = 1000
g = rep(1:n_groups, e = N/n_groups)

x = rnorm(N)                             # an observation level continuous variable
b = rbinom(n_groups, size = 1, prob=.5)  # a cluster level categorical variable
b = b[g]

sd_g = .5     # standard deviation for the random effect
sigma = 1     # standard deviation for the observation

re0 = rnorm(n_groups, sd = sd_g)  # random effects
re  = re0[g]

lp = 0 + .5*x + .25*b + re        # linear predictor 

y = rnorm(N, mean = lp, sd = sigma)               # create a continuous target variable
y_bin = rbinom(N, size = 1, prob = plogis(lp))    # create a binary target variable

d = tibble(x, b, y, y_bin, g = factor(g))</code></pre>
</div>
<p><br></p>
<p>Letâs take a look at the data first.</p>
<div class="layout-chunk" data-layout="l-body">
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
index
</th>
<th style="text-align:right;">
x
</th>
<th style="text-align:right;">
b
</th>
<th style="text-align:right;">
y
</th>
<th style="text-align:right;">
y_bin
</th>
<th style="text-align:left;">
g
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
-0.378
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
-0.278
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
-0.812
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
-0.343
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.218
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
-0.810
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
1.529
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.465
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
-1.877
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
-1.570
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
-0.427
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.047
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
999995
</td>
<td style="text-align:right;">
-1.181
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
-1.111
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
1000
</td>
</tr>
<tr>
<td style="text-align:right;">
999996
</td>
<td style="text-align:right;">
-1.487
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.563
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
1000
</td>
</tr>
<tr>
<td style="text-align:right;">
999997
</td>
<td style="text-align:right;">
-1.236
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
-0.603
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
1000
</td>
</tr>
<tr>
<td style="text-align:right;">
999998
</td>
<td style="text-align:right;">
0.412
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.736
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
1000
</td>
</tr>
<tr>
<td style="text-align:right;">
999999
</td>
<td style="text-align:right;">
-0.644
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
-1.257
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
1000
</td>
</tr>
<tr>
<td style="text-align:right;">
1000000
</td>
<td style="text-align:right;">
0.409
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.520
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
1000
</td>
</tr>
</tbody>
</table>
</div>
<p>Now with the data in place, letâs try lme4 to model the continuous outcome.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(lme4)

system.time({
  mixed_big = lmer(y ~ x + b + (1|g))
})</code></pre>
<pre><code>
   user  system elapsed 
  5.699   0.591   6.399 </code></pre>
<pre class="r"><code>
summary(mixed_big, cor = FALSE)</code></pre>
<pre><code>
Linear mixed model fit by REML [&#39;lmerMod&#39;]
Formula: y ~ x + b + (1 | g)

REML criterion at convergence: 2841256

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.6066 -0.6743 -0.0004  0.6744  5.0367 

Random effects:
 Groups   Name        Variance Std.Dev.
 g        (Intercept) 0.2509   0.5009  
 Residual             0.9978   0.9989  
Number of obs: 1000000, groups:  g, 1000

Fixed effects:
             Estimate Std. Error t value
(Intercept) 0.0364754  0.0223332   1.633
x           0.5017616  0.0009987 502.409
b           0.1904534  0.0317430   6.000</code></pre>
</div>
<p>This is great! We just ran a mixed model for 1,000,000 observations and 1,000 groups for our random effect in just a few seconds.</p>
<p>But the problem comes as soon as you move to the generalized mixed model, e.g.Â having a binary outcome, or include additional complexity while still dealing with large data. The following is essentially the same model, but for a binary outcome.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
system.time({
  mixed_big_glmm = glmer(y_bin ~ x + b + (1|g), family = binomial)
})</code></pre>
<pre><code>
   user  system elapsed 
 86.422  17.156 103.867 </code></pre>
</div>
<p>To begin with, you shouldnât be worried about models taking a few minutes to run, or even a couple hours. Once you have your model(s) squared away, the testing of which can be done on a smaller sample of the data set, there is no need to repeatedly run it. But in this case we had a greater than 15 fold increase in time for a very simple data scenario. So itâs good to have options when you need them. Letâs turn to those.</p>
<h2 id="additive-models-as-mixed-models">Additive Models as Mixed Models</h2>
<p>Simon Woodâs wonderful work on generalized additive models (GAM) and the mgcv package make it one of the better modeling tools in the R kingdom. As his text<span class="citation" data-cites="wood_generalized_2017">(S. N. Wood <a href="#ref-wood_generalized_2017">2017</a>)</span> and other work shows, additive models can be seen as random effects models, and he exploits this by providing numerous ways to include and explore random effects in the GAM approach. One key difference between the GAM and a standard mixed model approach is that the random effects are estimated parameters of the model, not BLUPs, as with lme4. Those coefficients are penalized, pretty much in the same was as L2/ridge regression. The âfixed effectsâ are not penalized, and so that part is basically just a generalized linear model. As we will see though, the results will be nearly the same between mgcv and lme4.</p>
<p>The following demonstrates the link between the approaches by showing a model that includes a random intercept and slope. We will use the standard mgcv approach for specifying a smooth term, but alternatives are shown for those familiar with the package.</p>
<aside>
If you just use <span class="func" style="">coef</span> on the following gam objects, you will see that the random effects are lumped in with the other estimated coefficients.
</aside>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(lme4)
library(mgcv)

mixed_model = lmer(
  Reaction ~ Days + (1 | Subject) + (0 + Days | Subject),
  data = sleepstudy
)

ga_model = gam(
  Reaction ~  Days + s(Subject, bs = &#39;re&#39;) + s(Days, Subject, bs = &#39;re&#39;),
  data = sleepstudy,
  method = &#39;REML&#39;
)

# Using gamm and gamm4 for the same model
# ga_model = gamm(
#   Reaction ~  Days ,
#   random = list(Subject = ~ 0 + Days),
#   data = sleepstudy,
#   method = &#39;REML&#39;
# )
# 
# ga_model = gamm4::gamm4(
#   Reaction ~  Days,
#   random =  ~ (Days||Subject),
#   data = sleepstudy,
#   REML = TRUE
# )</code></pre>
</div>
<p>Note that we use <span class="func" style="">s</span> to denote a <span class="emph" style="">smooth term</span> in the parlance of additive models, and the <code>bs = 're'</code> specifies that we want it as a random effect (as opposed to a spline or other basis function). The second smooth term <code>s(Days, Subject, bs = 're')</code> denotes random coefficients for the <code>Days</code> covariate.</p>
<aside>
As shown, one could use the <span class="func" style="">gamm</span> function for the <span class="pack" style="">nlme</span> style, or Woodâs <span class="pack" style="">gamm4</span> package to use the lme4 syntax. These alternate approaches allow for more flexibility in some ways, but will not be useful to us for big data.
</aside>
<h3 id="comparison-of-gam-to-the-mixed-model">Comparison of GAM to the Mixed Model</h3>
<p>Aside from the syntax, the underlying model between the two is the same, and the following shose that we obtain the same results for both lme4 and mgcv.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
summary(mixed_model, cor = FALSE)</code></pre>
<pre><code>
Linear mixed model fit by REML [&#39;lmerMod&#39;]
Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
   Data: sleepstudy

REML criterion at convergence: 1743.7

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.9626 -0.4626  0.0204  0.4653  5.1860 

Random effects:
 Groups    Name        Variance Std.Dev.
 Subject   (Intercept) 627.50   25.050  
 Subject.1 Days         35.86    5.989  
 Residual              653.58   25.565  
Number of obs: 180, groups:  Subject, 18

Fixed effects:
            Estimate Std. Error t value
(Intercept)  251.405      6.885  36.514
Days          10.467      1.560   6.711</code></pre>
<pre class="r"><code>
summary(ga_model)</code></pre>
<pre><code>
Family: gaussian 
Link function: identity 

Formula:
Reaction ~ Days + s(Subject, bs = &quot;re&quot;) + s(Days, Subject, bs = &quot;re&quot;)

Parametric coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  251.405      6.885  36.513  &lt; 2e-16 ***
Days          10.467      1.560   6.712 3.67e-10 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Approximate significance of smooth terms:
                  edf Ref.df      F  p-value    
s(Subject)      12.94     17  89.29 4.56e-07 ***
s(Days,Subject) 14.41     17 104.56 1.82e-12 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

R-sq.(adj) =  0.794   Deviance explained = 82.7%
-REML = 871.83  Scale est. = 653.58    n = 180</code></pre>
</div>
<p>I donât want to go into the details of the printout for mgcv, but it is worth noting that the parametric part is equivalent to the fixed effects portion of the lme4 output. Likewise the smooth terms output is related to the random effects, but weâll extract them in a manner more suited to typical mixed model output instead. So letâs compare the variance components, and get them ready for later comparison to bam results. Note, Iâve been using mgcv a lot for mixed models lately, so I created a package called <span class="pack" style="">gammit</span> to provide tidier output in general, and which is more similar to lme4. I note the corresponding mgcv function where appropriate.</p>
<aside>
The <span class="pack" style="">gammit</span> package is available on <a href="https://github.com/m-clark/gammit">GitHub</a>.
</aside>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(gammit)

# extract just the fixed effects for later.
mixed_fe = fixef(mixed_model)
gam_fe   = extract_fixed(ga_model)  # coefs with se and confidence interval

# variance components
lmer_vcov = data.frame(VarCorr(mixed_model))
gam_vcov  = extract_vc(ga_model)    # cleaner gam.vcomp</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:initial-vc-show">Table 1: </span>LME Result
</caption>
<thead>
<tr>
<th style="text-align:left;">
grp
</th>
<th style="text-align:left;">
var1
</th>
<th style="text-align:left;">
var2
</th>
<th style="text-align:right;">
vcov
</th>
<th style="text-align:right;">
sdcor
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:left;">
NA
</td>
<td style="text-align:right;">
627.500
</td>
<td style="text-align:right;">
25.050
</td>
</tr>
<tr>
<td style="text-align:left;">
Subject.1
</td>
<td style="text-align:left;">
Days
</td>
<td style="text-align:left;">
NA
</td>
<td style="text-align:right;">
35.864
</td>
<td style="text-align:right;">
5.989
</td>
</tr>
<tr>
<td style="text-align:left;">
Residual
</td>
<td style="text-align:left;">
NA
</td>
<td style="text-align:left;">
NA
</td>
<td style="text-align:right;">
653.580
</td>
<td style="text-align:right;">
25.565
</td>
</tr>
</tbody>
</table>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:initial-vc-show">Table 1: </span>GAM Result
</caption>
<thead>
<tr>
<th style="text-align:left;">
component
</th>
<th style="text-align:right;">
std.dev
</th>
<th style="text-align:right;">
lower
</th>
<th style="text-align:right;">
upper
</th>
<th style="text-align:right;">
variance
</th>
<th style="text-align:right;">
proportion
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
25.051
</td>
<td style="text-align:right;">
16.085
</td>
<td style="text-align:right;">
39.015
</td>
<td style="text-align:right;">
627.571
</td>
<td style="text-align:right;">
0.477
</td>
</tr>
<tr>
<td style="text-align:left;">
Days|Subject
</td>
<td style="text-align:right;">
5.988
</td>
<td style="text-align:right;">
4.025
</td>
<td style="text-align:right;">
8.908
</td>
<td style="text-align:right;">
35.858
</td>
<td style="text-align:right;">
0.027
</td>
</tr>
<tr>
<td style="text-align:left;">
scale
</td>
<td style="text-align:right;">
25.565
</td>
<td style="text-align:right;">
22.792
</td>
<td style="text-align:right;">
28.676
</td>
<td style="text-align:right;">
653.582
</td>
<td style="text-align:right;">
0.496
</td>
</tr>
</tbody>
</table>
</div>
<aside>
The penalty parameter in the GAM model is inversely related to the variance estimate of the random effects. See <a href="https://m-clark.github.io/generalized-additive-models/appendix.html#a-comparison-to-mixed-models">this demo</a>.
</aside>
<h3 id="the-bam-approach">The bam approach</h3>
<p>For large data, mgcv provides the bam function. For this small data setting we donât really need it, but we can establish that we would get similar results using it without having to wait. We will see the benefits when we apply bam to large data later. None of our syntax changes, just the function.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
ba_model = bam(Reaction ~  Days + s(Subject, bs=&#39;re&#39;) + s(Days, Subject, bs=&#39;re&#39;), 
               data = sleepstudy)

bam_fe   = extract_fixed(ba_model)
bam_vcov = extract_vc(ba_model)</code></pre>
</div>
<p>How does it work? The function uses a parallelized approach where possible, essentially working on subsets of the model matrices simultaneously. Details can be found in the references<span class="citation" data-cites="li_faster_2019">(Li and Wood <a href="#ref-li_faster_2019">2019</a>)</span><span class="citation" data-cites="wood_generalized_2015">(S. N. Wood, Goude, and Shaw <a href="#ref-wood_generalized_2015">2015</a><a href="#ref-wood_generalized_2015">a</a>)</span><span class="citation" data-cites="wood_generalized_2017-1">(S. N. Wood et al. <a href="#ref-wood_generalized_2017-1">2017</a>)</span>, but basically mgcv parallelizes the parts that can be, and additionally provides an option to discretize the data to work with the minimal information necessary to produce viable estimates. The following uses the discrete option. As there isnât really anything to discretize with so little data, this is just to demonstrate the syntax.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
ba_d_model = bam(Reaction ~  Days + s(Subject, bs=&#39;re&#39;) + s(Days, Subject, bs=&#39;re&#39;), 
                 data = sleepstudy,
                 discrete = TRUE)

bam_d_fe   = extract_fixed(ba_d_model)
bam_d_vcov = extract_vc(ba_d_model)</code></pre>
</div>
<h3 id="fixed-effects-comparison">Fixed effects comparison</h3>
<p>We start by comparing the fixed effects of all models run thus far. No surprises here, the results are the same.</p>
<div class="layout-chunk" data-layout="l-body">
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:compare-fixef">Table 2: </span>Fixed Effects Estimates
</caption>
<thead>
<tr>
<th style="text-align:left;">
Model
</th>
<th style="text-align:right;">
Intercept
</th>
<th style="text-align:right;">
Days
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
mixed
</td>
<td style="text-align:right;">
251.405
</td>
<td style="text-align:right;">
10.467
</td>
</tr>
<tr>
<td style="text-align:left;">
gam
</td>
<td style="text-align:right;">
251.405
</td>
<td style="text-align:right;">
10.467
</td>
</tr>
<tr>
<td style="text-align:left;">
bam
</td>
<td style="text-align:right;">
251.405
</td>
<td style="text-align:right;">
10.467
</td>
</tr>
<tr>
<td style="text-align:left;">
bam_d
</td>
<td style="text-align:right;">
251.405
</td>
<td style="text-align:right;">
10.467
</td>
</tr>
</tbody>
</table>
</div>
<p>Letâs examine the standard errors. Note that there are options for the gam models for standard error estimation, including a Bayesian one. For more details, see <code>?gamObject</code>, but I will offer the summary:</p>
<h5 id="ve">Ve</h5>
<p>frequentist estimated covariance matrix for the parameter estimators. Particularly useful for testing whether terms are zero. Not so useful for CIâs as smooths are usually biased.</p>
<h5 id="vp">Vp</h5>
<p>estimated covariance matrix for the parameters. This is a Bayesian posterior covariance matrix that results from adopting a particular Bayesian model of the smoothing process. Paricularly useful for creating credible/confidence intervals.</p>
<h5 id="vc">Vc</h5>
<p>Under ML or REML smoothing parameter estimation it is possible to correct the covariance matrix Vp for smoothing parameter uncertainty. This is the corrected version.</p>
<p>We will use the Bayesian estimates (<code>Vp</code>), but for this setting there are no appreciable differences. I expand the digits to show they are in fact different to some decimal place.</p>
<div class="layout-chunk" data-layout="l-body">
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:compare-fixef-se">Table 3: </span>Fixed Effects Standard Errors
</caption>
<thead>
<tr>
<th style="text-align:left;">
Model
</th>
<th style="text-align:right;">
Intercept
</th>
<th style="text-align:right;">
Days
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
mixed
</td>
<td style="text-align:right;">
6.88510
</td>
<td style="text-align:right;">
1.55967
</td>
</tr>
<tr>
<td style="text-align:left;">
gam
</td>
<td style="text-align:right;">
6.88540
</td>
<td style="text-align:right;">
1.55956
</td>
</tr>
<tr>
<td style="text-align:left;">
bam
</td>
<td style="text-align:right;">
6.88538
</td>
<td style="text-align:right;">
1.55957
</td>
</tr>
<tr>
<td style="text-align:left;">
bam_d
</td>
<td style="text-align:right;">
6.88538
</td>
<td style="text-align:right;">
1.55957
</td>
</tr>
</tbody>
</table>
</div>
<h3 id="variance-components-comparison">Variance components comparison</h3>
<p>Now we move to the variance component estimates. Reported are the standard deviations for subject level random effects for intercept, <code>Days</code> coefficient, and residual.</p>
<div class="layout-chunk" data-layout="l-body">
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:compare-vc">Table 4: </span>Variance Components Estimates
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Intercept
</th>
<th style="text-align:right;">
Days
</th>
<th style="text-align:right;">
Residual
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
mixed
</td>
<td style="text-align:right;">
25.050
</td>
<td style="text-align:right;">
5.989
</td>
<td style="text-align:right;">
25.565
</td>
</tr>
<tr>
<td style="text-align:left;">
gam
</td>
<td style="text-align:right;">
25.051
</td>
<td style="text-align:right;">
5.988
</td>
<td style="text-align:right;">
25.565
</td>
</tr>
<tr>
<td style="text-align:left;">
bam
</td>
<td style="text-align:right;">
25.051
</td>
<td style="text-align:right;">
5.988
</td>
<td style="text-align:right;">
25.565
</td>
</tr>
<tr>
<td style="text-align:left;">
bam_d
</td>
<td style="text-align:right;">
25.051
</td>
<td style="text-align:right;">
5.988
</td>
<td style="text-align:right;">
25.565
</td>
</tr>
</tbody>
</table>
</div>
<p>We can also look at their interval estimates. We use the profile likelihood for the lme4 mixed model. In this case we can see slightly wider and somewhat different boundary estimates.</p>
<div class="layout-chunk" data-layout="l-body">

</div>
<div class="layout-chunk" data-layout="l-body">
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:compare-vc-int-table">Table 5: </span>Interval Estimates for Variance Components
</caption>
<thead>
<tr>
<th style="text-align:left;">
Model
</th>
<th style="text-align:left;">
component
</th>
<th style="text-align:right;">
lower
</th>
<th style="text-align:right;">
upper
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;vertical-align: top !important;" rowspan="3">
mixed
</td>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:right;">
15.2587
</td>
<td style="text-align:right;">
37.7865
</td>
</tr>
<tr>
<td style="text-align:left;">
Days
</td>
<td style="text-align:right;">
3.9641
</td>
<td style="text-align:right;">
8.7692
</td>
</tr>
<tr>
<td style="text-align:left;">
Residual
</td>
<td style="text-align:right;">
22.8806
</td>
<td style="text-align:right;">
28.7876
</td>
</tr>
<tr>
<td style="text-align:left;vertical-align: top !important;" rowspan="3">
gam
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
16.0854
</td>
<td style="text-align:right;">
39.0150
</td>
</tr>
<tr>
<td style="text-align:left;">
Days|Subject
</td>
<td style="text-align:right;">
4.0252
</td>
<td style="text-align:right;">
8.9083
</td>
</tr>
<tr>
<td style="text-align:left;">
Residual
</td>
<td style="text-align:right;">
22.7917
</td>
<td style="text-align:right;">
28.6763
</td>
</tr>
<tr>
<td style="text-align:left;vertical-align: top !important;" rowspan="3">
bam
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
16.0853
</td>
<td style="text-align:right;">
39.0150
</td>
</tr>
<tr>
<td style="text-align:left;">
Days|Subject
</td>
<td style="text-align:right;">
4.0253
</td>
<td style="text-align:right;">
8.9083
</td>
</tr>
<tr>
<td style="text-align:left;">
Residual
</td>
<td style="text-align:right;">
22.7918
</td>
<td style="text-align:right;">
28.6763
</td>
</tr>
<tr>
<td style="text-align:left;vertical-align: top !important;" rowspan="3">
bam_d
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
16.0853
</td>
<td style="text-align:right;">
39.0150
</td>
</tr>
<tr>
<td style="text-align:left;">
Days|Subject
</td>
<td style="text-align:right;">
4.0253
</td>
<td style="text-align:right;">
8.9083
</td>
</tr>
<tr>
<td style="text-align:left;">
Residual
</td>
<td style="text-align:right;">
22.7918
</td>
<td style="text-align:right;">
28.6763
</td>
</tr>
</tbody>
</table>
</div>
<h3 id="estimated-random-effects">Estimated random effects</h3>
<p>Now letâs look at the random effect estimates.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
mixed_re = ranef(mixed_model)[[1]] %&gt;% 
  rename(mixed_Subject = `(Intercept)`, `mixed_Days|Subject` = Days)

gam_re_init   = extract_ranef(ga_model)
bam_re_init   = extract_ranef(ba_model)
bam_d_re_init = extract_ranef(ba_d_model)</code></pre>
</div>
<p>Weâll start with the random effects for the intercept. To several decimal places, we start to see differences, so again we know they arenât doing exactly the same thing.</p>
<div class="layout-chunk" data-layout="l-body">
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:re-int">Table 6: </span>Estimated Random Effects
</caption>
<thead>
<tr>
<th style="text-align:right;">
mixed_Subject
</th>
<th style="text-align:right;">
gam_Subject
</th>
<th style="text-align:right;">
bam_Subject
</th>
<th style="text-align:right;">
bam_d_Subject
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1.51170
</td>
<td style="text-align:right;">
1.51272
</td>
<td style="text-align:right;">
1.51270
</td>
<td style="text-align:right;">
1.51270
</td>
</tr>
<tr>
<td style="text-align:right;">
-40.37201
</td>
<td style="text-align:right;">
-40.37397
</td>
<td style="text-align:right;">
-40.37390
</td>
<td style="text-align:right;">
-40.37390
</td>
</tr>
<tr>
<td style="text-align:right;">
-39.17951
</td>
<td style="text-align:right;">
-39.18111
</td>
<td style="text-align:right;">
-39.18104
</td>
<td style="text-align:right;">
-39.18104
</td>
</tr>
<tr>
<td style="text-align:right;">
24.51881
</td>
<td style="text-align:right;">
24.51893
</td>
<td style="text-align:right;">
24.51890
</td>
<td style="text-align:right;">
24.51890
</td>
</tr>
<tr>
<td style="text-align:right;">
22.91419
</td>
<td style="text-align:right;">
22.91446
</td>
<td style="text-align:right;">
22.91443
</td>
<td style="text-align:right;">
22.91443
</td>
</tr>
<tr>
<td style="text-align:right;">
9.22178
</td>
<td style="text-align:right;">
9.22199
</td>
<td style="text-align:right;">
9.22197
</td>
<td style="text-align:right;">
9.22197
</td>
</tr>
<tr>
<td style="text-align:right;">
17.15572
</td>
<td style="text-align:right;">
17.15614
</td>
<td style="text-align:right;">
17.15612
</td>
<td style="text-align:right;">
17.15612
</td>
</tr>
<tr>
<td style="text-align:right;">
-7.45166
</td>
<td style="text-align:right;">
-7.45174
</td>
<td style="text-align:right;">
-7.45173
</td>
<td style="text-align:right;">
-7.45173
</td>
</tr>
<tr>
<td style="text-align:right;">
0.57984
</td>
<td style="text-align:right;">
0.57870
</td>
<td style="text-align:right;">
0.57872
</td>
<td style="text-align:right;">
0.57872
</td>
</tr>
<tr>
<td style="text-align:right;">
34.76617
</td>
<td style="text-align:right;">
34.76800
</td>
<td style="text-align:right;">
34.76793
</td>
<td style="text-align:right;">
34.76793
</td>
</tr>
<tr>
<td style="text-align:right;">
-25.75382
</td>
<td style="text-align:right;">
-25.75436
</td>
<td style="text-align:right;">
-25.75432
</td>
<td style="text-align:right;">
-25.75432
</td>
</tr>
<tr>
<td style="text-align:right;">
-13.86539
</td>
<td style="text-align:right;">
-13.86504
</td>
<td style="text-align:right;">
-13.86504
</td>
<td style="text-align:right;">
-13.86504
</td>
</tr>
<tr>
<td style="text-align:right;">
4.91618
</td>
<td style="text-align:right;">
4.91598
</td>
<td style="text-align:right;">
4.91598
</td>
<td style="text-align:right;">
4.91598
</td>
</tr>
<tr>
<td style="text-align:right;">
20.92816
</td>
<td style="text-align:right;">
20.92908
</td>
<td style="text-align:right;">
20.92904
</td>
<td style="text-align:right;">
20.92904
</td>
</tr>
<tr>
<td style="text-align:right;">
3.25848
</td>
<td style="text-align:right;">
3.25865
</td>
<td style="text-align:right;">
3.25865
</td>
<td style="text-align:right;">
3.25865
</td>
</tr>
<tr>
<td style="text-align:right;">
-26.47568
</td>
<td style="text-align:right;">
-26.47585
</td>
<td style="text-align:right;">
-26.47583
</td>
<td style="text-align:right;">
-26.47583
</td>
</tr>
<tr>
<td style="text-align:right;">
0.90573
</td>
<td style="text-align:right;">
0.90565
</td>
<td style="text-align:right;">
0.90565
</td>
<td style="text-align:right;">
0.90565
</td>
</tr>
<tr>
<td style="text-align:right;">
12.42132
</td>
<td style="text-align:right;">
12.42178
</td>
<td style="text-align:right;">
12.42176
</td>
<td style="text-align:right;">
12.42176
</td>
</tr>
</tbody>
</table>
</div>
<p>Random effects for the Days coefficient.</p>
<div class="layout-chunk" data-layout="l-body">
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:re-days">Table 7: </span>Estimated Random Intercepts
</caption>
<thead>
<tr>
<th style="text-align:right;">
mixed_Days|Subject
</th>
<th style="text-align:right;">
gam_Days|Subject
</th>
<th style="text-align:right;">
bam_Days|Subject
</th>
<th style="text-align:right;">
bam_d_Days|Subject
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
9.32373
</td>
<td style="text-align:right;">
9.32348
</td>
<td style="text-align:right;">
9.32349
</td>
<td style="text-align:right;">
9.32349
</td>
</tr>
<tr>
<td style="text-align:right;">
-8.59954
</td>
<td style="text-align:right;">
-8.59916
</td>
<td style="text-align:right;">
-8.59917
</td>
<td style="text-align:right;">
-8.59917
</td>
</tr>
<tr>
<td style="text-align:right;">
-5.38807
</td>
<td style="text-align:right;">
-5.38778
</td>
<td style="text-align:right;">
-5.38779
</td>
<td style="text-align:right;">
-5.38779
</td>
</tr>
<tr>
<td style="text-align:right;">
-4.96868
</td>
<td style="text-align:right;">
-4.96865
</td>
<td style="text-align:right;">
-4.96865
</td>
<td style="text-align:right;">
-4.96865
</td>
</tr>
<tr>
<td style="text-align:right;">
-3.19393
</td>
<td style="text-align:right;">
-3.19394
</td>
<td style="text-align:right;">
-3.19393
</td>
<td style="text-align:right;">
-3.19393
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.30847
</td>
<td style="text-align:right;">
-0.30850
</td>
<td style="text-align:right;">
-0.30849
</td>
<td style="text-align:right;">
-0.30849
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.28715
</td>
<td style="text-align:right;">
-0.28721
</td>
<td style="text-align:right;">
-0.28721
</td>
<td style="text-align:right;">
-0.28721
</td>
</tr>
<tr>
<td style="text-align:right;">
1.11599
</td>
<td style="text-align:right;">
1.11599
</td>
<td style="text-align:right;">
1.11599
</td>
<td style="text-align:right;">
1.11599
</td>
</tr>
<tr>
<td style="text-align:right;">
-10.90624
</td>
<td style="text-align:right;">
-10.90596
</td>
<td style="text-align:right;">
-10.90597
</td>
<td style="text-align:right;">
-10.90597
</td>
</tr>
<tr>
<td style="text-align:right;">
8.62796
</td>
<td style="text-align:right;">
8.62760
</td>
<td style="text-align:right;">
8.62762
</td>
<td style="text-align:right;">
8.62762
</td>
</tr>
<tr>
<td style="text-align:right;">
1.28063
</td>
<td style="text-align:right;">
1.28069
</td>
<td style="text-align:right;">
1.28069
</td>
<td style="text-align:right;">
1.28069
</td>
</tr>
<tr>
<td style="text-align:right;">
6.75652
</td>
<td style="text-align:right;">
6.75640
</td>
<td style="text-align:right;">
6.75640
</td>
<td style="text-align:right;">
6.75640
</td>
</tr>
<tr>
<td style="text-align:right;">
-3.07519
</td>
<td style="text-align:right;">
-3.07513
</td>
<td style="text-align:right;">
-3.07513
</td>
<td style="text-align:right;">
-3.07513
</td>
</tr>
<tr>
<td style="text-align:right;">
3.51238
</td>
<td style="text-align:right;">
3.51220
</td>
<td style="text-align:right;">
3.51221
</td>
<td style="text-align:right;">
3.51221
</td>
</tr>
<tr>
<td style="text-align:right;">
0.87308
</td>
<td style="text-align:right;">
0.87305
</td>
<td style="text-align:right;">
0.87305
</td>
<td style="text-align:right;">
0.87305
</td>
</tr>
<tr>
<td style="text-align:right;">
4.98381
</td>
<td style="text-align:right;">
4.98379
</td>
<td style="text-align:right;">
4.98379
</td>
<td style="text-align:right;">
4.98379
</td>
</tr>
<tr>
<td style="text-align:right;">
-1.00532
</td>
<td style="text-align:right;">
-1.00529
</td>
<td style="text-align:right;">
-1.00529
</td>
<td style="text-align:right;">
-1.00529
</td>
</tr>
<tr>
<td style="text-align:right;">
1.25848
</td>
<td style="text-align:right;">
1.25840
</td>
<td style="text-align:right;">
1.25840
</td>
<td style="text-align:right;">
1.25840
</td>
</tr>
</tbody>
</table>
</div>
<p>Standard errors for the random effects. In the balanced design these are essentially constant across clusters. We can see that the Bayesian estimates from mgcv reflect greater uncertainty.</p>
<aside>
The bam results may actually be slightly different for some clusters.
</aside>
<p><br></p>
<div class="layout-chunk" data-layout="l-body">
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:re-int-se">Table 8: </span>Standard Errors of the Random Coefficients
</caption>
<thead>
<tr>
<th style="text-align:left;">
Model
</th>
<th style="text-align:right;">
Intercepts
</th>
<th style="text-align:right;">
Days
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
mixed
</td>
<td style="text-align:right;">
12.239
</td>
<td style="text-align:right;">
2.335
</td>
</tr>
<tr>
<td style="text-align:left;">
gam
</td>
<td style="text-align:right;">
13.279
</td>
<td style="text-align:right;">
2.673
</td>
</tr>
<tr>
<td style="text-align:left;">
bam
</td>
<td style="text-align:right;">
13.279
</td>
<td style="text-align:right;">
2.673
</td>
</tr>
<tr>
<td style="text-align:left;">
bam_discrete
</td>
<td style="text-align:right;">
13.279
</td>
<td style="text-align:right;">
2.673
</td>
</tr>
</tbody>
</table>
</div>
<h3 id="comparisons-to-bayesian-estimates">Comparisons to Bayesian Estimates</h3>
<p>As we have noted, one of the differences between lme4 and mgcv output is that the default uncertainty estimates for the GAM are Bayesian. As such, it might be interesting to compare these to a fully Bayes approach. Weâll use <span class="pack" style="">rstanarm</span>, which uses the <span class="pack" style="">lme4</span> style syntax.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(rstanarm)
bayes = stan_lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject), 
                  data=sleepstudy,
                  cores = 4)

bayes_fe = broom::tidy(bayes)
bayes_vc = broom::tidy(bayes, &#39;hierarchical&#39;)
bayes_re = broom::tidy(bayes, &#39;varying&#39;)</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:bayes-compare">Table 9: </span>Bayesian fixed effects and variance components
</caption>
<thead>
<tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
std.error
</th>
<th style="text-align:left;">
group
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
251.240
</td>
<td style="text-align:right;">
6.844
</td>
<td style="text-align:left;">
NA
</td>
</tr>
<tr>
<td style="text-align:left;">
Days
</td>
<td style="text-align:right;">
10.439
</td>
<td style="text-align:right;">
1.689
</td>
<td style="text-align:left;">
NA
</td>
</tr>
<tr>
<td style="text-align:left;">
sd_(Intercept).Subject
</td>
<td style="text-align:right;">
26.761
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:left;">
Subject
</td>
</tr>
<tr>
<td style="text-align:left;">
sd_Days.Subject
</td>
<td style="text-align:right;">
6.598
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:left;">
Subject
</td>
</tr>
<tr>
<td style="text-align:left;">
sd_Observation.Residual
</td>
<td style="text-align:right;">
25.762
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:left;">
Residual
</td>
</tr>
</tbody>
</table>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:bayes-compare">Table 9: </span>Bayesian random effects
</caption>
<thead>
<tr>
<th style="text-align:left;">
level
</th>
<th style="text-align:left;">
group
</th>
<th style="text-align:right;">
(Intercept)
</th>
<th style="text-align:right;">
Days
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
308
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
1.477
</td>
<td style="text-align:right;">
9.375
</td>
</tr>
<tr>
<td style="text-align:left;">
309
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
-39.941
</td>
<td style="text-align:right;">
-8.519
</td>
</tr>
<tr>
<td style="text-align:left;">
310
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
-38.457
</td>
<td style="text-align:right;">
-5.374
</td>
</tr>
<tr>
<td style="text-align:left;">
330
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
23.780
</td>
<td style="text-align:right;">
-4.759
</td>
</tr>
<tr>
<td style="text-align:left;">
331
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
22.666
</td>
<td style="text-align:right;">
-3.080
</td>
</tr>
<tr>
<td style="text-align:left;">
332
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
9.213
</td>
<td style="text-align:right;">
-0.166
</td>
</tr>
<tr>
<td style="text-align:left;">
333
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
17.078
</td>
<td style="text-align:right;">
-0.193
</td>
</tr>
<tr>
<td style="text-align:left;">
334
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
-7.448
</td>
<td style="text-align:right;">
1.187
</td>
</tr>
<tr>
<td style="text-align:left;">
335
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
0.602
</td>
<td style="text-align:right;">
-10.824
</td>
</tr>
<tr>
<td style="text-align:left;">
337
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
34.090
</td>
<td style="text-align:right;">
8.701
</td>
</tr>
<tr>
<td style="text-align:left;">
349
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
-24.995
</td>
<td style="text-align:right;">
1.277
</td>
</tr>
<tr>
<td style="text-align:left;">
350
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
-13.217
</td>
<td style="text-align:right;">
6.719
</td>
</tr>
<tr>
<td style="text-align:left;">
351
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
4.880
</td>
<td style="text-align:right;">
-2.979
</td>
</tr>
<tr>
<td style="text-align:left;">
352
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
20.986
</td>
<td style="text-align:right;">
3.605
</td>
</tr>
<tr>
<td style="text-align:left;">
369
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
3.128
</td>
<td style="text-align:right;">
0.950
</td>
</tr>
<tr>
<td style="text-align:left;">
370
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
-25.709
</td>
<td style="text-align:right;">
4.986
</td>
</tr>
<tr>
<td style="text-align:left;">
371
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
1.084
</td>
<td style="text-align:right;">
-1.002
</td>
</tr>
<tr>
<td style="text-align:left;">
372
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
12.587
</td>
<td style="text-align:right;">
1.305
</td>
</tr>
</tbody>
</table>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:bayes-compare">Table 9: </span>Random effect standard errors
</caption>
<thead>
<tr>
<th style="text-align:left;">
level
</th>
<th style="text-align:left;">
group
</th>
<th style="text-align:right;">
(Intercept)
</th>
<th style="text-align:right;">
Days
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
308
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
13.751
</td>
<td style="text-align:right;">
2.825
</td>
</tr>
<tr>
<td style="text-align:left;">
309
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
14.096
</td>
<td style="text-align:right;">
2.850
</td>
</tr>
<tr>
<td style="text-align:left;">
310
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
14.122
</td>
<td style="text-align:right;">
2.875
</td>
</tr>
<tr>
<td style="text-align:left;">
330
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
14.144
</td>
<td style="text-align:right;">
2.878
</td>
</tr>
<tr>
<td style="text-align:left;">
331
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
13.165
</td>
<td style="text-align:right;">
2.789
</td>
</tr>
<tr>
<td style="text-align:left;">
332
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
13.629
</td>
<td style="text-align:right;">
2.782
</td>
</tr>
<tr>
<td style="text-align:left;">
333
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
14.185
</td>
<td style="text-align:right;">
2.806
</td>
</tr>
<tr>
<td style="text-align:left;">
334
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
13.260
</td>
<td style="text-align:right;">
2.780
</td>
</tr>
<tr>
<td style="text-align:left;">
335
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
14.069
</td>
<td style="text-align:right;">
2.862
</td>
</tr>
<tr>
<td style="text-align:left;">
337
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
14.091
</td>
<td style="text-align:right;">
2.839
</td>
</tr>
<tr>
<td style="text-align:left;">
349
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
13.720
</td>
<td style="text-align:right;">
2.753
</td>
</tr>
<tr>
<td style="text-align:left;">
350
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
13.562
</td>
<td style="text-align:right;">
2.753
</td>
</tr>
<tr>
<td style="text-align:left;">
351
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
13.245
</td>
<td style="text-align:right;">
2.793
</td>
</tr>
<tr>
<td style="text-align:left;">
352
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
13.706
</td>
<td style="text-align:right;">
2.798
</td>
</tr>
<tr>
<td style="text-align:left;">
369
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
12.790
</td>
<td style="text-align:right;">
2.753
</td>
</tr>
<tr>
<td style="text-align:left;">
370
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
13.524
</td>
<td style="text-align:right;">
2.798
</td>
</tr>
<tr>
<td style="text-align:left;">
371
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
13.532
</td>
<td style="text-align:right;">
2.677
</td>
</tr>
<tr>
<td style="text-align:left;">
372
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:right;">
13.400
</td>
<td style="text-align:right;">
2.734
</td>
</tr>
</tbody>
</table>
</div>
<p>We can see that the mgcv estimates for standard errors of the random effects are close to the average standard errors from the fully Bayesian approach. For the Bayesian result we have (13.666 and 2.797 for Intercept and Days coefficient respectively, while for mgcv this is 13.279 and 2.673.</p>
<h2 id="back-to-the-initial-problem">Back to the initial problem</h2>
<p>So weâve established that both default gam and bam output are providing what we want. However, the reason for using mgcv for mixed models is the speed gain weâll get with big data. So letâs return to the binary outcome example that took over a minute for lme4 to run.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
system.time({
  bam_big &lt;- bam(
    y_bin ~ x + b + s(g, bs=&#39;re&#39;), 
    data = d,
    nthreads = 8,
    family = binomial
  )
})</code></pre>
<pre><code>
    user   system  elapsed 
8162.522  126.399 1293.742 </code></pre>
</div>
<p>That didnât actually improve our situation, and was much worse in time- more than 20 minutes! Remember though, that the mgcv approach has to estimate all those random effect coefficients, while lme4 is dealing with a perfectly balanced data set with which to estimate a single variance component. In practice, with additional complexities bam would win out eventually<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</p>
<p>However, even here we havenât used all our secret weapons. Another option with bam works on a modified data set using binned/rounded values for continuous covariates, and working with only the minimum data necessary to estimate the coefficients<span class="citation" data-cites="wood_generalized_2015">(S. N. Wood, Goude, and Shaw <a href="#ref-wood_generalized_2015">2015</a><a href="#ref-wood_generalized_2015">a</a>)</span>. With large enough data, as is the case here, the estimated parameters might not be different at all, while the efficiency gains could be tremendous. Letâs add <code>discrete = TRUE</code> and see what happens.</p>
<aside>
We just need the distinct set of values after rounding.
</aside>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
system.time({
  bam_big_d &lt;- bam(
    y_bin ~ x + b + s(g, bs=&#39;re&#39;), 
    data = d,
    nthreads = 8,
    family = binomial, 
    discrete = TRUE
  )
})</code></pre>
<pre><code>
   user  system elapsed 
 45.629   2.140  11.492 </code></pre>
</div>
<p><strong>Wow!</strong> That was as fast almost as lme4 with the linear mixed model! Letâs check the results. Weâll start with the fixed effects. I add some digits to the result so we can see the very slight differences.</p>
<div class="layout-chunk" data-layout="l-body">

</div>
<div class="layout-chunk" data-layout="l-body">
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:bam-fe-results">Table 10: </span>Fixed Effects
</caption>
<thead>
<tr>
<th style="text-align:left;">
Model
</th>
<th style="text-align:left;">
Term
</th>
<th style="text-align:right;">
Estimate
</th>
<th style="text-align:right;">
SE
</th>
<th style="text-align:right;">
LL
</th>
<th style="text-align:right;">
UL
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
True
</td>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
0.00000000
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
</tr>
<tr>
<td style="text-align:left;">
True
</td>
<td style="text-align:left;">
x
</td>
<td style="text-align:right;">
0.50000000
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
</tr>
<tr>
<td style="text-align:left;">
True
</td>
<td style="text-align:left;">
b
</td>
<td style="text-align:right;">
0.25000000
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
</tr>
<tr>
<td style="text-align:left;">
bam_big
</td>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
0.03730496
</td>
<td style="text-align:right;">
0.02257804
</td>
<td style="text-align:right;">
-0.00694801
</td>
<td style="text-align:right;">
0.08155792
</td>
</tr>
<tr>
<td style="text-align:left;">
bam_big
</td>
<td style="text-align:left;">
x
</td>
<td style="text-align:right;">
0.50087021
</td>
<td style="text-align:right;">
0.00223195
</td>
<td style="text-align:right;">
0.49649558
</td>
<td style="text-align:right;">
0.50524483
</td>
</tr>
<tr>
<td style="text-align:left;">
bam_big
</td>
<td style="text-align:left;">
b
</td>
<td style="text-align:right;">
0.19083417
</td>
<td style="text-align:right;">
0.03209232
</td>
<td style="text-align:right;">
0.12793322
</td>
<td style="text-align:right;">
0.25373513
</td>
</tr>
<tr>
<td style="text-align:left;">
bam_big_d
</td>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
0.03730496
</td>
<td style="text-align:right;">
0.02257845
</td>
<td style="text-align:right;">
-0.00694880
</td>
<td style="text-align:right;">
0.08155872
</td>
</tr>
<tr>
<td style="text-align:left;">
bam_big_d
</td>
<td style="text-align:left;">
x
</td>
<td style="text-align:right;">
0.50087024
</td>
<td style="text-align:right;">
0.00223195
</td>
<td style="text-align:right;">
0.49649561
</td>
<td style="text-align:right;">
0.50524487
</td>
</tr>
<tr>
<td style="text-align:left;">
bam_big_d
</td>
<td style="text-align:left;">
b
</td>
<td style="text-align:right;">
0.19083419
</td>
<td style="text-align:right;">
0.03209290
</td>
<td style="text-align:right;">
0.12793212
</td>
<td style="text-align:right;">
0.25373627
</td>
</tr>
<tr>
<td style="text-align:left;">
lme4
</td>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
0.03734366
</td>
<td style="text-align:right;">
0.02254863
</td>
<td style="text-align:right;">
-0.00685085
</td>
<td style="text-align:right;">
0.08153816
</td>
</tr>
<tr>
<td style="text-align:left;">
lme4
</td>
<td style="text-align:left;">
x
</td>
<td style="text-align:right;">
0.50136904
</td>
<td style="text-align:right;">
0.00223341
</td>
<td style="text-align:right;">
0.49699163
</td>
<td style="text-align:right;">
0.50574645
</td>
</tr>
<tr>
<td style="text-align:left;">
lme4
</td>
<td style="text-align:left;">
b
</td>
<td style="text-align:right;">
0.19105107
</td>
<td style="text-align:right;">
0.03204661
</td>
<td style="text-align:right;">
0.12824086
</td>
<td style="text-align:right;">
0.25386127
</td>
</tr>
</tbody>
</table>
</div>
<p>Now for the variance components.</p>
<div class="layout-chunk" data-layout="l-body">
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:big-vc-results">Table 11: </span>Variance Components
</caption>
<thead>
<tr>
<th style="text-align:left;">
Model
</th>
<th style="text-align:right;">
std.dev
</th>
<th style="text-align:right;">
variance
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
true
</td>
<td style="text-align:right;">
0.500000
</td>
<td style="text-align:right;">
0.250000
</td>
</tr>
<tr>
<td style="text-align:left;">
bam_big
</td>
<td style="text-align:right;">
0.502942
</td>
<td style="text-align:right;">
0.252950
</td>
</tr>
<tr>
<td style="text-align:left;">
bam_big_d
</td>
<td style="text-align:right;">
0.502951
</td>
<td style="text-align:right;">
0.252959
</td>
</tr>
<tr>
<td style="text-align:left;">
lme4
</td>
<td style="text-align:right;">
0.502994
</td>
<td style="text-align:right;">
0.253003
</td>
</tr>
</tbody>
</table>
</div>
<p>And finally, letâs look at the estimated random effects for the first 5 clusters.</p>
<aside>
Just a note, unless you have very many observations per cluster, you should not expect to get very close to the true values of the random effects except on average. For example, if you just want to estimate a noisy mean, how many observations would you need to get reasonably close?
</aside>
<div class="layout-chunk" data-layout="l-body">
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:big-re-results">Table 12: </span>Estimated Random Effects
</caption>
<thead>
<tr>
<th style="text-align:left;">
cluster
</th>
<th style="text-align:right;">
true
</th>
<th style="text-align:right;">
bam_big
</th>
<th style="text-align:right;">
bam_big_d
</th>
<th style="text-align:right;">
lme4
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
1
</td>
<td style="text-align:right;">
0.0912404
</td>
<td style="text-align:right;">
-0.0361639
</td>
<td style="text-align:right;">
-0.0361640
</td>
<td style="text-align:right;">
-0.0362082
</td>
</tr>
<tr>
<td style="text-align:left;">
2
</td>
<td style="text-align:right;">
0.1310768
</td>
<td style="text-align:right;">
0.1483554
</td>
<td style="text-align:right;">
0.1483554
</td>
<td style="text-align:right;">
0.1481519
</td>
</tr>
<tr>
<td style="text-align:left;">
3
</td>
<td style="text-align:right;">
-0.0562572
</td>
<td style="text-align:right;">
-0.1424477
</td>
<td style="text-align:right;">
-0.1424478
</td>
<td style="text-align:right;">
-0.1426813
</td>
</tr>
<tr>
<td style="text-align:left;">
4
</td>
<td style="text-align:right;">
0.6194238
</td>
<td style="text-align:right;">
0.5563464
</td>
<td style="text-align:right;">
0.5563468
</td>
<td style="text-align:right;">
0.5561672
</td>
</tr>
<tr>
<td style="text-align:left;">
5
</td>
<td style="text-align:right;">
-0.5022289
</td>
<td style="text-align:right;">
-0.4145603
</td>
<td style="text-align:right;">
-0.4145606
</td>
<td style="text-align:right;">
-0.4148388
</td>
</tr>
</tbody>
</table>
</div>
<div class="layout-chunk" data-layout="l-body">

</div>
<div class="layout-chunk" data-layout="l-body">

</div>
<p>So weâre getting what we should in general.</p>
<h2 id="when-to-use-bam">When to use bam</h2>
<p>The following are some guidelines for when bam might be prefereable.</p>
<h3 id="linear-mixed-models">Linear Mixed Models</h3>
<p>In general youâll need very large data for bam to be preferable linear mixed model unless:</p>
<ul>
<li>You have complicated structure that begins to bog down lme4</li>
<li>You want to add smooth terms<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></li>
<li>You have memory issues</li>
</ul>
<p>The following shows some timings for lme4, glmmTMB, and mgcv for the linear mixed model case under a variety of settings. In some sense, this is not exactly a fair comparison as mgcv parallelizes computations while lme4 and glmmTMB do not. However, this is also exactly the point of the demonstration - those who can, do. In general though, the lme4 advantage holds until around 1 million observations.</p>
<aside>
For lme4, I set at least one setting to possibly improve speed performance for both lme and glmm models. In addition, for mgcv I only used 10 cores for parallelization so as to be similar to what is common on modern machines (8-12), but anyone with access to a cluster computing environment would see even more speed gain by utilizing additional cores. For glmmTMB, settings were left at defaults, as Iâve not come across any specific recommendations. See <span class="citation" data-cites="brooks_glmmtmb_2017">Brooks et al. (<a href="#ref-brooks_glmmtmb_2017">2017</a>)</span> for more speed comparisons of glmmTMB, mgcv, lme4, and others.
</aside>
<div class="layout-chunk" data-layout="l-body">
<p><img src="../../img/bam/lme_results.svg" style="display: block; margin: auto;" /></p>
</div>
<h3 id="generalized-linear-mixed-models">Generalized Linear Mixed Models</h3>
<div class="layout-chunk" data-layout="l-body">
<p><img src="../../img/bam/gmm_results.svg" style="display: block; margin: auto;" /></p>
</div>
<p>For the generalized setting with binary, count, and other outcomes up to 50-100k observations:</p>
<ul>
<li>lme4, at least at the time of this writing, will almost certainly start giving convergence warnings even in well-behaved data settings, and as such, will require tweaking to mitigate</li>
<li>glmmTMB is probably viable up to 100k and one or two random effects</li>
<li>Use mgcv for same reasons as above</li>
</ul>
<p>Beyond that, I think bam will be a great choice.</p>
<h3 id="limitations">Limitations</h3>
<ul>
<li>The number of parameters to estimate increases greatly, which may void any gains until very large data with complex models</li>
<li>No estimation of random effect correlations, e.g.Â slopes and intercepts</li>
<li>When <code>discrete = TRUE</code>, some <span class="func" style="">predict.gam</span> functionality may be lost</li>
</ul>
<p>All in all, these are pretty minor, and the last one likely will be remedied in a future release.</p>
<h2 id="other-options">Other options</h2>
<p>When looking into mixed models for big data, you typically wonât find much. Iâve seen some packages or offereings for some machine learning approaches like random forests<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>, but this doesnât address the issue of large data. A Spark module is available, <a href="https://github.com/linkedin/photon-ml">photonML</a>, provided by LinkedIn, but itâs not clear how easy it is to implement. Julia has recently made multithreading a viable option for any function. This is notable since Doug Bates, one of the lme4 authors, develops the <a href="https://github.com/dmbates/MixedModels.jl">MixedModels</a> module for Julia. Should multithreading functionality be added, it could be a very powerful tool.</p>
<p>Among proprietary options, SAS and Stata are the more commonly used tools. SAS PROC HPMIXED essentially uses the lme4 approach, but can be faster for well-behaved data. Stata, while commonly used for mixed models, is generally slower than the lme4 even for standard settings and the smallest data settings noted here<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>.</p>
<aside>
SAS uses disk rather than RAM for processing, so may be preferred for low RAM devices.
</aside>
<h2 id="summary">Summary</h2>
<p>The take home point here is that you now have viable tools to run mixed models on very large data. This doesnât mean you wonât have to wait for it, especially for more complicated models, but you may even be able to run some of these on standard machines in reasonable times. The alternative estimation procedures may even make otherwise problematic models more</p>
<p><a href="http://svmiller.com/blog/2018/06/mixed-effects-models-optimizer-checks/" class="uri">http://svmiller.com/blog/2018/06/mixed-effects-models-optimizer-checks/</a> <a href="https://cran.r-project.org/web/packages/lme4/vignettes/lmerperf.html" class="uri">https://cran.r-project.org/web/packages/lme4/vignettes/lmerperf.html</a></p>
<div id="refs" class="references">
<div id="ref-brooks_glmmtmb_2017">
<p>Brooks, Mollie E., Kasper Kristensen, Koen J. van Benthem, Arni Magnusson, Casper W. Berg, Anders Nielsen, Hans J. Skaug, Martin MÃ¤chler, and Benjamin M. Bolker. 2017. âGlmmTMB Balances Speed and Flexibility Among Packages for Zero-Inflated Generalized Linear Mixed Modeling.â <em>The R Journal</em> 9 (2): 378â400. <a href="https://journal.r-project.org/archive/2017/RJ-2017-066">https://journal.r-project.org/archive/2017/RJ-2017-066</a>.</p>
</div>
<div id="ref-li_faster_2019">
<p>Li, Zheyuan, and Simon N. Wood. 2019. âFaster Model Matrix Crossproducts for Large Generalized Linear Models with Discretized Covariates.â <em>Statistics and Computing</em>, March. <a href="https://doi.org/10.1007/s11222-019-09864-2">https://doi.org/10.1007/s11222-019-09864-2</a>.</p>
</div>
<div id="ref-wood_mgcv:_2012">
<p>Wood, Simon. 2012. âMgcv: Mixed GAM Computation Vehicle with GCV/AIC/REML Smoothness Estimation,â October. <a href="https://researchportal.bath.ac.uk/en/publications/mgcv-mixed-gam-computation-vehicle-with-gcvaicreml-smoothness-est">https://researchportal.bath.ac.uk/en/publications/mgcv-mixed-gam-computation-vehicle-with-gcvaicreml-smoothness-est</a>.</p>
</div>
<div id="ref-wood_generalized_2017">
<p>Wood, Simon N. 2017. <em>Generalized Additive Models : An Introduction with R, Second Edition</em>. Chapman; Hall/CRC. <a href="https://doi.org/10.1201/9781315370279">https://doi.org/10.1201/9781315370279</a>.</p>
</div>
<div id="ref-wood_generalized_2015">
<p>Wood, Simon N., Yannig Goude, and Simon Shaw. 2015a. âGeneralized Additive Models for Large Data Sets.â <em>Journal of the Royal Statistical Society: Series C (Applied Statistics)</em> 64 (1): 139â55. <a href="https://doi.org/10.1111/rssc.12068">https://doi.org/10.1111/rssc.12068</a>.</p>
</div>
<div id="ref-wood_generalized_2015-1">
<p>âââ. 2015b. âGeneralized Additive Models for Large Data Sets.â <em>Journal of the Royal Statistical Society: Series C (Applied Statistics)</em> 64 (1): 139â55. <a href="https://doi.org/10.1111/rssc.12068">https://doi.org/10.1111/rssc.12068</a>.</p>
</div>
<div id="ref-wood_generalized_2017-1">
<p>Wood, Simon N., Zheyuan Li, Gavin Shaddick, and Nicole H. Augustin. 2017. âGeneralized Additive Models for Gigadata: Modeling the U.k. Black Smoke Network Daily Data.â <em>Journal of the American Statistical Association</em> 112 (519): 1199â1210. <a href="https://doi.org/10.1080/01621459.2016.1195744">https://doi.org/10.1080/01621459.2016.1195744</a>.</p>
</div>
</div>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Even just adding an additional random effect would possibly be enough for this data example.<a href="#fnref1" class="footnote-back">â©</a></p></li>
<li id="fn2"><p>You could use construct the smooth with mgcv and add it to the model matrix for lme4.<a href="#fnref2" class="footnote-back">â©</a></p></li>
<li id="fn3"><p>See <a href="https://cran.r-project.org/web/packages/REEMtree/">REEMtree</a>, <a href="https://cran.r-project.org/web/packages/MixRF/">mixRF</a> for example.<a href="#fnref3" class="footnote-back">â©</a></p></li>
<li id="fn4"><p>See McCoach et al. (2018) <a href="https://journals.sagepub.com/doi/abs/10.3102/1076998618776348">Does the Package Matter?</a>. They also look at HLM and Mplus. However, I havenât in years consulted with anyone across dozens of disciplines that was using it for mixed models. With Mplus, the verbosity of the syntax, plus additional data processing required, plus huge lack of post-processing of the model would negate any speed gain one might get from simply running the model. Couple this with the fact that campus-wide licenses are rare for either, neither could be recommended for mixed models. Note also, that one setting of lmer probably would have negated almost all their convergence issues.<a href="#fnref4" class="footnote-back">â©</a></p></li>
</ol>
</section>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>


<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom"></div>
<script id="distill-bibliography" type="text/bibtex">

@article{beyerlein_alternative_2008,
	title = {Alternative regression models to assess increase in childhood {BMI}},
	volume = {8},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/8/59},
	doi = {10.1186/1471-2288-8-59},
	number = {1},
	urldate = {2012-05-16},
	journal = {BMC Medical Research Methodology},
	author = {Beyerlein, Andreas and Fahrmeir, Ludwig and Mansmann, Ulrich and Toschke, AndrÃ© M},
	year = {2008},
	pages = {59},
	file = {BMC Medical Research Methodology | Full text | Alternative regression models to assess increase in childhood BMI:/Users/micl/Zotero/storage/KADXBB3S/59.html:text/html}
}

@book{wood_generalized_2006,
	title = {Generalized additive models: an introduction with {R}},
	volume = {66},
	shorttitle = {Generalized additive models},
	publisher = {CRC Press},
	author = {Wood, S. N},
	year = {2006},
	file = {[PDF] from bath.ac.uk:/Users/micl/Zotero/storage/N99I9S57/Wood - 2006 - Generalized additive models an introduction with .pdf:application/pdf;Snapshot:/Users/micl/Zotero/storage/AVVZHAIN/Wood - 2006 - Generalized additive models an introduction with .html:text/html}
}

@book{rasmussen_gaussian_2006,
	address = {Cambridge, Mass.},
	title = {Gaussian processes for machine learning},
	isbn = {0-262-18253-X 978-0-262-18253-9},
	abstract = {"Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics."--Jacket.},
	language = {English},
	publisher = {MIT Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I},
	year = {2006}
}

@book{ruppert_semiparametric_2003,
	title = {Semiparametric {Regression}},
	isbn = {978-0-521-78516-7},
	abstract = {Semiparametric regression is concerned with the flexible incorporation of non-linear functional relationships in regression analyses. Any application area that benefits from regression analysis can also benefit from semiparametric regression. Assuming only a basic familiarity with ordinary parametric regression, this user-friendly book explains the techniques and benefits of semiparametric regression in a concise and modular fashion. The authors make liberal use of graphics and examples plus case studies taken from environmental, financial, and other applications. They include practical advice on implementation and pointers to relevant software. The book is suitable as a textbook for students with little background in regression as well as a reference book for statistically oriented scientists such as biostatisticians, econometricians, quantitative social scientists, epidemiologists, with a good working knowledge of regression and the desire to begin using more flexible semiparametric models. Even experts on semiparametric regression should find something new here.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Ruppert, David and Wand, Matt P. and Carroll, Raymond J.},
	month = jul,
	year = {2003},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / General, Regression analysis, Mathematics / Probability \& Statistics / Regression Analysis, Medical / Epidemiology, Nonparametric statistics}
}

@book{fox_nonparametric_2000,
	title = {Nonparametric {Simple} {Regression}: {Smoothing} {Scatterplots}},
	isbn = {978-0-7619-1585-0},
	shorttitle = {Nonparametric {Simple} {Regression}},
	abstract = {John Fox introduces readers to the techniques of kernel estimation, additive nonparametric regression, and the ways nonparametric regression can be employed to select transformations of the data preceding a linear least-squares fit.},
	language = {en},
	publisher = {SAGE},
	author = {Fox, John},
	month = jan,
	year = {2000},
	keywords = {Mathematics / Probability \& Statistics / General, Social Science / Research, Regression analysis, Nonparametric statistics, Medical / General, Social Science / General, Social Science / Statistics, Social sciences}
}

@book{fox_multiple_2000,
	title = {Multiple and {Generalized} {Nonparametric} {Regression}},
	isbn = {978-0-7619-2189-9},
	abstract = {This book builds on John Fox's previous volume in the QASS Series, Non Parametric Simple Regression. In this book, the reader learns how to estimate and plot smooth functions when there are multiple independent variables.},
	language = {en},
	publisher = {SAGE},
	author = {Fox, John},
	month = may,
	year = {2000},
	keywords = {Mathematics / Probability \& Statistics / General, Social Science / Research, Regression analysis, Mathematics / Probability \& Statistics / Regression Analysis, Nonparametric statistics, Social Science / General, Social Science / Statistics, Social sciences, Social Science / Methodology, Social sciences - Statistical methods, Social sciences/ Statistical methods}
}

@book{wasserman_all_2006,
	title = {All of {Nonparametric} {Statistics}},
	isbn = {978-0-387-25145-5},
	abstract = {The goal of this text is to provide the reader with a single book where they can find a brief account of many, modern topics in nonparametric inference. The book is aimed at Master's level or Ph.D. level students in statistics, computer science, and engineering. It is also suitable for researchers who want to get up to speed quickly on modern nonparametric methods. This text covers a wide range of topics including: the bootstrap, the nonparametric delta method, nonparametric regression, density estimation, orthogonal function methods, minimax estimation, nonparametric confidence sets, and wavelets. The book has a mixture of methods and theory. From the reviews: "...The book is excellent." (Short Book Reviews of the ISI, June 2006) "Now we have All of Nonparametric Statistics a?{\textbar} . the writing is excellent and the author is to be congratulated on the clarity achieved. a?{\textbar} the book is excellent." (N.R. Draper, Short Book Reviews, Vol. 26 (1), 2006) "Overall, I enjoyed reading this book very much. I like Wasserman's intuitive explanations and careful insights into why one path or approach is taken over another. Most of all, I am impressed with the wealth of information on the subject of asymptotic nonparametric inferences." (Stergios B. Fotopoulos for Technometrics, Vol. 49, No. 1., February 2007)},
	language = {en},
	publisher = {Springer},
	author = {Wasserman, Larry},
	year = {2006},
	keywords = {statistics, Mathematics / Probability \& Statistics / General, Mathematics / General, Nonparametric statistics, Artificial intelligence, Computers / Intelligence (AI) \& Semantics, Mathematical statistics}
}

@book{venables_modern_2002,
	title = {Modern {Applied} {Statistics} {With} {S}},
	isbn = {978-0-387-95457-8},
	abstract = {S-PLUS is a powerful environment for the statistical and graphical analysis of data. It provides the tools to implement many statistical ideas which have been made possible by the widespread availability of workstations having good graphics and computational capabilities. This book is a guide to using S-PLUS to perform statistical analyses and provides both an introduction to the use of S-PLUS and a course in modern statistical methods. S-PLUS is available for both Windows and UNIX workstations, and both versions are covered in depth.The aim of the book is to show how to use S-PLUS as a powerful and graphical data analysis system. Readers are assumed to have a basic grounding in statistics, and so the book in intended for would-be users of S-PLUS and both students and researchers using statistics. Throughout, the emphasis is on presenting practical problems and full analyses of real data sets. Many of the methods discussed are state-of-the-art approaches to topics such as linear, nonlinear, and smooth regression models, tree-based methods, multivariate analysis and pattern recognition, survival analysis, time series and spatial statistics. Throughout, modern techniques such as robust methods, non-parametric smoothing, and bootstrapping are used where appropriate.This third edition is intended for users of S-PLUS 4.5, 5.0, 2000 or later, although S-PLUS 3.3/4 are also considered. The major change from the second edition is coverage of the current versions of S-PLUS. The material has been extensively rewritten using new examples and the latest computationally intensive methods. The companion volume on S Programming will provide an in-depth guide for those writing software in the S language.The authors have written several software libraries that enhance S-PLUS; these and all the datasets used are available on the Internet in versions for Windows and UNIX. There are extensive on-line complements covering advanced material, user-contributed extensions, further exercises, and new features of S-PLUS as they are introduced.Dr. Venables is now Statistician with CSRIO in Queensland, having been at the Department of Statistics, University of Adelaide, for many years previously. He has given many short courses on S-PLUS in Australia, Europe, and the USA. Professor Ripley holds the Chair of Applied Statistics at the University of Oxford, and is the author of four other books on spatial statistics, simulation, pattern recognition, and neural networks.},
	language = {en},
	publisher = {BirkhÃ¤user},
	author = {Venables, William N. and Ripley, Brian D.},
	month = aug,
	year = {2002},
	keywords = {statistics, Mathematics / Probability \& Statistics / General, Computers / Mathematical \& Statistical Software, Mathematical statistics, Business \& Economics / Statistics, Mathematical statistics - Data processing, Mathematical statistics/ Data processing, S, S (Computer program language), S (Computer system), S-PLUS (Computer program language), Statistics - Data processing, Statistics/ Data processing}
}

@book{hastie_generalized_1990,
	title = {Generalized {Additive} {Models}},
	isbn = {978-0-412-34390-2},
	language = {en},
	publisher = {CRC Press},
	author = {Hastie, T.J. and Tibshirani, R.J.},
	month = jun,
	year = {1990},
	keywords = {Mathematics / Probability \& Statistics / General}
}

@book{hastie_elements_2009,
	edition = {2nd ed. 2009. Corr. 3rd printing 5th Printing.},
	title = {The {Elements} of {Statistical} {Learning}: {Data} {Mining}, {Inference}, and {Prediction}, {Second} {Edition}},
	isbn = {0-387-84857-6},
	shorttitle = {The {Elements} of {Statistical} {Learning}},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	month = feb,
	year = {2009}
}

@article{breiman_statistical_2001,
	title = {Statistical {Modeling}: {The} {Two} {Cultures} (with comments and a           rejoinder by the author)},
	volume = {16},
	issn = {0883-4237},
	shorttitle = {Statistical {Modeling}},
	url = {http://projecteuclid.org/euclid.ss/1009213726},
	doi = {10.1214/ss/1009213726},
	abstract = {There are two cultures in the use of statistical modeling to reach
             conclusions from data. One assumes that the data are generated by a given
             stochastic data model. The other uses algorithmic models and treats the data
             mechanism as unknown. The statistical community has been committed to the
             almost exclusive use of data models. This commitment has led to irrelevant
             theory, questionable conclusions, and has kept statisticians from working on a
             large range of interesting current problems. Algorithmic modeling, both in
             theory and practice, has developed rapidly in fields outside statistics. It can
             be used both on large complex data sets and as a more accurate and informative
             alternative to data modeling on smaller data sets. If our goal as a field is to
             use data to solve problems, then we need to move away from exclusive dependence
             on data models and adopt a more diverse set of tools.},
	number = {3},
	urldate = {2012-07-22},
	journal = {Statistical Science},
	author = {Breiman, Leo},
	month = aug,
	year = {2001},
	note = {Mathematical Reviews number (MathSciNet): MR1874152},
	pages = {199--231}
}

@article{rigby_generalized_2005,
	title = {Generalized additive models for location, scale and shape},
	volume = {54},
	issn = {1467-9876},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9876.2005.00510.x/abstract},
	doi = {10.1111/j.1467-9876.2005.00510.x},
	abstract = {Summary. A general class of statistical models for a univariate response variable is presented which we call the generalized additive model for location, scale and shape (GAMLSS). The model assumes independent observations of the response variable y given the parameters, the explanatory variables and the values of the random effects. The distribution for the response variable in the GAMLSS can be selected from a very general family of distributions including highly skew or kurtotic continuous and discrete distributions. The systematic part of the model is expanded to allow modelling not only of the mean (or location) but also of the other parameters of the distribution of y, as parametric and/or additive nonparametric (smooth) functions of explanatory variables and/or random-effects terms. Maximum (penalized) likelihood estimation is used to fit the (non)parametric models. A NewtonâRaphson or Fisher scoring algorithm is used to maximize the (penalized) likelihood. The additive terms in the model are fitted by using a backfitting algorithm. Censored data are easily incorporated into the framework. Five data sets from different fields of application are analysed to emphasize the generality of the GAMLSS class of models.},
	language = {en},
	number = {3},
	urldate = {2012-07-13},
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	author = {Rigby, R. A. and Stasinopoulos, D. M.},
	year = {2005},
	keywords = {Betaâbinomial distribution, BoxâCox transformation, Centile estimation, Cubic smoothing splines, Generalized linear mixed model, LMS method, Negative binomial distribution, Non-normality, Nonparametric models, Overdispersion, Penalized likelihood, Random effects, Skewness and kurtosis},
	pages = {507--554},
	file = {Full Text PDF:/Users/micl/Zotero/storage/D6PZADQB/Rigby and Stasinopoulos - 2005 - Generalized additive models for location, scale an.pdf:application/pdf;Snapshot:/Users/micl/Zotero/storage/W752GFV4/full.html:text/html}
}

@book{hardin_generalized_2012,
	edition = {3},
	title = {Generalized {Linear} {Models} and {Extensions}, {Third} {Edition}},
	isbn = {1-59718-105-6},
	publisher = {Stata Press},
	author = {Hardin, James W. and Hilbe, Joseph M.},
	month = jun,
	year = {2012}
}

@article{friedman_projection_1981,
	title = {Projection {Pursuit} {Regression}},
	volume = {76},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2287576},
	doi = {10.2307/2287576},
	abstract = {A new method for nonparametric multiple regression is presented. The procedure models the regression surface as a sum of general smooth functions of linear combinations of the predictor variables in an iterative manner. It is more general than standard stepwise and stagewise regression procedures, does not require the definition of a metric in the predictor space, and lends itself to graphical interpretation.},
	number = {376},
	urldate = {2012-06-26},
	journal = {Journal of the American Statistical Association},
	author = {Friedman, Jerome H. and Stuetzle, Werner},
	month = dec,
	year = {1981},
	note = {ArticleType: research-article / Full publication date: Dec., 1981 / Copyright Â© 1981 American Statistical Association},
	pages = {817--823}
}

@book{bybee_pisa_2009,
	title = {Pisa {Science} 2006: {Implications} for {Science} {Teachers} and {Teaching}},
	isbn = {978-1-933531-31-1},
	shorttitle = {Pisa {Science} 2006},
	language = {en},
	publisher = {NSTA Press},
	author = {Bybee, Rodger W. and McCrae, Barry},
	month = may,
	year = {2009},
	keywords = {Education / Testing \& Measurement, Education / General, Education / Student Life \& Student Affairs, Education / Teaching Methods \& Materials / Science \& Technology, Educational tests and measurements, High school students, High school students - Rating of, High school students/ Rating of, Programme for International Student Assessment, Science, Science - Study and teaching - United States, Science - Study and teaching (Secondary), Science / Study \& Teaching, Science/ Study and teaching (Secondary)}
}

@book{hardin_generalized_2007,
	title = {Generalized linear models and extensions},
	publisher = {Stata Corp},
	author = {Hardin, J. W and Hilbe, J.},
	year = {2007},
	file = {Snapshot:/Users/micl/Zotero/storage/9T3DGIWI/Hardin and Hilbe - 2007 - Generalized linear models and extensions.html:text/html}
}

@article{simpson_modelling_2018,
	title = {Modelling {Palaeoecological} {Time} {Series} {Using} {Generalised} {Additive} {Models}},
	volume = {6},
	issn = {2296-701X},
	url = {https://www.frontiersin.org/articles/10.3389/fevo.2018.00149/full},
	doi = {10.3389/fevo.2018.00149},
	abstract = {In the absence of annual laminations, time series generated from lake sediments or other similar stratigraphic sequences are irregularly spaced in time, which complicates formal analysis using classical statistical time series models. In lieu, statistical analyses of trends in palaeoenvironmental time series, if done at all, have typically used simpler linear regressions or (non-) parametric correlations with little regard for the violation of assumptions that almost surely occurs due to temporal dependencies in the data or that correlations do not provide estimates of the magnitude of change, just whether or not there is a linear or monotonic trend. Alternative approaches have used LOESS-estimated trends to justify data interpretations or test hypotheses as to the causal factors without considering the inherent subjectivity of the choice of parameters used to achieve the LOESS fit (e.g. span width, degree of polynomial). Generalized additive models (GAMs) are statistical models that can be used to estimate trends as smooth functions of time. Unlike LOESS, GAMs use automatic smoothness selection methods to objectively determine the complexity of the fitted trend, and as formal statistical models, GAMs, allow for potentially complex, non-linear trends, a proper accounting of model uncertainty, and the identification of periods of significant temporal change. Here, I present a consistent and modern approach to the estimation of trends in palaeoenvironmental time series using GAMs, illustrating features of the methodology with two example time series of contrasting complexity; a 150-year bulk organic matter Î´15N time series from Small Water, UK, and a 3000-year alkenone record from Braya-SÃ¸, Greenland. I discuss the underlying mechanics of GAMs that allow them to learn the shape of the trend from the data themselves and how simultaneous confidence intervals and the first derivatives of the trend are used to properly account for model uncertainty and identify periods of change. It is hoped that by using GAMs greater attention is paid to the statistical estimation of trends in palaeoenvironmental time series leading to more a robust and reproducible palaeoscience.},
	language = {English},
	urldate = {2019-02-10},
	journal = {Frontiers in Ecology and Evolution},
	author = {Simpson, Gavin L.},
	year = {2018},
	keywords = {environmental change, generalized additive models, simultaneous interval, Spline, time series},
	file = {Full Text PDF:/Users/micl/Zotero/storage/8RLFG7ZV/Simpson - 2018 - Modelling Palaeoecological Time Series Using Gener.pdf:application/pdf}
}

@article{friedman_additive_2000,
	title = {Additive logistic regression: a statistical view of boosting ({With} discussion and a rejoinder by the authors)},
	volume = {28},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Additive logistic regression},
	url = {https://projecteuclid.org/euclid.aos/1016218223},
	doi = {10.1214/aos/1016218223},
	abstract = {Boosting is one of the most important recent developments in classification methodology. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced. For many classification algorithms, this simple strategy results in dramatic improvements in performance. We show that this seemingly mysterious phenomenon can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to boosting. Direct multiclass generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multiclass generalizations of boosting in most situations, and far superior in some. We suggest a minor modification to boosting that can reduce computation, often by factors of 10 to 50. Finally, we apply these insights to produce an alternative formulation of boosting decision trees. This approach, based on best-first truncated tree induction, often leads to better performance, and can provide interpretable descriptions of the aggregate decision rule. It is also much faster computationally, making it more suitable to large-scale data mining applications.},
	language = {EN},
	number = {2},
	urldate = {2019-02-10},
	journal = {The Annals of Statistics},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	month = apr,
	year = {2000},
	mrnumber = {MR1790002},
	zmnumber = {1106.62323},
	keywords = {classification, machine learning, nonparametric estimation, stagewise fitting, tree},
	pages = {337--407},
	file = {Full Text PDF:/Users/micl/Zotero/storage/DRP4S38J/Friedman et al. - 2000 - Additive logistic regression a statistical view o.pdf:application/pdf;Snapshot:/Users/micl/Zotero/storage/IU9EH95A/1016218223.html:text/html}
}

@article{wood_mgcv:_2012,
	title = {mgcv: {Mixed} {GAM} {Computation} {Vehicle} with {GCV}/{AIC}/{REML} smoothness estimation},
	shorttitle = {mgcv},
	url = {https://researchportal.bath.ac.uk/en/publications/mgcv-mixed-gam-computation-vehicle-with-gcvaicreml-smoothness-est},
	language = {English},
	urldate = {2019-09-29},
	author = {Wood, Simon},
	month = oct,
	year = {2012},
	file = {Snapshot:/Users/micl/Zotero/storage/V7NYNZR8/mgcv-mixed-gam-computation-vehicle-with-gcvaicreml-smoothness-est.html:text/html}
}

@book{wood_generalized_2017,
	title = {Generalized Additive Models : An Introduction with R, Second Edition},
	isbn = {978-1-315-37027-9},
	shorttitle = {Generalized {Additive} {Models}},
	url = {https://www.taylorfrancis.com/books/9781315370279},
	abstract = {The first edition of this book has established itself as one of the leading references on generalized additive models (GAMs), and the only book on the topic to},
	language = {en},
	urldate = {2019-09-29},
	publisher = {Chapman and Hall/CRC},
	author = {Wood, Simon N.},
	month = may,
	year = {2017},
	doi = {10.1201/9781315370279},
	file = {Full Text PDF:/Users/micl/Zotero/storage/CUH9EAKX/Wood - 2017 - Generalized Additive Models  An Introduction with.pdf:application/pdf;Snapshot:/Users/micl/Zotero/storage/VQ2HCD9B/9781315370279.html:text/html}
}

@article{li_faster_2019,
	title = {Faster model matrix crossproducts for large generalized linear models with discretized covariates},
	issn = {1573-1375},
	url = {https://doi.org/10.1007/s11222-019-09864-2},
	doi = {10.1007/s11222-019-09864-2},
	abstract = {Wood et al. (J Am Stat Assoc 112(519):1199â1210, 2017) developed methods for fitting penalized regression spline based generalized additive models, with of the order of 10410410{\textasciicircum}4 coefficients, to up to 10810810{\textasciicircum}8 data. The methods offered two to three orders of magnitude reduction in computational cost relative to the most efficient previous methods. Part of the gain resulted from the development of a set of methods for efficiently computing model matrix products when model covariates each take only a discrete set of values substantially smaller than the sample size [generalizing an idea first appearing in Lang et al. (Stat Comput 24(2):223â238, 2014)]. Covariates can always be rounded to achieve such discretization, and it should be noted that the covariate discretization is marginal. That is we do not rely on discretizing covariates jointly, which would typically require the use of very coarse discretization. The most expensive computation in model estimation is the formation of the matrix cross product ðð³ððXTWX{\textbackslash}mathbf\{X\}{\textasciicircum}\{{\textbackslash}mathsf\{T\}\}\{{\textbackslash}mathbf\{WX\}\} where ðX{\textbackslash}mathbf\{X\} is a model matrix and ðW\{{\textbackslash}mathbf\{W\}\} a diagonal or tri-diagonal matrix. The purpose of this paper is to present a simple, novel and substantially more efficient approach to the computation of this cross product. The new method offers, for example, a 30 fold reduction in cross product computation time for the Black Smoke model dataset motivating Wood et al. (2017). Given this reduction in computational cost, the subsequent Cholesky decomposition of ðð³ððXTWX{\textbackslash}mathbf\{X\}{\textasciicircum}\{{\textbackslash}mathsf\{T\}\}\{{\textbackslash}mathbf\{WX\}\} and follow on computation of (ðð³ðð)â1(XTWX)â1({\textbackslash}mathbf\{X\}{\textasciicircum}\{{\textbackslash}mathsf\{T\}\}\{{\textbackslash}mathbf\{WX\}\}){\textasciicircum}\{-1\} become a more significant part of the computational burden, and we also discuss the choice of methods for improving their speed.},
	language = {en},
	urldate = {2019-09-29},
	journal = {Statistics and Computing},
	author = {Li, Zheyuan and Wood, Simon N.},
	month = mar,
	year = {2019},
	keywords = {BLAS, Fast regression, Generalized additive model},
	file = {Springer Full Text PDF:/Users/micl/Zotero/storage/7KC4ZXMP/Li and Wood - 2019 - Faster model matrix crossproducts for large genera.pdf:application/pdf}
}

@article{fasiolo_scalable_2019,
	title = {Scalable Visualization Methods for Modern Generalized Additive Models},
	volume = {0},
	issn = {1061-8600},
	url = {https://doi.org/10.1080/10618600.2019.1629942},
	doi = {10.1080/10618600.2019.1629942},
	abstract = {In the last two decades, the growth of computational resources has made it possible to handle generalized additive models (GAMs) that formerly were too costly for serious applications. However, the growth in model complexity has not been matched by improved visualizations for model development and results presentation. Motivated by an industrial application in electricity load forecasting, we identify the areas where the lack of modern visualization tools for GAMs is particularly severe, and we address the shortcomings of existing methods by proposing a set of visual tools that (a) are fast enough for interactive use, (b) exploit the additive structure of GAMs, (c) scale to large data sets, and (d) can be used in conjunction with a wide range of response distributions. The new visual methods proposed here are implemented by the mgcViz R package, available on the Comprehensive R Archive Network. Supplementary materials for this article are available online.},
	number = {0},
	urldate = {2019-09-29},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Fasiolo, Matteo and Nedellec, RaphaÃ«l and Goude, Yannig and Wood, Simon N.},
	month = jun,
	year = {2019},
	keywords = {Electricity load forecasting, Generalized additive models, Interactive model building, Regression modeling, Residuals checking, Visualization},
	pages = {1--9},
	file = {Snapshot:/Users/micl/Zotero/storage/V6LHAVE7/weblogin.umich.edu.html:text/html;Submitted Version:/Users/micl/Zotero/storage/4K2JDUWT/Fasiolo et al. - 2019 - Scalable Visualization Methods for Modern Generali.pdf:application/pdf}
}

@article{wood_generalized_2015,
	title = {Generalized additive models for large data sets},
	volume = {64},
	copyright = {Â© 2014 The Authors. Journal of the Royal Statistical Society: Series C Applied Statistics Published by John Wiley \& Sons Ltd on behalf of the Royal Statistical Society.},
	issn = {1467-9876},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssc.12068},
	doi = {10.1111/rssc.12068},
	abstract = {We consider an application in electricity grid load prediction, where generalized additive models are appropriate, but where the data set's size can make their use practically intractable with existing methods. We therefore develop practical generalized additive model fitting methods for large data sets in the case in which the smooth terms in the model are represented by using penalized regression splines. The methods use iterative update schemes to obtain factors of the model matrix while requiring only subblocks of the model matrix to be computed at any one time. We show that efficient smoothing parameter estimation can be carried out in a well-justified manner. The grid load prediction problem requires updates of the model fit, as new data become available, and some means for dealing with residual auto-correlation in grid load. Methods are provided for these problems and parallel implementation is covered. The methods allow estimation of generalized additive models for large data sets by using modest computer hardware, and the grid load prediction problem illustrates the utility of reduced rank spline smoothing methods for dealing with complex modelling problems.},
	language = {en},
	number = {1},
	urldate = {2019-09-29},
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	author = {Wood, Simon N. and Goude, Yannig and Shaw, Simon},
	year = {2015},
	keywords = {Correlated additive model, Electricity load prediction, Generalized additive model estimation},
	pages = {139--155},
	file = {Snapshot:/Users/micl/Zotero/storage/8LYQID9I/rssc.html:text/html}
}

@article{wood_generalized_2017-1,
	title = {Generalized Additive Models for Gigadata: Modeling the U.K. Black Smoke Network Daily Data},
	volume = {112},
	issn = {0162-1459},
	shorttitle = {Generalized {Additive} {Models} for {Gigadata}},
	url = {https://amstat.tandfonline.com/doi/full/10.1080/01621459.2016.1195744},
	doi = {10.1080/01621459.2016.1195744},
	abstract = {We develop scalable methods for fitting penalized regression spline based generalized additive models with of the order of 104 coefficients to up to 108 data. Computational feasibility rests on: (i) a new iteration scheme for estimation of model coefficients and smoothing parameters, avoiding poorly scaling matrix operations; (ii) parallelization of the iterationâs pivoted block Cholesky and basic matrix operations; (iii) the marginal discretization of model covariates to reduce memory footprint, with efficient scalable methods for computing required crossproducts directly from the discrete representation. Marginal discretization enables much finer discretization than joint discretization would permit. We were motivated by the need to model four decades worth of daily particulate data from the U.K. Black Smoke and Sulphur Dioxide Monitoring Network. Although reduced in size recently, over 2000 stations have at some time been part of the network, resulting in some 10 million measurements. Modeling at a daily scale is desirable for accurate trend estimation and mapping, and to provide daily exposure estimates for epidemiological cohort studies. Because of the dataset size, previous work has focused on modeling time or space averaged pollution levels, but this is unsatisfactory from a health perspective, since it is often acute exposure locally and on the time scale of days that is of most importance in driving adverse health outcomes. If computed by conventional means our black smoke model would require a half terabyte of storage just for the model matrix, whereas we are able to compute with it on a desktop workstation. The best previously available reduced memory footprint method would have required three orders of magnitude more computing time than our new method. Supplementary materials for this article are available online.},
	number = {519},
	urldate = {2019-09-29},
	journal = {Journal of the American Statistical Association},
	author = {Wood, Simon N. and Li, Zheyuan and Shaddick, Gavin and Augustin, Nicole H.},
	month = jul,
	year = {2017},
	pages = {1199--1210},
	file = {Full Text:/Users/micl/Zotero/storage/2N4Z5LJ3/Wood et al. - 2017 - Generalized Additive Models for Gigadata Modeling.pdf:application/pdf;Snapshot:/Users/micl/Zotero/storage/73X423H7/01621459.2016.html:text/html}
}

@article{wood_generalized_2015-1,
	title = {Generalized additive models for large data sets},
	volume = {64},
	copyright = {Â© 2014 The Authors. Journal of the Royal Statistical Society: Series C Applied Statistics Published by John Wiley \& Sons Ltd on behalf of the Royal Statistical Society.},
	issn = {1467-9876},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssc.12068},
	doi = {10.1111/rssc.12068},
	abstract = {We consider an application in electricity grid load prediction, where generalized additive models are appropriate, but where the data set's size can make their use practically intractable with existing methods. We therefore develop practical generalized additive model fitting methods for large data sets in the case in which the smooth terms in the model are represented by using penalized regression splines. The methods use iterative update schemes to obtain factors of the model matrix while requiring only subblocks of the model matrix to be computed at any one time. We show that efficient smoothing parameter estimation can be carried out in a well-justified manner. The grid load prediction problem requires updates of the model fit, as new data become available, and some means for dealing with residual auto-correlation in grid load. Methods are provided for these problems and parallel implementation is covered. The methods allow estimation of generalized additive models for large data sets by using modest computer hardware, and the grid load prediction problem illustrates the utility of reduced rank spline smoothing methods for dealing with complex modelling problems.},
	language = {en},
	number = {1},
	urldate = {2019-09-29},
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	author = {Wood, Simon N. and Goude, Yannig and Shaw, Simon},
	year = {2015},
	keywords = {Correlated additive model, Electricity load prediction, Generalized additive model estimation},
	pages = {139--155},
	file = {Snapshot:/Users/micl/Zotero/storage/66YDF5ZT/rssc.html:text/html}
}

@article{brooks_glmmtmb_2017,
	title = {glmmTMB Balances Speed and Flexibility Among Packages for Zero-inflated Generalized Linear Mixed Modeling},
	volume = {9},
	issn = {2073-4859},
	url = {https://journal.r-project.org/archive/2017/RJ-2017-066},
	language = {en},
	number = {2},
	urldate = {2019-10-13},
	journal = {The R Journal},
	author = {Brooks, Mollie E. and Kristensen, Kasper and Benthem, Koen J. van and Magnusson, Arni and Berg, Casper W. and Nielsen, Anders and Skaug, Hans J. and MÃ¤chler, Martin and Bolker, Benjamin M.},
	year = {2017},
	pages = {378--400},
	file = {Snapshot:/Users/micl/Zotero/storage/4ZBA8UHP/RJ-2017-066.html:text/html}
}
</script>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
