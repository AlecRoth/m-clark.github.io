
@book{wood_generalized_2006,
	title = {Generalized additive models: an introduction with {R}},
	volume = {66},
	shorttitle = {Generalized additive models},
	publisher = {CRC Press},
	author = {Wood, S. N},
	year = {2006},
	file = {[PDF] from bath.ac.uk:/Users/micl/Zotero/storage/N99I9S57/Wood - 2006 - Generalized additive models an introduction with .pdf:application/pdf;Snapshot:/Users/micl/Zotero/storage/AVVZHAIN/Wood - 2006 - Generalized additive models an introduction with .html:text/html}
}

@article{wood_mgcv:_2012,
	title = {mgcv: {Mixed} {GAM} {Computation} {Vehicle} with {GCV}/{AIC}/{REML} smoothness estimation},
	shorttitle = {mgcv},
	url = {https://researchportal.bath.ac.uk/en/publications/mgcv-mixed-gam-computation-vehicle-with-gcvaicreml-smoothness-est},
	language = {English},
	urldate = {2019-09-29},
	author = {Wood, Simon},
	month = oct,
	year = {2012},
	file = {Snapshot:/Users/micl/Zotero/storage/V7NYNZR8/mgcv-mixed-gam-computation-vehicle-with-gcvaicreml-smoothness-est.html:text/html}
}

@book{wood_generalized_2017,
	title = {Generalized Additive Models : An Introduction with R, Second Edition},
	isbn = {978-1-315-37027-9},
	shorttitle = {Generalized {Additive} {Models}},
	url = {https://www.taylorfrancis.com/books/9781315370279},
	abstract = {The first edition of this book has established itself as one of the leading references on generalized additive models (GAMs), and the only book on the topic to},
	language = {en},
	urldate = {2019-09-29},
	publisher = {Chapman and Hall/CRC},
	author = {Wood, Simon N.},
	month = may,
	year = {2017},
	doi = {10.1201/9781315370279},
	file = {Full Text PDF:/Users/micl/Zotero/storage/CUH9EAKX/Wood - 2017 - Generalized Additive Models  An Introduction with.pdf:application/pdf;Snapshot:/Users/micl/Zotero/storage/VQ2HCD9B/9781315370279.html:text/html}
}

@article{li_faster_2019,
	title = {Faster model matrix crossproducts for large generalized linear models with discretized covariates},
	issn = {1573-1375},
	url = {https://doi.org/10.1007/s11222-019-09864-2},
	doi = {10.1007/s11222-019-09864-2},
	abstract = {Wood et al. (J Am Stat Assoc 112(519):1199‚Äì1210, 2017) developed methods for fitting penalized regression spline based generalized additive models, with of the order of 10410410{\textasciicircum}4 coefficients, to up to 10810810{\textasciicircum}8 data. The methods offered two to three orders of magnitude reduction in computational cost relative to the most efficient previous methods. Part of the gain resulted from the development of a set of methods for efficiently computing model matrix products when model covariates each take only a discrete set of values substantially smaller than the sample size [generalizing an idea first appearing in Lang et al. (Stat Comput 24(2):223‚Äì238, 2014)]. Covariates can always be rounded to achieve such discretization, and it should be noted that the covariate discretization is marginal. That is we do not rely on discretizing covariates jointly, which would typically require the use of very coarse discretization. The most expensive computation in model estimation is the formation of the matrix cross product ùêóùñ≥ùêñùêóXTWX{\textbackslash}mathbf\{X\}{\textasciicircum}\{{\textbackslash}mathsf\{T\}\}\{{\textbackslash}mathbf\{WX\}\} where ùêóX{\textbackslash}mathbf\{X\} is a model matrix and ùêñW\{{\textbackslash}mathbf\{W\}\} a diagonal or tri-diagonal matrix. The purpose of this paper is to present a simple, novel and substantially more efficient approach to the computation of this cross product. The new method offers, for example, a 30 fold reduction in cross product computation time for the Black Smoke model dataset motivating Wood et al. (2017). Given this reduction in computational cost, the subsequent Cholesky decomposition of ùêóùñ≥ùêñùêóXTWX{\textbackslash}mathbf\{X\}{\textasciicircum}\{{\textbackslash}mathsf\{T\}\}\{{\textbackslash}mathbf\{WX\}\} and follow on computation of (ùêóùñ≥ùêñùêó)‚àí1(XTWX)‚àí1({\textbackslash}mathbf\{X\}{\textasciicircum}\{{\textbackslash}mathsf\{T\}\}\{{\textbackslash}mathbf\{WX\}\}){\textasciicircum}\{-1\} become a more significant part of the computational burden, and we also discuss the choice of methods for improving their speed.},
	language = {en},
	urldate = {2019-09-29},
	journal = {Statistics and Computing},
	author = {Li, Zheyuan and Wood, Simon N.},
	month = mar,
	year = {2019},
	keywords = {BLAS, Fast regression, Generalized additive model},
	file = {Springer Full Text PDF:/Users/micl/Zotero/storage/7KC4ZXMP/Li and Wood - 2019 - Faster model matrix crossproducts for large genera.pdf:application/pdf}
}

@article{wood_generalized_2015,
	title = {Generalized additive models for large data sets},
	volume = {64},
	copyright = {¬© 2014 The Authors. Journal of the Royal Statistical Society: Series C Applied Statistics Published by John Wiley \& Sons Ltd on behalf of the Royal Statistical Society.},
	issn = {1467-9876},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssc.12068},
	doi = {10.1111/rssc.12068},
	abstract = {We consider an application in electricity grid load prediction, where generalized additive models are appropriate, but where the data set's size can make their use practically intractable with existing methods. We therefore develop practical generalized additive model fitting methods for large data sets in the case in which the smooth terms in the model are represented by using penalized regression splines. The methods use iterative update schemes to obtain factors of the model matrix while requiring only subblocks of the model matrix to be computed at any one time. We show that efficient smoothing parameter estimation can be carried out in a well-justified manner. The grid load prediction problem requires updates of the model fit, as new data become available, and some means for dealing with residual auto-correlation in grid load. Methods are provided for these problems and parallel implementation is covered. The methods allow estimation of generalized additive models for large data sets by using modest computer hardware, and the grid load prediction problem illustrates the utility of reduced rank spline smoothing methods for dealing with complex modelling problems.},
	language = {en},
	number = {1},
	urldate = {2019-09-29},
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	author = {Wood, Simon N. and Goude, Yannig and Shaw, Simon},
	year = {2015},
	keywords = {Correlated additive model, Electricity load prediction, Generalized additive model estimation},
	pages = {139--155},
	file = {Snapshot:/Users/micl/Zotero/storage/8LYQID9I/rssc.html:text/html}
}

@article{wood_generalized_2017-1,
	title = {Generalized Additive Models for Gigadata: Modeling the U.K. Black Smoke Network Daily Data},
	volume = {112},
	issn = {0162-1459},
	shorttitle = {Generalized Additive Models for Gigadata},
	url = {https://amstat.tandfonline.com/doi/full/10.1080/01621459.2016.1195744},
	doi = {10.1080/01621459.2016.1195744},
	abstract = {We develop scalable methods for fitting penalized regression spline based generalized additive models with of the order of 104 coefficients to up to 108 data. Computational feasibility rests on: (i) a new iteration scheme for estimation of model coefficients and smoothing parameters, avoiding poorly scaling matrix operations; (ii) parallelization of the iteration‚Äôs pivoted block Cholesky and basic matrix operations; (iii) the marginal discretization of model covariates to reduce memory footprint, with efficient scalable methods for computing required crossproducts directly from the discrete representation. Marginal discretization enables much finer discretization than joint discretization would permit. We were motivated by the need to model four decades worth of daily particulate data from the U.K. Black Smoke and Sulphur Dioxide Monitoring Network. Although reduced in size recently, over 2000 stations have at some time been part of the network, resulting in some 10 million measurements. Modeling at a daily scale is desirable for accurate trend estimation and mapping, and to provide daily exposure estimates for epidemiological cohort studies. Because of the dataset size, previous work has focused on modeling time or space averaged pollution levels, but this is unsatisfactory from a health perspective, since it is often acute exposure locally and on the time scale of days that is of most importance in driving adverse health outcomes. If computed by conventional means our black smoke model would require a half terabyte of storage just for the model matrix, whereas we are able to compute with it on a desktop workstation. The best previously available reduced memory footprint method would have required three orders of magnitude more computing time than our new method. Supplementary materials for this article are available online.},
	number = {519},
	urldate = {2019-09-29},
	journal = {Journal of the American Statistical Association},
	author = {Wood, Simon N. and Li, Zheyuan and Shaddick, Gavin and Augustin, Nicole H.},
	month = jul,
	year = {2017},
	pages = {1199--1210},
	file = {Full Text:/Users/micl/Zotero/storage/2N4Z5LJ3/Wood et al. - 2017 - Generalized Additive Models for Gigadata Modeling.pdf:application/pdf;Snapshot:/Users/micl/Zotero/storage/73X423H7/01621459.2016.html:text/html}
}

@article{wood_generalized_2015-1,
	title = {Generalized additive models for large data sets},
	volume = {64},
	copyright = {¬© 2014 The Authors. Journal of the Royal Statistical Society: Series C Applied Statistics Published by John Wiley \& Sons Ltd on behalf of the Royal Statistical Society.},
	issn = {1467-9876},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssc.12068},
	doi = {10.1111/rssc.12068},
	abstract = {We consider an application in electricity grid load prediction, where generalized additive models are appropriate, but where the data set's size can make their use practically intractable with existing methods. We therefore develop practical generalized additive model fitting methods for large data sets in the case in which the smooth terms in the model are represented by using penalized regression splines. The methods use iterative update schemes to obtain factors of the model matrix while requiring only subblocks of the model matrix to be computed at any one time. We show that efficient smoothing parameter estimation can be carried out in a well-justified manner. The grid load prediction problem requires updates of the model fit, as new data become available, and some means for dealing with residual auto-correlation in grid load. Methods are provided for these problems and parallel implementation is covered. The methods allow estimation of generalized additive models for large data sets by using modest computer hardware, and the grid load prediction problem illustrates the utility of reduced rank spline smoothing methods for dealing with complex modelling problems.},
	language = {en},
	number = {1},
	urldate = {2019-09-29},
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	author = {Wood, Simon N. and Goude, Yannig and Shaw, Simon},
	year = {2015},
	keywords = {Correlated additive model, Electricity load prediction, Generalized additive model estimation},
	pages = {139--155},
	file = {Snapshot:/Users/micl/Zotero/storage/66YDF5ZT/rssc.html:text/html}
}