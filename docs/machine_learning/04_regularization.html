<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="An Introduction to Machine Learning" />
<meta property="og:type" content="book" />
<meta property="og:url" content="https://m-clark.github.io/docs/" />
<meta property="og:image" content="https://m-clark.github.io/docs/img/nineteeneightyR.png" />
<meta property="og:description" content="An introduction to machine learning for applied researchers." />
<meta name="github-repo" content="m-clark/introduction-to-machine-learning/" />


<meta name="date" content="2017-04-21" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="An introduction to machine learning for applied researchers.">

<title>An Introduction to Machine Learning</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<script src="libs/jquery-1.12.4/jquery.min.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>


<link rel="stylesheet" href="standard_html.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="mytufte.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#section"></a></li>
<li><a href="00_preface.html#preface">Preface</a></li>
<li class="has-sub"><a href="01_intro.html#introduction">Introduction</a><ul>
<li><a href="01_intro.html#explanation-prediction">Explanation &amp; Prediction</a></li>
<li><a href="01_intro.html#some-terminology">Some Terminology</a></li>
</ul></li>
<li class="has-sub"><a href="02_tools.html#tools">Tools</a><ul>
<li><a href="02_tools.html#the-standard-linear-model">The Standard Linear Model</a></li>
<li><a href="02_tools.html#logistic-regression">Logistic Regression</a></li>
<li class="has-sub"><a href="02_tools.html#expansions-of-those-tools">Expansions of Those Tools</a><ul>
<li><a href="02_tools.html#generalized-linear-models">Generalized Linear Models</a></li>
<li><a href="02_tools.html#generalized-additive-models">Generalized Additive Models</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="03_loss.html#the-loss-function">The Loss Function</a><ul>
<li class="has-sub"><a href="03_loss.html#continuous-outcomes">Continuous Outcomes</a><ul>
<li><a href="03_loss.html#squared-error">Squared Error</a></li>
<li><a href="03_loss.html#absolute-error">Absolute Error</a></li>
<li><a href="03_loss.html#negative-log-likelihood">Negative Log-likelihood</a></li>
<li><a href="03_loss.html#r-example">R Example</a></li>
</ul></li>
<li class="has-sub"><a href="03_loss.html#categorical-outcomes">Categorical Outcomes</a><ul>
<li><a href="03_loss.html#misclassification">Misclassification</a></li>
<li><a href="03_loss.html#binomial-log-likelihood">Binomial log-likelihood</a></li>
<li><a href="03_loss.html#exponential">Exponential</a></li>
<li><a href="03_loss.html#hinge-loss">Hinge Loss</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="04_regularization.html#regularization">Regularization</a><ul>
<li><a href="04_regularization.html#r-example-1">R Example</a></li>
</ul></li>
<li class="has-sub"><a href="05_biasvar.html#bias-variance-tradeoff">Bias-Variance Tradeoff</a><ul>
<li><a href="05_biasvar.html#bias-variance">Bias <em>&amp;</em> Variance</a></li>
<li><a href="05_biasvar.html#the-tradeoff">The Tradeoff</a></li>
<li class="has-sub"><a href="05_biasvar.html#diagnosing-bias-variance-issues-possible-solutions">Diagnosing Bias-Variance Issues <em>&amp;</em> Possible Solutions</a><ul>
<li><a href="05_biasvar.html#worst-case-scenario">Worst Case Scenario</a></li>
<li><a href="05_biasvar.html#high-variance">High Variance</a></li>
<li><a href="05_biasvar.html#high-bias">High Bias</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="06_crossval.html#cross-validation">Cross-Validation</a><ul>
<li><a href="06_crossval.html#adding-another-validation-set">Adding Another Validation Set</a></li>
<li class="has-sub"><a href="06_crossval.html#k-fold-cross-validation">K-fold Cross-Validation</a><ul>
<li><a href="06_crossval.html#leave-one-out-cross-validation">Leave-one-out Cross-Validation</a></li>
</ul></li>
<li><a href="06_crossval.html#bootstrap">Bootstrap</a></li>
<li><a href="06_crossval.html#other-stuff">Other Stuff</a></li>
</ul></li>
<li class="has-sub"><a href="07_models.html#model-assessment-selection">Model Assessment &amp; Selection</a><ul>
<li><a href="07_models.html#beyond-classification-accuracy-other-measures-of-performance">Beyond Classification Accuracy: Other Measures of Performance</a></li>
</ul></li>
<li class="has-sub"><a href="08_overview.html#process-overview">Process Overview</a><ul>
<li class="has-sub"><a href="08_overview.html#data-preparation">Data Preparation</a><ul>
<li><a href="08_overview.html#define-data-and-data-partitions">Define Data and Data Partitions</a></li>
<li><a href="08_overview.html#feature-scaling">Feature Scaling</a></li>
<li><a href="08_overview.html#feature-engineering">Feature Engineering</a></li>
<li><a href="08_overview.html#discretization">Discretization</a></li>
</ul></li>
<li><a href="08_overview.html#model-selection">Model Selection</a></li>
<li><a href="08_overview.html#model-assessment">Model Assessment</a></li>
</ul></li>
<li class="has-sub"><a href="1000_appendix.html#appendix">Appendix</a><ul>
<li><a href="1000_appendix.html#bias-variance-demo">Bias Variance Demo</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="regularization" class="section level1">
<h1>Regularization</h1>
<p><span class="newthought">It is important to note</span> that a model fit to a single data set might do very well with the data at hand, but then suffer when predicting independent data<label for="tufte-sn-11" class="margin-toggle sidenote-number">11</label><input type="checkbox" id="tufte-sn-11" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">11</span> In terminology we will discuss further later, such models might have low bias but notable variance.</span>. Also, oftentimes we are interested in a ‘best’ subset of predictors among a great many, and in this scenario the estimated coefficients are overly optimistic. This general issue can be improved by shrinking estimates toward zero, such that some of the performance in the initial fit is sacrificed for improvement with regard to prediction.</p>
<p>Penalized estimation will provide estimates with some shrinkage, and we can use it with little additional effort with our common procedures. Concretely, let’s apply this to the standard linear model, where we are finding estimates of <span class="math inline">\(\beta\)</span> that minimize the squared error loss.</p>
<p><span class="math display">\[\hat\beta = \underset{\beta}{\mathrm{arg\, min}} \sum{(y-X\beta)^2}\]</span></p>
<p>In words, we’re finding the coefficients that minimize the sum of the squared residuals. With the approach to regression here we just add a penalty component to the procedure as follows.</p>
<p><span class="math display">\[\hat\beta = \underset{\beta}{\mathrm{arg\, min}} \sum{(y-X\beta)^2} + \lambda\overset{p}{\underset{j=1}{\sum}}{\left|\beta_j\right|}\]</span></p>
<p>In the above equation, <span class="math inline">\(\lambda\)</span> is our penalty term<label for="tufte-sn-12" class="margin-toggle sidenote-number">12</label><input type="checkbox" id="tufte-sn-12" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">12</span> This can be set explicitly or also estimated via a validation approach. As we do not know it beforehand, we can estimate it on a validation data set (not the test set) and then use the estimated value when estimating coefficients via cross-validation with the test set. We will talk more about validation later.</span> for which larger values will result in more shrinkage. It’s applied to the <span class="math inline">\(L_1\)</span> or Manhattan norm of the coefficients, <span class="math inline">\(\beta_1,\beta_2...\beta_p\)</span>, i.e. <em>not including the intercept</em> <span class="math inline">\(\beta_0\)</span>, and is the sum of their absolute values (commonly referred to as the <span class="emph">lasso</span><label for="tufte-sn-13" class="margin-toggle sidenote-number">13</label><input type="checkbox" id="tufte-sn-13" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">13</span> See Tibshirani (1996) Regression shrinkage and selection via the lasso.</span>). For generalized linear and additive models, we can conceptually express a penalized likelihood as follows:</p>
<p><span class="math display">\[l_p(\beta) = l(\beta) - \lambda\overset{p}{\underset{j=1}{\sum}}{\left|\beta_j\right|}\]</span></p>
<p>As we are maximizing the likelihood the penalty is a subtraction, but nothing inherently different is shown. This basic idea of adding a penalty term will be applied to all machine learning approaches, but as shown, we can apply such a tool to classical methods to boost prediction performance.</p>
<p><span class="marginnote">Interestingly, the lasso and ridge regression results can be seen as a Bayesian approach using a zero mean Laplace and Normal prior distribution respectively for the <span class="math inline">\(\beta_j\)</span>.</span>It should be noted that we can go about the regularization in different ways. For example, using the squared <span class="math inline">\(L_2\)</span> norm results in what is called <span class="emph"></span> (a.k.a. Tikhonov regularization), and using a weighted combination of the lasso and ridge penalties gives us <span class="emph">elastic net</span> regularization.</p>
<div id="r-example-1" class="section level2">
<h2>R Example</h2>
<p>In the following example, we take a look at the lasso approach for a standard linear model. We add the regularization component, with a fixed penalty <span class="math inline">\(\lambda\)</span> for demonstration purposes<label for="tufte-sn-14" class="margin-toggle sidenote-number">14</label><input type="checkbox" id="tufte-sn-14" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">14</span> As noted previously, in practice <span class="math inline">\(\lambda\)</span> would be estimated via some validation procedure.</span>. However you should insert your own values for <span class="math inline">\(\lambda\)</span> in the <span class="func">optim</span> line to see how the results are affected.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sqerrloss_reg =<span class="st"> </span>function(beta, X, y, <span class="dt">lambda=</span>.<span class="dv">1</span>){
  mu =<span class="st"> </span>X%*%beta
  <span class="kw">sum</span>((y-mu)^<span class="dv">2</span>) +<span class="st"> </span>lambda*<span class="kw">sum</span>(<span class="kw">abs</span>(beta[-<span class="dv">1</span>]))
}

regularized_result =<span class="st"> </span><span class="kw">optim</span>(<span class="dt">par=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>), <span class="dt">fn=</span>sqerrloss_reg, <span class="dt">X=</span>X, <span class="dt">y=</span>y, <span class="dt">method=</span><span class="st">&#39;BFGS&#39;</span>)
<span class="kw">rbind</span>(<span class="kw">c</span>(our_func$par, our_func$value), 
      <span class="kw">c</span>(<span class="kw">coef</span>(lm_result),<span class="kw">sum</span>(<span class="kw">resid</span>(lm_result)^<span class="dv">2</span>)), 
      <span class="kw">c</span>(regularized_result$par, regularized_result$value) )</code></pre></div>
<pre><code>     (Intercept)         X1        X2         
[1,]   0.1350654 -0.6331715 0.5238113 87.78187
[2,]   0.1350654 -0.6331715 0.5238113 87.78187
[3,]   0.1349579 -0.6325923 0.5232982 87.89751</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create test data</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>:<span class="dv">3</span>)
N_test =<span class="st"> </span><span class="dv">50</span>
X_test =<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>, <span class="kw">rnorm</span>(N_test), <span class="kw">rnorm</span>(N_test))
y_test =<span class="st"> </span><span class="kw">rnorm</span>(N_test, X_test%*%beta, <span class="dt">sd=</span><span class="dv">1</span>)

<span class="co"># squared error loss</span>
<span class="kw">crossprod</span>(y_test -<span class="st"> </span><span class="kw">predict</span>(lm_result, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(X_test[,-<span class="dv">1</span>])))  </code></pre></div>
<pre><code>         [,1]
[1,] 44.38851</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">crossprod</span>(y_test -<span class="st"> </span>X_test%*%regularized_result$par)</code></pre></div>
<pre><code>         [,1]
[1,] 44.37794</code></pre>
<p>From the above, we can see in this case that the penalized coefficients have indeed shrunk toward zero slightly, while the residual sum of squares has increased just a tad.</p>
<p>In general, we can add the same sort of penalty to any number of models, such as logistic regression, neural net models, recommender systems etc. The primary goal again is to hopefully increase our ability to generalize the selected model to new data. Note that the estimates produced are in fact biased, but we have decreased the variance with new predictions as a counterbalance, and this brings us to the topic of the next section.</p>

</div>
</div>
<p style="text-align: center;">
<a href="03_loss.html"><button class="btn btn-default">Previous</button></a>
<a href="05_biasvar.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
