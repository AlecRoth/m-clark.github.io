<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="An Introduction to Machine Learning" />
<meta property="og:type" content="book" />
<meta property="og:url" content="https://m-clark.github.io/docs/" />
<meta property="og:image" content="https://m-clark.github.io/docs/img/nineteeneightyR.png" />
<meta property="og:description" content="An introduction to machine learning for applied researchers." />
<meta name="github-repo" content="m-clark/introduction-to-machine-learning/" />


<meta name="date" content="2017-04-21" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="An introduction to machine learning for applied researchers.">

<title>An Introduction to Machine Learning</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<script src="libs/jquery-1.12.4/jquery.min.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>


<link rel="stylesheet" href="standard_html.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="mytufte.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#section"></a></li>
<li><a href="00_preface.html#preface">Preface</a></li>
<li class="has-sub"><a href="01_intro.html#introduction">Introduction</a><ul>
<li><a href="01_intro.html#explanation-prediction">Explanation &amp; Prediction</a></li>
<li><a href="01_intro.html#some-terminology">Some Terminology</a></li>
</ul></li>
<li class="has-sub"><a href="02_tools.html#tools">Tools</a><ul>
<li><a href="02_tools.html#the-standard-linear-model">The Standard Linear Model</a></li>
<li><a href="02_tools.html#logistic-regression">Logistic Regression</a></li>
<li class="has-sub"><a href="02_tools.html#expansions-of-those-tools">Expansions of Those Tools</a><ul>
<li><a href="02_tools.html#generalized-linear-models">Generalized Linear Models</a></li>
<li><a href="02_tools.html#generalized-additive-models">Generalized Additive Models</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="03_loss.html#the-loss-function">The Loss Function</a><ul>
<li class="has-sub"><a href="03_loss.html#continuous-outcomes">Continuous Outcomes</a><ul>
<li><a href="03_loss.html#squared-error">Squared Error</a></li>
<li><a href="03_loss.html#absolute-error">Absolute Error</a></li>
<li><a href="03_loss.html#negative-log-likelihood">Negative Log-likelihood</a></li>
<li><a href="03_loss.html#r-example">R Example</a></li>
</ul></li>
<li class="has-sub"><a href="03_loss.html#categorical-outcomes">Categorical Outcomes</a><ul>
<li><a href="03_loss.html#misclassification">Misclassification</a></li>
<li><a href="03_loss.html#binomial-log-likelihood">Binomial log-likelihood</a></li>
<li><a href="03_loss.html#exponential">Exponential</a></li>
<li><a href="03_loss.html#hinge-loss">Hinge Loss</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="04_regularization.html#regularization">Regularization</a><ul>
<li><a href="04_regularization.html#r-example-1">R Example</a></li>
</ul></li>
<li class="has-sub"><a href="05_biasvar.html#bias-variance-tradeoff">Bias-Variance Tradeoff</a><ul>
<li><a href="05_biasvar.html#bias-variance">Bias <em>&amp;</em> Variance</a></li>
<li><a href="05_biasvar.html#the-tradeoff">The Tradeoff</a></li>
<li class="has-sub"><a href="05_biasvar.html#diagnosing-bias-variance-issues-possible-solutions">Diagnosing Bias-Variance Issues <em>&amp;</em> Possible Solutions</a><ul>
<li><a href="05_biasvar.html#worst-case-scenario">Worst Case Scenario</a></li>
<li><a href="05_biasvar.html#high-variance">High Variance</a></li>
<li><a href="05_biasvar.html#high-bias">High Bias</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="06_crossval.html#cross-validation">Cross-Validation</a><ul>
<li><a href="06_crossval.html#adding-another-validation-set">Adding Another Validation Set</a></li>
<li class="has-sub"><a href="06_crossval.html#k-fold-cross-validation">K-fold Cross-Validation</a><ul>
<li><a href="06_crossval.html#leave-one-out-cross-validation">Leave-one-out Cross-Validation</a></li>
</ul></li>
<li><a href="06_crossval.html#bootstrap">Bootstrap</a></li>
<li><a href="06_crossval.html#other-stuff">Other Stuff</a></li>
</ul></li>
<li class="has-sub"><a href="07_models.html#model-assessment-selection">Model Assessment &amp; Selection</a><ul>
<li><a href="07_models.html#beyond-classification-accuracy-other-measures-of-performance">Beyond Classification Accuracy: Other Measures of Performance</a></li>
</ul></li>
<li class="has-sub"><a href="08_overview.html#process-overview">Process Overview</a><ul>
<li class="has-sub"><a href="08_overview.html#data-preparation">Data Preparation</a><ul>
<li><a href="08_overview.html#define-data-and-data-partitions">Define Data and Data Partitions</a></li>
<li><a href="08_overview.html#feature-scaling">Feature Scaling</a></li>
<li><a href="08_overview.html#feature-engineering">Feature Engineering</a></li>
<li><a href="08_overview.html#discretization">Discretization</a></li>
</ul></li>
<li><a href="08_overview.html#model-selection">Model Selection</a></li>
<li><a href="08_overview.html#model-assessment">Model Assessment</a></li>
</ul></li>
<li class="has-sub"><a href="1000_appendix.html#appendix">Appendix</a><ul>
<li><a href="1000_appendix.html#bias-variance-demo">Bias Variance Demo</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="bias-variance-tradeoff" class="section level1">
<h1>Bias-Variance Tradeoff</h1>
<p><span class="newthought">In most of science,</span> we are concerned with reducing uncertainty in our knowledge of some phenomenon. The more we know about the factors involved or related to some outcome of interest, the better we can predict that outcome upon the influx of new information. The initial step is to take the data at hand, and determine how well a model or set of models fit the data in various fashions. In many applications however, this part is also more or less the end of the game as well<label for="tufte-sn-15" class="margin-toggle sidenote-number">15</label><input type="checkbox" id="tufte-sn-15" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">15</span> I should note that I do not make any particular claim about the quality of such analysis. In many situations the cost of data collection is very high, and for all the current enamorment with ‘big’ data, a lot of folks will never have access to big data for their situation (e.g. certain clinical populations). In these situations getting new data for which one might make predictions is extremely difficult.</span>.</p>
<p>Unfortunately, such an approach in which we only fit models to one data set does not give a very good sense of <span class="emph">generalization</span> performance, i.e. the performance we would see with new data. While typically not reported, most researchers, if they are spending appropriate time with the data, are actually testing a great many models, for which the ‘best’ is then provided in detail in the end report. Without some generalization performance check however, such performance is overstated when it comes to new data.</p>
<p>In the following consider a standard linear model scenario, e.g. with squared-error loss function and perhaps some regularization, and a data set in which we split the data in some random fashion into a <span class="emph">training set</span> , for initial model fit, and a<span class="emph">test set</span>, which is a separate and independent data set, to measure generalization performance<label for="tufte-sn-16" class="margin-toggle sidenote-number">16</label><input type="checkbox" id="tufte-sn-16" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">16</span> In typical situations there are parameters specific to some analytical technique for which one would have no knowledge, and which must be estimated along with the usual parameters of the standard models. The <span class="math inline">\(\lambda\)</span> penalty parameter in regularized regression is one example of such a <span class="emph">tuning parameter</span>. In the best case scenario, we would also have a <span class="emph">validation set</span>, where we could determine appropriate values for such parameters based on performance with the validation data set, and then assess generalization performance on the test set when the final model has been chosen. However, methods are available to us in which we can approximate the validation step in other ways.</span>. We note <span class="emph">training error</span> as the (average) loss over the training set, and <span class="emph">test error</span> as the (average) prediction error obtained when a model resulting from the training data is fit to the test data. So in addition to the previously noted goal of finding the ‘best’ model (<span class="emph">model selection</span>), we are interested further in estimating the prediction error with new data (<span class="emph">model performance</span>).</p>
<div id="bias-variance" class="section level2">
<h2>Bias <em>&amp;</em> Variance</h2>
<p>Conceptually<span class="marginnote">Much of the following is essentially a paraphrase of parts of <span class="citation">Hastie, Tibshirani, and Friedman (<label for="tufte-mn-6" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-6" class="margin-toggle">2009<span class="marginnote">Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition</em>. 2nd ed. 2009. Corr. 10th Printing. Springer.</span>)</span> (chapters 2 and 7).</span>, with the standard model <span class="math inline">\(Y = f(X) + \epsilon\)</span> with we can think of the expected prediction error at a specific input <span class="math inline">\(X = x_0\)</span> as:</p>
<p><span class="math display">\[\text{Error}_{x_0} = \text{Irreducible Error} + \text{Bias}^2 + \text{Variance}\]</span></p>
<p>To better understand this, think of training models over and over, each time with new training data, but testing each model at <span class="math inline">\(x_0\)</span>. The <span class="math inline">\(\text{Error}_{x_0}\)</span> is the average, or expected value of the MSE in this scenario, or <span class="math inline">\(E(y - \hat f(x))^2\)</span>. In other words, we have three components to our general notion of prediction error:</p>
<ul>
<li>Irreducible error: The variance of the (new test) target (<span class="math inline">\(\sigma^2_\epsilon\)</span>). This is unavoidable, since our <span class="math inline">\(y\)</span> is measured with error..</li>
<li><span class="math inline">\(Bias^2\)</span>: the amount the <em>average</em> of our estimate varies from the true mean (<span class="math inline">\(E(\hat f) - f\)</span>). This is typically the result of trying to model the complexity of nature with something much simple that the human brain can understand.</li>
<li>Variance: the amount by which our prediction would change if we estimated it using a different training data set (<span class="math inline">\(Var(\hat f)\)</span>)</li>
</ul>
<p>Slightly more formally, we can present this as follows, with <span class="math inline">\(h_0\)</span> our estimated (hypothesized) value at <span class="math inline">\(x_0\)</span>:</p>
<p><span class="math display">\[\text{Error}_{x_0} = Var(\epsilon) + (\text{E}[h_0] - f(x_0))^2 + Var(h_0)\]</span></p>
<p>The latter two make up the mean squared error in our previous demonstration. While they are under our control, they compete with one another such that oftentimes we improve one at the detriment of the other.</p>
</div>
<div id="the-tradeoff" class="section level2">
<h2>The Tradeoff</h2>
<p>Outlining a general procedure, we start by noting the prediction error on a training data set with multiple models of varying complexity (e.g. increasing the number of predictor variables, adding polynomial terms, including interactions), and then assess the performance of the chosen models in terms of prediction error on the test set. We then perform the same activity for a total of 100 simulated data sets, for each level of complexity.</p>
<p><span class="marginnote"><img src="img/biasvar2.svg" style="display:block; margin: 0 auto;" width=75%></span>The results from this process might look like the image to the right taken from <span class="citation">Hastie, Tibshirani, and Friedman (<label for="tufte-mn-7" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-7" class="margin-toggle">2009<span class="marginnote">Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition</em>. 2nd ed. 2009. Corr. 10th Printing. Springer.</span>)</span>. With regard to the training data, we have <span class="math inline">\(\mathrm{error}_{train}\)</span> for one hundred training sets for each level of model complexity. The bold blue line notes this average error over the 100 sets by model complexity. The bold red line the average test error (<span class="math inline">\(\mathrm{error}_{test}\)</span>) across the 100 test data sets.</p>
<p>Ideally we’d like to see low bias and variance, but things are not so easy. One thing we can see clearly is that <span class="math inline">\(\mathrm{error}_{train}\)</span> is not a good estimate of <span class="math inline">\(\mathrm{error}_{test}\)</span>, which is now our focus in terms of performance. If we think of the training error as what we would see in typical research where one does everything with a single data set, we are using the same data set to fit the model and assess error. As the model is adapted to that data set specifically, it will be overly optimistic in the estimate of the error, that optimism being the difference between the error rate we see based on the training data versus the average of what we would get with many test data sets. We can think of this as a problem of overfitting to the training data. Models that do not incorporate any regularization or validation process of any kind are likely overfit to the data presented.</p>
<p>Generally speaking, the more complex the model, the lower the bias, but the higher the variance, as depicted in the graphic. Specifically however, the situation is more nuanced, where type of problem (classification with 0-1 loss vs. continuous response with squared error loss<label for="tufte-sn-17" class="margin-toggle sidenote-number">17</label><input type="checkbox" id="tufte-sn-17" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">17</span> See Friedman (1996) <em>On Bias, Variance, 0/1 Loss and the Curse of Dimensionality</em> for the unusual situations that can arise in dealing with classification error with regard to bias and variance.</span>) and technique (a standard linear model vs. regularized fit) will exhibit different bias-variance relationships.</p>
<p><span class="marginnote"><img src="img/biasvartarget.svg" style="display:block; margin: 0 auto;"> <br> Figure adapted from <span class="citation">Domingos (<label for="tufte-mn-8" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-8" class="margin-toggle">2012<span class="marginnote">Domingos, Pedro. 2012. “A Few Useful Things to Know About Machine Learning.” <em>Commun. ACM</em> 55 (10). doi:<a href="https://doi.org/10.1145/2347736.2347755">10.1145/2347736.2347755</a>.</span>)</span>.</span></p>
</div>
<div id="diagnosing-bias-variance-issues-possible-solutions" class="section level2">
<h2>Diagnosing Bias-Variance Issues <em>&amp;</em> Possible Solutions</h2>
<p>Let’s assume a regularized linear model with a standard data split into training and test sets. We will describe different scenarios with possible solutions.</p>
<div id="worst-case-scenario" class="section level3">
<h3>Worst Case Scenario</h3>
<p>Starting with the worst case scenario, poor models may exhibit high bias and high variance. One thing that will not help this situation (perhaps contrary to intuition) is adding more data, i.e. increasing N. You can’t make a silk purse out of a sow’s ear (<a href="https://libraries.mit.edu/archives/exhibits/purse/"><em>usually</em></a>), and adding more data just gives you a more accurate picture of how awful your model is. One might need to rework the model, e.g. adding new predictors or creating them via interaction terms, polynomials, or other smooth functions as in additive models, or simply collecting better and/or more relevant data.<span class="marginnote"><img src="img/biasvar_gp.svg" style="display:block; margin: 0 auto;"> <br> Figure inspired by <span class="citation">Murphy (<label for="tufte-mn-9" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-9" class="margin-toggle">2012<span class="marginnote">Murphy, Kevin P. 2012. <em>Machine Learning: A Probabilistic Perspective</em>. The MIT Press.</span>)</span> (figure 6.5) showing the bias-variance tradeoff. Sample (left) and average (right) fits of linear regression using a Gaussian radial basis function expansion. The green line represents the true relationship. The top row shows low variance between one fit and the next (left) but notable bias (right) in that the average fit is off. Compare to the less regularized (high variance, low bias) situation of the bottom row. See the <span class="pack">kernlab</span> package for the fitting function used.</span></p>
</div>
<div id="high-variance" class="section level3">
<h3>High Variance</h3>
<p>When variance is a problem, our training error is low while test error is relatively high (overfitting problem). Implementing more shrinkage or other penalization to model complexity may help with the issue. In this case more data may help as well.</p>
</div>
<div id="high-bias" class="section level3">
<h3>High Bias</h3>
<p>With bias issues, our training error is high and test error is not too different from training error (underfitting problem). Adding new predictors/features, e.g. interaction terms, polynomials etc., can help here. Additionally reducing the penalty parameter <span class="math inline">\(\lambda\)</span> would also work with even less effort, though generally it should be estimated rather than explicitly set.</p>

</div>
</div>
</div>
<p style="text-align: center;">
<a href="04_regularization.html"><button class="btn btn-default">Previous</button></a>
<a href="06_crossval.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
