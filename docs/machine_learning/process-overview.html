<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>An Introduction to Machine Learning</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="An introduction to machine learning for applied researchers.">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="An Introduction to Machine Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://m-clark.github.io/docs/" />
  <meta property="og:image" content="https://m-clark.github.io/docs/img/nineteeneightyR.png" />
  <meta property="og:description" content="An introduction to machine learning for applied researchers." />
  <meta name="github-repo" content="m-clark/introduction-to-machine-learning/" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="An Introduction to Machine Learning" />
  
  <meta name="twitter:description" content="An introduction to machine learning for applied researchers." />
  <meta name="twitter:image" content="https://m-clark.github.io/docs/img/nineteeneightyR.png" />



<meta name="date" content="2017-04-21">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="model-assessment-selection.html">
<link rel="next" href="appendix.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="standard_html.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://m-clark.github.io/workshops/stars/"><span style="font-size:125%; font-variant:small-caps; font-style:italic; color:#ff5503">Machine Learning</span></a></li>

<li class="divider"></li>
<li><a href="index.html#section"></a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#explanation-prediction"><i class="fa fa-check"></i>Explanation &amp; Prediction</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#some-terminology"><i class="fa fa-check"></i>Some Terminology</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="tools.html"><a href="tools.html"><i class="fa fa-check"></i>Tools</a><ul>
<li class="chapter" data-level="" data-path="tools.html"><a href="tools.html#the-standard-linear-model"><i class="fa fa-check"></i>The Standard Linear Model</a></li>
<li class="chapter" data-level="" data-path="tools.html"><a href="tools.html#logistic-regression"><i class="fa fa-check"></i>Logistic Regression</a></li>
<li class="chapter" data-level="" data-path="tools.html"><a href="tools.html#expansions-of-those-tools"><i class="fa fa-check"></i>Expansions of Those Tools</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="the-loss-function.html"><a href="the-loss-function.html"><i class="fa fa-check"></i>The Loss Function</a><ul>
<li class="chapter" data-level="" data-path="the-loss-function.html"><a href="the-loss-function.html#continuous-outcomes"><i class="fa fa-check"></i>Continuous Outcomes</a></li>
<li class="chapter" data-level="" data-path="the-loss-function.html"><a href="the-loss-function.html#categorical-outcomes"><i class="fa fa-check"></i>Categorical Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i>Regularization</a><ul>
<li class="chapter" data-level="" data-path="regularization.html"><a href="regularization.html#r-example-1"><i class="fa fa-check"></i>R Example</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html"><i class="fa fa-check"></i>Bias-Variance Tradeoff</a><ul>
<li><a href="bias-variance-tradeoff.html#bias-variance">Bias <em>&amp;</em> Variance</a></li>
<li class="chapter" data-level="" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#the-tradeoff"><i class="fa fa-check"></i>The Tradeoff</a></li>
<li><a href="bias-variance-tradeoff.html#diagnosing-bias-variance-issues-possible-solutions">Diagnosing Bias-Variance Issues <em>&amp;</em> Possible Solutions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i>Cross-Validation</a><ul>
<li class="chapter" data-level="" data-path="cross-validation.html"><a href="cross-validation.html#adding-another-validation-set"><i class="fa fa-check"></i>Adding Another Validation Set</a></li>
<li class="chapter" data-level="" data-path="cross-validation.html"><a href="cross-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i>K-fold Cross-Validation</a></li>
<li class="chapter" data-level="" data-path="cross-validation.html"><a href="cross-validation.html#bootstrap"><i class="fa fa-check"></i>Bootstrap</a></li>
<li class="chapter" data-level="" data-path="cross-validation.html"><a href="cross-validation.html#other-stuff"><i class="fa fa-check"></i>Other Stuff</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="model-assessment-selection.html"><a href="model-assessment-selection.html"><i class="fa fa-check"></i>Model Assessment &amp; Selection</a><ul>
<li class="chapter" data-level="" data-path="model-assessment-selection.html"><a href="model-assessment-selection.html#beyond-classification-accuracy-other-measures-of-performance"><i class="fa fa-check"></i>Beyond Classification Accuracy: Other Measures of Performance</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="process-overview.html"><a href="process-overview.html"><i class="fa fa-check"></i>Process Overview</a><ul>
<li class="chapter" data-level="" data-path="process-overview.html"><a href="process-overview.html#data-preparation"><i class="fa fa-check"></i>Data Preparation</a></li>
<li class="chapter" data-level="" data-path="process-overview.html"><a href="process-overview.html#model-selection"><i class="fa fa-check"></i>Model Selection</a></li>
<li class="chapter" data-level="" data-path="process-overview.html"><a href="process-overview.html#model-assessment"><i class="fa fa-check"></i>Model Assessment</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#bias-variance-demo"><i class="fa fa-check"></i>Bias Variance Demo</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://m-clark.github.io" target="blank" style="font-size:150%; font-variant:small-caps; color:#ff5500">Michael Clark</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="process-overview" class="section level1">
<h1>Process Overview</h1>
<p><span class="newthought">Despite the facade of a polished product</span> one finds in published research, most of the approach with the statistical analysis of data is full of data preparation, starts and stops, debugging, re-analysis, tweaking and fine-tuning etc. Statistical learning is no different in this sense. Before we begin with explicit examples, it might be best to give a general overview of the path we’ll take.</p>
<div id="data-preparation" class="section level2">
<h2>Data Preparation</h2>
<p>As with any typical statistical project, probably most of the time will be spent preparing the data for analysis. Data is never ready to analyze right away, and careful checks must be made in order to ensure the integrity of the information. This would include correcting errors of entry, noting extreme values, possibly imputing missing data and so forth. In addition to these typical activities, we will discuss a couple more things to think about during this initial data examination when engaged in machine learning.</p>
<div id="define-data-and-data-partitions" class="section level3">
<h3>Define Data and Data Partitions</h3>
<p>As we have noted previously, ideally we will have enough data to create a hold-out, test, or validation data set. This would be some random partition of the data such that we could safely conclude that the data in the test set comes from the same population as the training set. The training set is used to fit the initial models at various tuning parameter settings, with a ‘best’ model being that which satisfies some criterion on the validation set (or via a general validation process). With final model and parameters chosen, generalization error will be assessed with the the performance of the final model on the test data.</p>
</div>
<div id="feature-scaling" class="section level3">
<h3>Feature Scaling</h3>
<p>Even with standard regression modeling, centering continuous variables (subtracting the mean) is a good idea so that intercepts and zero points in general are meaningful. Standardizing variables so that they have similar variances or ranges will help some procedures find their minimums faster. Another common transformation is <em>min-max</em> normalization<a href="#fn22" class="footnoteRef" id="fnref22"><sup>22</sup></a>, which will transfer a scale to a new one of some chosen minimum and maximum. Note that whatever approach is done, it must be done <em>after</em> any explicit separation of data. So if you have separate training and test sets, they should be scaled separately.</p>
</div>
<div id="feature-engineering" class="section level3">
<h3>Feature Engineering</h3>
<p>If we’re lucky we’ll have ideas on potential combinations or other transformations of the predictors we have available. For example, in typical social science research there are two-way interactions one is often predisposed to try, or perhaps one can sum multiple items to a single scale score that may be more relevant. Another common technique is to use a dimension reduction scheme such as principal components, but this can (and probably should) actually be an implemented algorithm in the ML process<a href="#fn23" class="footnoteRef" id="fnref23"><sup>23</sup></a>.</p>
<p>One can implement a variety of such approaches in ML as well to create additional potentially relevant features, even automatically, but as a reminder, a key concern is overfitting, and doing broad construction of this sort with no contextual guidance would potentially be prone to such a pitfall. In other cases it may simply be not worth the time expense.</p>
</div>
<div id="discretization" class="section level3">
<h3>Discretization</h3>
<p>While there may be some contextual exceptions to the rule, it is generally a pretty bad idea in standard statistical modeling to discretize/categorize continuous variables<a href="#fn24" class="footnoteRef" id="fnref24"><sup>24</sup></a>. However some ML procedures will work better (or just faster) if dealing with discrete valued predictors rather than continuous. Others even require them; for example, logic regression needs binary input. While one could pick arbitrary intervals and cutpoints in an unsupervised fashion such as picking equal range bins or equal frequency bins, there are supervised algorithmic approaches that will use the information in the data to produce some ‘optimal’ discretization.</p>
<p>It’s generally not a good idea to force things in data analysis, and given that a lot of data situations will be highly mixed, it seems easier to simply apply some scaling to preserve the inherent relationships in the data. Again though, if one has only a relative few continuous variables or a context in which it makes sense to, it’s probably better to leave continuous variables as such.</p>
</div>
</div>
<div id="model-selection" class="section level2">
<h2>Model Selection</h2>
<p>With data prepared and ready to analyze, one can use a validation process to come up with a viable model. Use an optimization procedure or a simple grid search over a set of specific values to examine models at different tuning parameters. Perhaps make a finer search once an initial range of good performing values is found, though one should not split hairs over arbitrarily close performance. Select a ‘best’ model given some criterion such as overall accuracy, or if concerned about over fitting, select the simplest model within one standard error of the accuracy of the best, or perhaps the simplest within X% of the best model. For highly skewed classes, one might need to use a different measure of performance besides accuracy. If one has a great many predictor variables, one may use the model selection process to select features that are ‘most important’.</p>
</div>
<div id="model-assessment" class="section level2">
<h2>Model Assessment</h2>
<p>With tuning parameters/features chosen, we then examine performance on the independent test set (or via some validation procedure). For classification problems, consider other statistics besides accuracy as measures of performance, especially if classes are unbalanced. Consider other analytical techniques that are applicable and compare performance among the different approaches. One can even combine disparate models’ predictions to possibly create an even better classifier<a href="#fn25" class="footnoteRef" id="fnref25"><sup>25</sup></a>.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="22">
<li id="fn22"><p>See, for example, the <span class="func">rescale</span> function in the <span class="pack">scales</span> package.<a href="process-overview.html#fnref22">↩</a></p></li>
<li id="fn23"><p>For example, via principal components or partial least squares regression.<a href="process-overview.html#fnref23">↩</a></p></li>
<li id="fn24"><p>See <span class="citation">Harrell (<a href="#ref-harrell2015regression">2015</a>)</span> for a good summary of reasons why not to.<a href="process-overview.html#fnref24">↩</a></p></li>
<li id="fn25"><p>The topic of ensembles is briefly noted later.<a href="process-overview.html#fnref25">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="model-assessment-selection.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appendix.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "san-serif",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/08_overview.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection"
},
"highlight": "pygments",
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
