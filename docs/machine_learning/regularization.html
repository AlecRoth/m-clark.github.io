<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>An Introduction to Machine Learning</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="An introduction to machine learning for applied researchers.">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="An Introduction to Machine Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://m-clark.github.io/docs/" />
  <meta property="og:image" content="https://m-clark.github.io/docs/img/nineteeneightyR.png" />
  <meta property="og:description" content="An introduction to machine learning for applied researchers." />
  <meta name="github-repo" content="m-clark/introduction-to-machine-learning/" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="An Introduction to Machine Learning" />
  
  <meta name="twitter:description" content="An introduction to machine learning for applied researchers." />
  <meta name="twitter:image" content="https://m-clark.github.io/docs/img/nineteeneightyR.png" />



<meta name="date" content="2017-04-21">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="the-loss-function.html">
<link rel="next" href="bias-variance-tradeoff.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="standard_html.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://m-clark.github.io/workshops/stars/"><span style="font-size:125%; font-variant:small-caps; font-style:italic; color:#ff5503">Machine Learning</span></a></li>

<li class="divider"></li>
<li><a href="index.html#section"></a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#explanation-prediction"><i class="fa fa-check"></i>Explanation &amp; Prediction</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#some-terminology"><i class="fa fa-check"></i>Some Terminology</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="tools.html"><a href="tools.html"><i class="fa fa-check"></i>Tools</a><ul>
<li class="chapter" data-level="" data-path="tools.html"><a href="tools.html#the-standard-linear-model"><i class="fa fa-check"></i>The Standard Linear Model</a></li>
<li class="chapter" data-level="" data-path="tools.html"><a href="tools.html#logistic-regression"><i class="fa fa-check"></i>Logistic Regression</a></li>
<li class="chapter" data-level="" data-path="tools.html"><a href="tools.html#expansions-of-those-tools"><i class="fa fa-check"></i>Expansions of Those Tools</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="the-loss-function.html"><a href="the-loss-function.html"><i class="fa fa-check"></i>The Loss Function</a><ul>
<li class="chapter" data-level="" data-path="the-loss-function.html"><a href="the-loss-function.html#continuous-outcomes"><i class="fa fa-check"></i>Continuous Outcomes</a></li>
<li class="chapter" data-level="" data-path="the-loss-function.html"><a href="the-loss-function.html#categorical-outcomes"><i class="fa fa-check"></i>Categorical Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i>Regularization</a><ul>
<li class="chapter" data-level="" data-path="regularization.html"><a href="regularization.html#r-example-1"><i class="fa fa-check"></i>R Example</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html"><i class="fa fa-check"></i>Bias-Variance Tradeoff</a><ul>
<li><a href="bias-variance-tradeoff.html#bias-variance">Bias <em>&amp;</em> Variance</a></li>
<li class="chapter" data-level="" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#the-tradeoff"><i class="fa fa-check"></i>The Tradeoff</a></li>
<li><a href="bias-variance-tradeoff.html#diagnosing-bias-variance-issues-possible-solutions">Diagnosing Bias-Variance Issues <em>&amp;</em> Possible Solutions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i>Cross-Validation</a><ul>
<li class="chapter" data-level="" data-path="cross-validation.html"><a href="cross-validation.html#adding-another-validation-set"><i class="fa fa-check"></i>Adding Another Validation Set</a></li>
<li class="chapter" data-level="" data-path="cross-validation.html"><a href="cross-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i>K-fold Cross-Validation</a></li>
<li class="chapter" data-level="" data-path="cross-validation.html"><a href="cross-validation.html#bootstrap"><i class="fa fa-check"></i>Bootstrap</a></li>
<li class="chapter" data-level="" data-path="cross-validation.html"><a href="cross-validation.html#other-stuff"><i class="fa fa-check"></i>Other Stuff</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="model-assessment-selection.html"><a href="model-assessment-selection.html"><i class="fa fa-check"></i>Model Assessment &amp; Selection</a><ul>
<li class="chapter" data-level="" data-path="model-assessment-selection.html"><a href="model-assessment-selection.html#beyond-classification-accuracy-other-measures-of-performance"><i class="fa fa-check"></i>Beyond Classification Accuracy: Other Measures of Performance</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="process-overview.html"><a href="process-overview.html"><i class="fa fa-check"></i>Process Overview</a><ul>
<li class="chapter" data-level="" data-path="process-overview.html"><a href="process-overview.html#data-preparation"><i class="fa fa-check"></i>Data Preparation</a></li>
<li class="chapter" data-level="" data-path="process-overview.html"><a href="process-overview.html#model-selection"><i class="fa fa-check"></i>Model Selection</a></li>
<li class="chapter" data-level="" data-path="process-overview.html"><a href="process-overview.html#model-assessment"><i class="fa fa-check"></i>Model Assessment</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#bias-variance-demo"><i class="fa fa-check"></i>Bias Variance Demo</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://m-clark.github.io" target="blank" style="font-size:150%; font-variant:small-caps; color:#ff5500">Michael Clark</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regularization" class="section level1">
<h1>Regularization</h1>
<p><span class="newthought">It is important to note</span> that a model fit to a single data set might do very well with the data at hand, but then suffer when predicting independent data<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a>. Also, oftentimes we are interested in a ‘best’ subset of predictors among a great many, and in this scenario the estimated coefficients are overly optimistic. This general issue can be improved by shrinking estimates toward zero, such that some of the performance in the initial fit is sacrificed for improvement with regard to prediction.</p>
<p>Penalized estimation will provide estimates with some shrinkage, and we can use it with little additional effort with our common procedures. Concretely, let’s apply this to the standard linear model, where we are finding estimates of <span class="math inline">\(\beta\)</span> that minimize the squared error loss.</p>
<p><span class="math display">\[\hat\beta = \underset{\beta}{\mathrm{arg\, min}} \sum{(y-X\beta)^2}\]</span></p>
<p>In words, we’re finding the coefficients that minimize the sum of the squared residuals. With the approach to regression here we just add a penalty component to the procedure as follows.</p>
<p><span class="math display">\[\hat\beta = \underset{\beta}{\mathrm{arg\, min}} \sum{(y-X\beta)^2} + \lambda\overset{p}{\underset{j=1}{\sum}}{\left|\beta_j\right|}\]</span></p>
<p>In the above equation, <span class="math inline">\(\lambda\)</span> is our penalty term<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a> for which larger values will result in more shrinkage. It’s applied to the <span class="math inline">\(L_1\)</span> or Manhattan norm of the coefficients, <span class="math inline">\(\beta_1,\beta_2...\beta_p\)</span>, i.e. <em>not including the intercept</em> <span class="math inline">\(\beta_0\)</span>, and is the sum of their absolute values (commonly referred to as the <span class="emph">lasso</span><a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a>). For generalized linear and additive models, we can conceptually express a penalized likelihood as follows:</p>
<p><span class="math display">\[l_p(\beta) = l(\beta) - \lambda\overset{p}{\underset{j=1}{\sum}}{\left|\beta_j\right|}\]</span></p>
<p>As we are maximizing the likelihood the penalty is a subtraction, but nothing inherently different is shown. This basic idea of adding a penalty term will be applied to all machine learning approaches, but as shown, we can apply such a tool to classical methods to boost prediction performance.</p>
<p><span class="marginnote">Interestingly, the lasso and ridge regression results can be seen as a Bayesian approach using a zero mean Laplace and Normal prior distribution respectively for the <span class="math inline">\(\beta_j\)</span>.</span>It should be noted that we can go about the regularization in different ways. For example, using the squared <span class="math inline">\(L_2\)</span> norm results in what is called <span class="emph"></span> (a.k.a. Tikhonov regularization), and using a weighted combination of the lasso and ridge penalties gives us <span class="emph">elastic net</span> regularization.</p>
<div id="r-example-1" class="section level2">
<h2>R Example</h2>
<p>In the following example, we take a look at the lasso approach for a standard linear model. We add the regularization component, with a fixed penalty <span class="math inline">\(\lambda\)</span> for demonstration purposes<a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a>. However you should insert your own values for <span class="math inline">\(\lambda\)</span> in the <span class="func">optim</span> line to see how the results are affected.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sqerrloss_reg =<span class="st"> </span>function(beta, X, y, <span class="dt">lambda=</span>.<span class="dv">1</span>){
  mu =<span class="st"> </span>X%*%beta
  <span class="kw">sum</span>((y-mu)^<span class="dv">2</span>) +<span class="st"> </span>lambda*<span class="kw">sum</span>(<span class="kw">abs</span>(beta[-<span class="dv">1</span>]))
}

regularized_result =<span class="st"> </span><span class="kw">optim</span>(<span class="dt">par=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>), <span class="dt">fn=</span>sqerrloss_reg, <span class="dt">X=</span>X, <span class="dt">y=</span>y, <span class="dt">method=</span><span class="st">&#39;BFGS&#39;</span>)
<span class="kw">rbind</span>(<span class="kw">c</span>(our_func$par, our_func$value), 
      <span class="kw">c</span>(<span class="kw">coef</span>(lm_result),<span class="kw">sum</span>(<span class="kw">resid</span>(lm_result)^<span class="dv">2</span>)), 
      <span class="kw">c</span>(regularized_result$par, regularized_result$value) )</code></pre></div>
<pre><code>     (Intercept)         X1        X2         
[1,]   0.1350654 -0.6331715 0.5238113 87.78187
[2,]   0.1350654 -0.6331715 0.5238113 87.78187
[3,]   0.1349579 -0.6325923 0.5232982 87.89751</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create test data</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>:<span class="dv">3</span>)
N_test =<span class="st"> </span><span class="dv">50</span>
X_test =<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>, <span class="kw">rnorm</span>(N_test), <span class="kw">rnorm</span>(N_test))
y_test =<span class="st"> </span><span class="kw">rnorm</span>(N_test, X_test%*%beta, <span class="dt">sd=</span><span class="dv">1</span>)

<span class="co"># squared error loss</span>
<span class="kw">crossprod</span>(y_test -<span class="st"> </span><span class="kw">predict</span>(lm_result, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(X_test[,-<span class="dv">1</span>])))  </code></pre></div>
<pre><code>         [,1]
[1,] 44.38851</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">crossprod</span>(y_test -<span class="st"> </span>X_test%*%regularized_result$par)</code></pre></div>
<pre><code>         [,1]
[1,] 44.37794</code></pre>
<p>From the above, we can see in this case that the penalized coefficients have indeed shrunk toward zero slightly, while the residual sum of squares has increased just a tad.</p>
<p>In general, we can add the same sort of penalty to any number of models, such as logistic regression, neural net models, recommender systems etc. The primary goal again is to hopefully increase our ability to generalize the selected model to new data. Note that the estimates produced are in fact biased, but we have decreased the variance with new predictions as a counterbalance, and this brings us to the topic of the next section.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="11">
<li id="fn11"><p>In terminology we will discuss further later, such models might have low bias but notable variance.<a href="regularization.html#fnref11">↩</a></p></li>
<li id="fn12"><p>This can be set explicitly or also estimated via a validation approach. As we do not know it beforehand, we can estimate it on a validation data set (not the test set) and then use the estimated value when estimating coefficients via cross-validation with the test set. We will talk more about validation later.<a href="regularization.html#fnref12">↩</a></p></li>
<li id="fn13"><p>See Tibshirani (1996) Regression shrinkage and selection via the lasso.<a href="regularization.html#fnref13">↩</a></p></li>
<li id="fn14"><p>As noted previously, in practice <span class="math inline">\(\lambda\)</span> would be estimated via some validation procedure.<a href="regularization.html#fnref14">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="the-loss-function.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bias-variance-tradeoff.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "san-serif",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/04_regularization.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection"
},
"highlight": "pygments",
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
