<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>An Introduction to Machine Learning</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="An introduction to machine learning for applied researchers.">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="An Introduction to Machine Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://m-clark.github.io/docs/" />
  <meta property="og:image" content="https://m-clark.github.io/docs/img/nineteeneightyR.png" />
  <meta property="og:description" content="An introduction to machine learning for applied researchers." />
  <meta name="github-repo" content="m-clark/introduction-to-machine-learning/" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="An Introduction to Machine Learning" />
  
  <meta name="twitter:description" content="An introduction to machine learning for applied researchers." />
  <meta name="twitter:image" content="https://m-clark.github.io/docs/img/nineteeneightyR.png" />



<meta name="date" content="2017-04-21">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="tools.html">
<link rel="next" href="regularization.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="standard_html.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://m-clark.github.io/workshops/stars/"><span style="font-size:125%; font-variant:small-caps; font-style:italic; color:#ff5503">Machine Learning</span></a></li>

<li class="divider"></li>
<li><a href="index.html#section"></a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#explanation-prediction"><i class="fa fa-check"></i>Explanation &amp; Prediction</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#some-terminology"><i class="fa fa-check"></i>Some Terminology</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="tools.html"><a href="tools.html"><i class="fa fa-check"></i>Tools</a><ul>
<li class="chapter" data-level="" data-path="tools.html"><a href="tools.html#the-standard-linear-model"><i class="fa fa-check"></i>The Standard Linear Model</a></li>
<li class="chapter" data-level="" data-path="tools.html"><a href="tools.html#logistic-regression"><i class="fa fa-check"></i>Logistic Regression</a></li>
<li class="chapter" data-level="" data-path="tools.html"><a href="tools.html#expansions-of-those-tools"><i class="fa fa-check"></i>Expansions of Those Tools</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="the-loss-function.html"><a href="the-loss-function.html"><i class="fa fa-check"></i>The Loss Function</a><ul>
<li class="chapter" data-level="" data-path="the-loss-function.html"><a href="the-loss-function.html#continuous-outcomes"><i class="fa fa-check"></i>Continuous Outcomes</a></li>
<li class="chapter" data-level="" data-path="the-loss-function.html"><a href="the-loss-function.html#categorical-outcomes"><i class="fa fa-check"></i>Categorical Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i>Regularization</a><ul>
<li class="chapter" data-level="" data-path="regularization.html"><a href="regularization.html#r-example-1"><i class="fa fa-check"></i>R Example</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html"><i class="fa fa-check"></i>Bias-Variance Tradeoff</a><ul>
<li><a href="bias-variance-tradeoff.html#bias-variance">Bias <em>&amp;</em> Variance</a></li>
<li class="chapter" data-level="" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#the-tradeoff"><i class="fa fa-check"></i>The Tradeoff</a></li>
<li><a href="bias-variance-tradeoff.html#diagnosing-bias-variance-issues-possible-solutions">Diagnosing Bias-Variance Issues <em>&amp;</em> Possible Solutions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i>Cross-Validation</a><ul>
<li class="chapter" data-level="" data-path="cross-validation.html"><a href="cross-validation.html#adding-another-validation-set"><i class="fa fa-check"></i>Adding Another Validation Set</a></li>
<li class="chapter" data-level="" data-path="cross-validation.html"><a href="cross-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i>K-fold Cross-Validation</a></li>
<li class="chapter" data-level="" data-path="cross-validation.html"><a href="cross-validation.html#bootstrap"><i class="fa fa-check"></i>Bootstrap</a></li>
<li class="chapter" data-level="" data-path="cross-validation.html"><a href="cross-validation.html#other-stuff"><i class="fa fa-check"></i>Other Stuff</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="model-assessment-selection.html"><a href="model-assessment-selection.html"><i class="fa fa-check"></i>Model Assessment &amp; Selection</a><ul>
<li class="chapter" data-level="" data-path="model-assessment-selection.html"><a href="model-assessment-selection.html#beyond-classification-accuracy-other-measures-of-performance"><i class="fa fa-check"></i>Beyond Classification Accuracy: Other Measures of Performance</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="process-overview.html"><a href="process-overview.html"><i class="fa fa-check"></i>Process Overview</a><ul>
<li class="chapter" data-level="" data-path="process-overview.html"><a href="process-overview.html#data-preparation"><i class="fa fa-check"></i>Data Preparation</a></li>
<li class="chapter" data-level="" data-path="process-overview.html"><a href="process-overview.html#model-selection"><i class="fa fa-check"></i>Model Selection</a></li>
<li class="chapter" data-level="" data-path="process-overview.html"><a href="process-overview.html#model-assessment"><i class="fa fa-check"></i>Model Assessment</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#bias-variance-demo"><i class="fa fa-check"></i>Bias Variance Demo</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://m-clark.github.io" target="blank" style="font-size:150%; font-variant:small-caps; color:#ff5500">Michael Clark</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-loss-function" class="section level1">
<h1>The Loss Function</h1>
<p><span class="newthought">Given a set of predictor variables</span> <span class="math inline">\(X\)</span> and some target <span class="math inline">\(y\)</span>, we look for some function <span class="math inline">\(f(X)\)</span> to make predictions of y from those input variables. We also need a function to penalize errors in prediction- a <span class="emph">loss function</span>, <span class="math inline">\(L(Y, f(X))\)</span>. With a chosen loss function, we then find the model which will minimize loss, generally speaking. We will start with the familiar and note a couple others that might be used.</p>
<div id="continuous-outcomes" class="section level2">
<h2>Continuous Outcomes</h2>
<div id="squared-error" class="section level3">
<h3>Squared Error</h3>
<p>The classic loss function for linear models with continuous response is the squared error loss function, or the residual sum of squares.</p>
<p><span class="math display">\[L(Y, f(X)) = \sum(y-f(X))^2\]</span></p>
</div>
<div id="absolute-error" class="section level3">
<h3>Absolute Error</h3>
<p>For an approach more robust to extreme observations, we might choose absolute rather than squared error as follows. In this case, predictions are a conditional median rather than a conditional mean.</p>
<p><span class="math display">\[L(Y, f(X)) = \sum|(y-f(X))|\]</span></p>
</div>
<div id="negative-log-likelihood" class="section level3">
<h3>Negative Log-likelihood</h3>
<p>We can also think of our usual likelihood methods learned in a standard applied statistics course as incorporating a loss function that is the negative log-likelihood pertaining to the model of interest. If we assume a normal distribution for the response we can note the loss function as:</p>
<p><span class="math display">\[L(Y, f(X)) = n\ln{\sigma} + \sum \frac{1}{2\sigma^2}(y-f(X))^2\]</span></p>
<p>In this case it would converge to the same answer as the squared error/least squares solution.</p>
</div>
<div id="r-example" class="section level3">
<h3>R Example</h3>
<p>The following provides code that one could use with the <span class="func">optim</span> function in R to find estimates of regression coefficients (beta) that minimize the squared error. X is a design matrix of our predictor variables with the first column a vector of 1s in order to estimate the intercept. y is the continuous variable to be modeled<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sqerrloss =<span class="st"> </span>function(beta, X, y){
  mu =<span class="st"> </span>X%*%beta
  <span class="kw">sum</span>((y-mu)^<span class="dv">2</span>)
}

<span class="co"># data setup</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)
N =<span class="st"> </span><span class="dv">100</span>
X =<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>, <span class="kw">rnorm</span>(N), <span class="kw">rnorm</span>(N))
beta =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, -.<span class="dv">5</span>, .<span class="dv">5</span>)
y =<span class="st">  </span><span class="kw">rnorm</span>(N, X%*%beta, <span class="dt">sd=</span><span class="dv">1</span>)

<span class="co"># results</span>
our_func =<span class="st"> </span><span class="kw">optim</span>(<span class="dt">par=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>), <span class="dt">fn=</span>sqerrloss, <span class="dt">X=</span>X, <span class="dt">y=</span>y, <span class="dt">method=</span><span class="st">&#39;BFGS&#39;</span>)
lm_result =<span class="st"> </span><span class="kw">lm</span>(y ~<span class="st"> </span>., <span class="kw">data.frame</span>(X[,-<span class="dv">1</span>]))  <span class="co"># check with lm </span>
<span class="kw">rbind</span>(<span class="kw">c</span>(our_func$par, our_func$value), <span class="kw">c</span>(<span class="kw">coef</span>(lm_result), <span class="kw">sum</span>(<span class="kw">resid</span>(lm_result)^<span class="dv">2</span>)))</code></pre></div>
<pre><code>     (Intercept)         X1        X2         
[1,]   0.1350654 -0.6331715 0.5238113 87.78187
[2,]   0.1350654 -0.6331715 0.5238113 87.78187</code></pre>
</div>
</div>
<div id="categorical-outcomes" class="section level2">
<h2>Categorical Outcomes</h2>
<p>Here we’ll also look at some loss functions useful in classification problems. Note that there is not necessary exclusion in loss functions for continuous vs. categorical outcomes<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a>. Generally though we’ll have different options.</p>
<div id="misclassification" class="section level3">
<h3>Misclassification</h3>
<p>Probably the most straightforward is misclassification, or 0-1 loss. If we note <span class="math inline">\(f\)</span> as the prediction, and for convenience we assume a [-1,1] response instead of a [0,1] response:</p>
<p><span class="math display">\[L(Y, f(X)) = \sum I(y\neq \mathrm{sign}(f))\]</span></p>
<p>In the above, <span class="math inline">\(I\)</span> is the indicator function, and so we are simply summing misclassifications.</p>
</div>
<div id="binomial-log-likelihood" class="section level3">
<h3>Binomial log-likelihood</h3>
<p><span class="math display">\[L(Y, f(X)) = \sum log(1 + e^{-2yf})\]</span></p>
<p>The above is in deviance form<span class="marginnote">Deviance can conceptually be thought of as the GLM version of residual variance.</span>, but is equivalent to binomial log likelihood if <span class="math inline">\(y\)</span> is on the 0-1 scale.</p>
</div>
<div id="exponential" class="section level3">
<h3>Exponential</h3>
<p>Exponential loss is yet another loss function at our disposal.</p>
<p><span class="math display">\[L(Y, f(X)) = \sum e^{-yf}\]</span></p>
</div>
<div id="hinge-loss" class="section level3">
<h3>Hinge Loss</h3>
<p>A final loss function to consider, typically used with support vector machines, is the hinge loss function.</p>
<p><span class="math display">\[L(Y, f(X)) = \max(1-yf, 0)\]</span></p>
<p>Here negative values of <span class="math inline">\(yf\)</span> are misclassifications, and so correct classifications do not contribute to the loss. We could also note it as <span class="math inline">\(\sum (1-yf)_+\)</span> , i.e. summing only those positive values of <span class="math inline">\(1-yf\)</span>.</p>
<p><span class="marginnote"><img src="img/lossfuncs.png" style="display:block; margin: 0 auto;"></span></p>
<p>Which of these might work best may be specific to the situation, but the gist is that they penalize negative values (misclassifications) more heavily and increasingly so the worse the misclassification (except for misclassification error, which penalizes all misclassifications equally), with their primary difference in how heavy that penalty is. At right is a depiction of the loss as a functions above, taken from <span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-hastie_elements_2009">2009</a>)</span>.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-hastie_elements_2009">
<p>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition</em>. 2nd ed. 2009. Corr. 10th Printing. Springer.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="9">
<li id="fn9"><p>Type ?optim at the console for more detail.<a href="the-loss-function.html#fnref9">↩</a></p></li>
<li id="fn10"><p>For example, if dealing with probabilities, we technically could use minimize squared errors in the case of classification also. We could use a maximum likelihood for either setting (or minimize the negative log likelihood to turn it into a loss function).<a href="the-loss-function.html#fnref10">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="tools.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regularization.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "san-serif",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/03_loss.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection"
},
"highlight": "pygments",
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
