<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="An Introduction to Machine Learning" />
<meta property="og:type" content="book" />
<meta property="og:url" content="https://m-clark.github.io/docs/" />
<meta property="og:image" content="https://m-clark.github.io/docs/img/nineteeneightyR.png" />
<meta property="og:description" content="An introduction to machine learning for applied researchers." />
<meta name="github-repo" content="m-clark/introduction-to-machine-learning/" />


<meta name="date" content="2017-04-21" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="An introduction to machine learning for applied researchers.">

<title>An Introduction to Machine Learning</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<script src="libs/jquery-1.12.4/jquery.min.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>


<link rel="stylesheet" href="standard_html.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="mytufte.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#section"></a></li>
<li><a href="00_preface.html#preface">Preface</a></li>
<li class="has-sub"><a href="01_intro.html#introduction">Introduction</a><ul>
<li><a href="01_intro.html#explanation-prediction">Explanation &amp; Prediction</a></li>
<li><a href="01_intro.html#some-terminology">Some Terminology</a></li>
</ul></li>
<li class="has-sub"><a href="02_tools.html#tools">Tools</a><ul>
<li><a href="02_tools.html#the-standard-linear-model">The Standard Linear Model</a></li>
<li><a href="02_tools.html#logistic-regression">Logistic Regression</a></li>
<li class="has-sub"><a href="02_tools.html#expansions-of-those-tools">Expansions of Those Tools</a><ul>
<li><a href="02_tools.html#generalized-linear-models">Generalized Linear Models</a></li>
<li><a href="02_tools.html#generalized-additive-models">Generalized Additive Models</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="03_loss.html#the-loss-function">The Loss Function</a><ul>
<li class="has-sub"><a href="03_loss.html#continuous-outcomes">Continuous Outcomes</a><ul>
<li><a href="03_loss.html#squared-error">Squared Error</a></li>
<li><a href="03_loss.html#absolute-error">Absolute Error</a></li>
<li><a href="03_loss.html#negative-log-likelihood">Negative Log-likelihood</a></li>
<li><a href="03_loss.html#r-example">R Example</a></li>
</ul></li>
<li class="has-sub"><a href="03_loss.html#categorical-outcomes">Categorical Outcomes</a><ul>
<li><a href="03_loss.html#misclassification">Misclassification</a></li>
<li><a href="03_loss.html#binomial-log-likelihood">Binomial log-likelihood</a></li>
<li><a href="03_loss.html#exponential">Exponential</a></li>
<li><a href="03_loss.html#hinge-loss">Hinge Loss</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="04_regularization.html#regularization">Regularization</a><ul>
<li><a href="04_regularization.html#r-example-1">R Example</a></li>
</ul></li>
<li class="has-sub"><a href="05_biasvar.html#bias-variance-tradeoff">Bias-Variance Tradeoff</a><ul>
<li><a href="05_biasvar.html#bias-variance">Bias <em>&amp;</em> Variance</a></li>
<li><a href="05_biasvar.html#the-tradeoff">The Tradeoff</a></li>
<li class="has-sub"><a href="05_biasvar.html#diagnosing-bias-variance-issues-possible-solutions">Diagnosing Bias-Variance Issues <em>&amp;</em> Possible Solutions</a><ul>
<li><a href="05_biasvar.html#worst-case-scenario">Worst Case Scenario</a></li>
<li><a href="05_biasvar.html#high-variance">High Variance</a></li>
<li><a href="05_biasvar.html#high-bias">High Bias</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="06_crossval.html#cross-validation">Cross-Validation</a><ul>
<li><a href="06_crossval.html#adding-another-validation-set">Adding Another Validation Set</a></li>
<li class="has-sub"><a href="06_crossval.html#k-fold-cross-validation">K-fold Cross-Validation</a><ul>
<li><a href="06_crossval.html#leave-one-out-cross-validation">Leave-one-out Cross-Validation</a></li>
</ul></li>
<li><a href="06_crossval.html#bootstrap">Bootstrap</a></li>
<li><a href="06_crossval.html#other-stuff">Other Stuff</a></li>
</ul></li>
<li class="has-sub"><a href="07_models.html#model-assessment-selection">Model Assessment &amp; Selection</a><ul>
<li><a href="07_models.html#beyond-classification-accuracy-other-measures-of-performance">Beyond Classification Accuracy: Other Measures of Performance</a></li>
</ul></li>
<li class="has-sub"><a href="08_overview.html#process-overview">Process Overview</a><ul>
<li class="has-sub"><a href="08_overview.html#data-preparation">Data Preparation</a><ul>
<li><a href="08_overview.html#define-data-and-data-partitions">Define Data and Data Partitions</a></li>
<li><a href="08_overview.html#feature-scaling">Feature Scaling</a></li>
<li><a href="08_overview.html#feature-engineering">Feature Engineering</a></li>
<li><a href="08_overview.html#discretization">Discretization</a></li>
</ul></li>
<li><a href="08_overview.html#model-selection">Model Selection</a></li>
<li><a href="08_overview.html#model-assessment">Model Assessment</a></li>
</ul></li>
<li class="has-sub"><a href="1000_appendix.html#appendix">Appendix</a><ul>
<li><a href="1000_appendix.html#bias-variance-demo">Bias Variance Demo</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="process-overview" class="section level1">
<h1>Process Overview</h1>
<p><span class="newthought">Despite the facade of a polished product</span> one finds in published research, most of the approach with the statistical analysis of data is full of data preparation, starts and stops, debugging, re-analysis, tweaking and fine-tuning etc. Statistical learning is no different in this sense. Before we begin with explicit examples, it might be best to give a general overview of the path we’ll take.</p>
<div id="data-preparation" class="section level2">
<h2>Data Preparation</h2>
<p>As with any typical statistical project, probably most of the time will be spent preparing the data for analysis. Data is never ready to analyze right away, and careful checks must be made in order to ensure the integrity of the information. This would include correcting errors of entry, noting extreme values, possibly imputing missing data and so forth. In addition to these typical activities, we will discuss a couple more things to think about during this initial data examination when engaged in machine learning.</p>
<div id="define-data-and-data-partitions" class="section level3">
<h3>Define Data and Data Partitions</h3>
<p>As we have noted previously, ideally we will have enough data to create a hold-out, test, or validation data set. This would be some random partition of the data such that we could safely conclude that the data in the test set comes from the same population as the training set. The training set is used to fit the initial models at various tuning parameter settings, with a ‘best’ model being that which satisfies some criterion on the validation set (or via a general validation process). With final model and parameters chosen, generalization error will be assessed with the the performance of the final model on the test data.</p>
</div>
<div id="feature-scaling" class="section level3">
<h3>Feature Scaling</h3>
<p>Even with standard regression modeling, centering continuous variables (subtracting the mean) is a good idea so that intercepts and zero points in general are meaningful. Standardizing variables so that they have similar variances or ranges will help some procedures find their minimums faster. Another common transformation is <em>min-max</em> normalization<label for="tufte-sn-22" class="margin-toggle sidenote-number">22</label><input type="checkbox" id="tufte-sn-22" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">22</span> See, for example, the <span class="func">rescale</span> function in the <span class="pack">scales</span> package.</span>, which will transfer a scale to a new one of some chosen minimum and maximum. Note that whatever approach is done, it must be done <em>after</em> any explicit separation of data. So if you have separate training and test sets, they should be scaled separately.</p>
</div>
<div id="feature-engineering" class="section level3">
<h3>Feature Engineering</h3>
<p>If we’re lucky we’ll have ideas on potential combinations or other transformations of the predictors we have available. For example, in typical social science research there are two-way interactions one is often predisposed to try, or perhaps one can sum multiple items to a single scale score that may be more relevant. Another common technique is to use a dimension reduction scheme such as principal components, but this can (and probably should) actually be an implemented algorithm in the ML process<label for="tufte-sn-23" class="margin-toggle sidenote-number">23</label><input type="checkbox" id="tufte-sn-23" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">23</span> For example, via principal components or partial least squares regression.</span>.</p>
<p>One can implement a variety of such approaches in ML as well to create additional potentially relevant features, even automatically, but as a reminder, a key concern is overfitting, and doing broad construction of this sort with no contextual guidance would potentially be prone to such a pitfall. In other cases it may simply be not worth the time expense.</p>
</div>
<div id="discretization" class="section level3">
<h3>Discretization</h3>
<p>While there may be some contextual exceptions to the rule, it is generally a pretty bad idea in standard statistical modeling to discretize/categorize continuous variables<label for="tufte-sn-24" class="margin-toggle sidenote-number">24</label><input type="checkbox" id="tufte-sn-24" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">24</span> See <span class="citation">Harrell (<label for="tufte-mn-11" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-11" class="margin-toggle">2015<span class="marginnote">Harrell, F. 2015. <em>Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis</em>. Springer Series in Statistics. Springer International Publishing.</span>)</span> for a good summary of reasons why not to.</span>. However some ML procedures will work better (or just faster) if dealing with discrete valued predictors rather than continuous. Others even require them; for example, logic regression needs binary input. While one could pick arbitrary intervals and cutpoints in an unsupervised fashion such as picking equal range bins or equal frequency bins, there are supervised algorithmic approaches that will use the information in the data to produce some ‘optimal’ discretization.</p>
<p>It’s generally not a good idea to force things in data analysis, and given that a lot of data situations will be highly mixed, it seems easier to simply apply some scaling to preserve the inherent relationships in the data. Again though, if one has only a relative few continuous variables or a context in which it makes sense to, it’s probably better to leave continuous variables as such.</p>
</div>
</div>
<div id="model-selection" class="section level2">
<h2>Model Selection</h2>
<p>With data prepared and ready to analyze, one can use a validation process to come up with a viable model. Use an optimization procedure or a simple grid search over a set of specific values to examine models at different tuning parameters. Perhaps make a finer search once an initial range of good performing values is found, though one should not split hairs over arbitrarily close performance. Select a ‘best’ model given some criterion such as overall accuracy, or if concerned about over fitting, select the simplest model within one standard error of the accuracy of the best, or perhaps the simplest within X% of the best model. For highly skewed classes, one might need to use a different measure of performance besides accuracy. If one has a great many predictor variables, one may use the model selection process to select features that are ‘most important’.</p>
</div>
<div id="model-assessment" class="section level2">
<h2>Model Assessment</h2>
<p>With tuning parameters/features chosen, we then examine performance on the independent test set (or via some validation procedure). For classification problems, consider other statistics besides accuracy as measures of performance, especially if classes are unbalanced. Consider other analytical techniques that are applicable and compare performance among the different approaches. One can even combine disparate models’ predictions to possibly create an even better classifier<label for="tufte-sn-25" class="margin-toggle sidenote-number">25</label><input type="checkbox" id="tufte-sn-25" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">25</span> The topic of ensembles is briefly noted later.</span>.</p>

</div>
</div>
<p style="text-align: center;">
<a href="07_models.html"><button class="btn btn-default">Previous</button></a>
<a href="1000_appendix.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
