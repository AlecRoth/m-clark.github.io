[
["index.html", "Engaging the Web with \\(\\dots\\) Introduction Prerequisites", " Engaging the Web with \\(\\dots\\) Michael Clark Statistician Lead 2016-10-19 Introduction Well known for its statistical capabilities, R can also be used for web-scraping, connecting with websites via APIs, html documents and presentations, interactive visualizations, dashboards, and even building entire, possibly interactive, websites. This document and related talk will provide an overview of web-based use of R. Conceptual introductions, package synopses, and small demonstrations will be presented. Prerequisites Basic R knowledge is all that is required, and not even statistical knowledge of any kind is assumed. The focus will be on breadth (e.g. common use) rather than depth. One of the things you’ll want to do before getting started is to peruse the Task Views for Web Technologies. Color coding: emphasis package function object/class link "],
["web-scraping.html", "Web scraping Direct data download Key concepts The basic approach Examples Issues", " Web scraping We begin with a discussion on web scraping. The term itself is ambiguous, and could potentially mean anything1, but the gist is that there is something out on the web (or some connected machine) that we want, and we’d like to use R to get it. This section won’t (at least for now) get into lower level utilities such as that provided by httr or Rcurl, though some packages will be using them under the hood. Instead, focus will be on higher-level approaches with an eye toward common tasks. As a starting point, open a browser and go to a website of your preference. For most browsers, Ctrl+U will open up the underlying html file. If you’re on a typical webpage it will almost certainly look like a mess of html, JavaScript, XML and possibly other things. Simpler pages are more easily discernible, while more complex/secure pages are not. The take home message is that what you want is represented by something in there, and you’ll have to know something about that structure in order to get it. Unfortunately, a lot of web design is quite poor2, which will make your job difficult. Even when the sole purpose of a site is to provide data, you can almost be sure that the simplest/most flexible path will not be taken to do so. Thankfully you can use R and possibly other tools to make the process at least somewhat less painful. Direct data download One thing to be aware of is that you can get data from the web that are in typical formats just as you would any file on your own machine. For example: mydata = read.csv(&#39;http://somewebsite/data.csv&#39;) I wouldn’t even call this web scraping, just as you wouldn’t if you were getting a file off a network connection. However, if you’re just starting out with R, it’s important to know this is possible. In any case, do not make things more difficult than they need to be- if the file is available just grab it. Key concepts The web works in mysterious ways, but you don’t have to know a lot about it to use it or benefit from its data. Many people have had their own websites for years without knowing any html. For our purposes, it helps to know a little and is actually required, at least so that we know what to look for. Common elements or tags3 of a webpage include things like div table and ul and body, and within such things our data will be found. Some common ones include: div and span: used when other elements are not appropriate but one still wants to define classes, ids etc. p paragraph a link (technically ‘anchor’) ul ol lists table tr td tables, table rows, table data (cell) h# i.e. h1 h2 etc. headers img images Consider the following: &lt;h3&gt; This header denotes a section of text &lt;img src=&quot;picture_of_cat.png&quot;&gt;&lt;/img&gt; &lt;p&gt; This is a paragraph! &lt;/p&gt; &lt;p&gt; Here is another! This one has &lt;a href=&quot;www.someurl.com&quot;&gt;a link&lt;/a&gt;! &lt;/p&gt; As an example, if we wanted to scrape every paragraph that had the word paragraph in it, we’d need to first get all the &lt;p&gt; elements and work from there. Just like in other programming languages, these parts of a webpage have their own class, e.g. &lt;table class=&quot;wikitable sortable&quot;&gt;, and this allows even easier access to the objects of specific interest. Take a look at the source for the Wikipedia page for towns in Michigan. The id is another attribute, or way to note specific types of objects. Unlike classes, which might be used over and over within a page, ids are unique, and thus for web scraping, probably more useful when appropriate to the task. Also, while elements can have multiple classes, they can have only one id. Any particular element might also have different attributes that specify things like style, alignment, color, file source etc. The sad thing is that attributes are greatly underutilized in general. For example, perhaps you found a page with several html tables of data, one for each year. It would have been simple to add an id = year to each one, enabling you to grab just the specific year(s) of interest, but it’ll be rare that you’d find a page that goes that far. Part of this could be due to the fact that much content is auto-generated via WYSIWYG editors, but these are not very professional pages if so, which might cause concern for the data given such a source. Take a look at the openfda.gov site: It has lots of attributes for practically every element, which would make getting stuff off the page fairly easy, if it didn’t already have an APIs. In any case, you’ll at least need to know the tag within which the data or link to it may be found. Classes and ids will help to further drill down the web page content, but often won’t be available. In such a case you might have to grab, for example, all the links, and then use some regular expression to grab only the one you want. See this link for more on relations between attributes and elements. The basic approach To summarize the basic approach, we can outline a few steps: If a direct link or API is available use it4 Inspect the page to find the relevant tags within which the content is found Start with a base URL Use the relevant R packages to parse the base page Extract the desired content Examples One of the packages that can make scraping easy is rvest, which is modeled after/inspired by the Beautiful Soup module in Python5. As the goal here is to get you quickly started, we won’t inundate you with a lot of packages yet. I recommend getting comfy with rvest and moving to others when needed. Tables Wikipedia A lot of Wikipedia has pages ripe for the scraping, but it also has an API, so that we can use it as an example later as well. Between the page layout and rvest it will be very easy to get the content. So let’s do so. Back to the page of towns in Michigan. Let’s see if we can get the table of towns with their type, county, and population. First things first, we need to get the page. page = &#39;https://en.wikipedia.org/wiki/List_of_cities,_villages,_and_townships_in_Michigan&#39; library(rvest) towns = read_html(page) str(towns) List of 2 $ node:&lt;externalptr&gt; $ doc :&lt;externalptr&gt; - attr(*, &quot;class&quot;)= chr [1:2] &quot;xml_document&quot; &quot;xml_node&quot; towns {xml_document} &lt;html class=&quot;client-nojs&quot; lang=&quot;en&quot; dir=&quot;ltr&quot;&gt; [1] &lt;head&gt;\\n &lt;meta charset=&quot;UTF-8&quot;/&gt;\\n &lt;title&gt;List of cities, villages, and townships in Michigan - Wikipedia, the ... [2] &lt;body class=&quot;mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject page-List_of_cities_villages_and_townshi ... The result of read_html is an xml_document class object. For the uninitiated, XML is a markup language (Extensible Markup Language) like HTML, and which allows one to access its parts as nodes in tree6, where parents have children and grandchildren etc. It will require further parsing in order to get what we want, but it was easy enough to snag the page. Let’s look at some of the nodes. html_nodes(towns, &#39;ul&#39;) %&gt;% head # unordered lists (first few) {xml_nodeset (6)} [1] &lt;ul&gt;&lt;li&gt;&lt;a href=&quot;#A&quot;&gt;A&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;a href=&quot;#B&quot;&gt;B&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;a href=&quot;#C&quot;&gt;C&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;a href=&quot;#D&quot;&gt;D&lt;/a&gt; ... [2] &lt;ul&gt;&lt;li&gt;&lt;a href=&quot;#See_also&quot;&gt;See also&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;a href=&quot;#References&quot;&gt;References&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;a href=&quot;#Exter ... [3] &lt;ul&gt;&lt;li&gt;&lt;a href=&quot;/wiki/Administrative_divisions_of_Michigan&quot; title=&quot;Administrative divisions of Michigan&quot;&gt;Adminis ... [4] &lt;ul&gt;&lt;li&gt;&lt;a rel=&quot;nofollow&quot; class=&quot;external text&quot; href=&quot;https://web.archive.org/web/20040908085409/http://www.vienn ... [5] &lt;ul&gt;&lt;li&gt;&lt;a rel=&quot;nofollow&quot; class=&quot;external text&quot; href=&quot;http://www.census.gov/geo/www/gazetteer/places2k.html&quot;&gt;Cens ... [6] &lt;ul&gt;&lt;li class=&quot;nv-view&quot;&gt;&lt;a href=&quot;/wiki/Template:Michigan&quot; title=&quot;Template:Michigan&quot;&gt;&lt;abbr title=&quot;View this templa ... html_nodes(towns, &#39;a&#39;) %&gt;% head # links {xml_nodeset (6)} [1] &lt;a id=&quot;top&quot;/&gt; [2] &lt;a href=&quot;#mw-head&quot;&gt;navigation&lt;/a&gt; [3] &lt;a href=&quot;#p-search&quot;&gt;search&lt;/a&gt; [4] &lt;a href=&quot;/wiki/Wikipedia:Citing_sources&quot; title=&quot;Wikipedia:Citing sources&quot;&gt;list of references&lt;/a&gt; [5] &lt;a href=&quot;/wiki/Wikipedia:External_links&quot; title=&quot;Wikipedia:External links&quot;&gt;external links&lt;/a&gt; [6] &lt;a href=&quot;/wiki/Wikipedia:Citing_sources#Inline_citations&quot; title=&quot;Wikipedia:Citing sources&quot;&gt;inline citations&lt;/a&gt; html_nodes(towns, &#39;table&#39;) # tables {xml_nodeset (5)} [1] &lt;table class=&quot;plainlinks metadata ambox ambox-style ambox-No_footnotes&quot; role=&quot;presentation&quot;&gt;\\n &lt;tr&gt;&lt;td class=&quot;mb ... [2] &lt;table class=&quot;wikitable sortable&quot;&gt;\\n &lt;tr&gt;&lt;th&gt;Place&lt;/th&gt;\\n&lt;th&gt;Type&lt;/th&gt;\\n&lt;th&gt;County&lt;/th&gt;\\n&lt;th&gt;2010 Population&lt;/th ... [3] &lt;table role=&quot;presentation&quot; class=&quot;mbox-small plainlinks sistersitebox&quot; style=&quot;border:1px solid #aaa;background-co ... [4] &lt;table class=&quot;nowraplinks collapsible autocollapse navbox-inner&quot; style=&quot;border-spacing:0;background:transparent;c ... [5] &lt;table class=&quot;nowraplinks collapsible autocollapse navbox-inner&quot; style=&quot;border-spacing:0;background:transparent;c ... Perhaps now you are getting a sense of what we can possibly extract from this page. It turns out that what we want is a table element, and in particular, the one that is of class wikitable sortable. This will make things very easy. But what if we didn’t know what the element was? it turns out there are tools like SelectorGadget7 that can be used to inspect the elements of any webpage. SelectorGadget can be added as a browser extension, which would allow you to easily turn it on and off as needed. The following depicts what the Michigan towns looks like when using SelectorGadget and selecting a link. It highlights everything else in the page that is of a similar type (i.e. other links), as well as provides the css/xml info that we would need to grab that particular item. As mentioned, we want the wikitable sortable class table, so lets grab that. The rvest package comes with the html_table function, that will grab any table and attempt to put it into a data.frame object. I only show the first couple because one of the tables comes out pretty messy at the R console. str(html_table(towns)[1:3]) List of 3 $ :&#39;data.frame&#39;: 1 obs. of 2 variables: ..$ X1: logi NA ..$ X2: chr &quot;This article includes a list of references, related reading or external links, but its sources remain unclear because it lacks &quot;| __truncated__ $ :&#39;data.frame&#39;: 2185 obs. of 4 variables: ..$ Place : chr [1:2185] &quot;Acme&quot; &quot;Acme Township&quot; &quot;Ada Township&quot; &quot;Adams Township, Arenac County&quot; ... ..$ Type : chr [1:2185] &quot;unincorporated community&quot; &quot;township&quot; &quot;township&quot; &quot;township&quot; ... ..$ County : chr [1:2185] &quot;&quot; &quot;&quot; &quot;Kent&quot; &quot;Arenac&quot; ... ..$ 2010 Population: chr [1:2185] &quot;&quot; &quot;4,375&quot; &quot;13,142&quot; &quot;563&quot; ... $ :&#39;data.frame&#39;: 1 obs. of 2 variables: ..$ X1: logi NA ..$ X2: chr &quot;Wikimedia Commons has media related to Populated places in Michigan.&quot; We know which class object we want though, so we could have used html_nodes to grab only that object and work with it. html_nodes(towns, &#39;.wikitable.sortable&#39;) %&gt;% html_table() %&gt;% .[[1]] %&gt;% head() Place Type County 2010 Population 1 Acme unincorporated community 2 Acme Township township 4,375 3 Ada Township township Kent 13,142 4 Adams Township, Arenac County township Arenac 563 5 Adams Township, Hillsdale County township Hillsdale 2,493 6 Adams Township, Houghton County township Houghton 2,573 Here the . in .wikitable.sortable represents the class/subclass (use # for ids). In general though, if you know it’s a table, then use html_table. When the table has no class then you’ll have to use a magic number or some other means to grab it, which may not be as robust. Of course, the class itself might change over time also. So let’s put this all together. library(stringr) towns %&gt;% html_table() %&gt;% .[[2]] %&gt;% rename(Population = `2010 Population`) %&gt;% mutate(Population = strtoi(str_replace_all(Population, &#39;,&#39;, &#39;&#39;)), Type = factor(Type)) %&gt;% filter(!Type %in% c(&#39;unincorporated community&#39;, &#39;CDP&#39;)) %&gt;% ggplot(aes(x=Population, fill=Type, color=Type)) + scale_x_log10() + geom_density(alpha=.2) + theme(panel.background=element_rect(fill=&#39;transparent&#39;, color=NA), plot.background=element_rect(fill=&#39;transparent&#39;, color=NA)) I won’t go into details about the rest of the code regarding data processing, as that’s for another workshop. For now it suffices to say that it didn’t take much to go from URL to visualization. Basketball Reference As an additional example let’s get some data from basketball-reference.com. The following gets the totals table from the URL8. Issues include header rows after every 20th row of data, and converting all but a few columns from character to numeric. url = &quot;http://www.basketball-reference.com/leagues/NBA_2016_totals.html&quot; bball = read_html(url) %&gt;% html_node(&quot;#totals_stats&quot;) %&gt;% # grab element with id &#39;total_stats&#39; html_table() %&gt;% # the data filter(Rk != &quot;Rk&quot;) %&gt;% # remove header rows mutate_at(vars(-Player, -Pos, -Tm), as.numeric) # convert to numeric str(bball) &#39;data.frame&#39;: 578 obs. of 30 variables: $ Rk : num 1 2 3 4 5 6 7 8 9 10 ... $ Player: chr &quot;Quincy Acy&quot; &quot;Jordan Adams&quot; &quot;Steven Adams&quot; &quot;Arron Afflalo&quot; ... $ Pos : chr &quot;PF&quot; &quot;SG&quot; &quot;C&quot; &quot;SG&quot; ... $ Age : num 25 21 22 30 27 27 30 20 26 34 ... $ Tm : chr &quot;SAC&quot; &quot;MEM&quot; &quot;OKC&quot; &quot;NYK&quot; ... $ G : num 59 2 80 71 59 60 74 8 79 64 ... $ GS : num 29 0 80 57 17 5 74 0 28 57 ... $ MP : num 876 15 2014 2371 861 ... $ FG : num 119 2 261 354 150 134 536 5 191 215 ... $ FGA : num 214 6 426 799 315 ... $ FG% : num 0.556 0.333 0.613 0.443 0.476 0.596 0.513 0.5 0.516 0.458 ... $ 3P : num 19 0 0 91 0 0 0 0 0 15 ... $ 3PA : num 49 1 0 238 1 0 16 0 0 42 ... $ 3P% : num 0.388 0 NA 0.382 0 NA 0 NA NA 0.357 ... $ 2P : num 100 2 261 263 150 134 536 5 191 200 ... $ 2PA : num 165 5 426 561 314 ... $ 2P% : num 0.606 0.4 0.613 0.469 0.478 0.596 0.521 0.5 0.516 0.468 ... $ eFG% : num 0.6 0.333 0.613 0.5 0.476 0.596 0.513 0.5 0.516 0.474 ... $ FT : num 50 3 114 110 52 60 259 0 46 90 ... $ FTA : num 68 5 196 131 62 84 302 0 73 138 ... $ FT% : num 0.735 0.6 0.582 0.84 0.839 0.714 0.858 NA 0.63 0.652 ... $ ORB : num 65 0 219 23 75 86 176 2 162 104 ... $ DRB : num 123 2 314 243 194 202 456 4 262 192 ... $ TRB : num 188 2 533 266 269 288 632 6 424 296 ... $ AST : num 27 3 62 144 31 50 110 0 76 70 ... $ STL : num 29 3 42 25 19 47 38 1 26 110 ... $ BLK : num 24 0 89 10 36 68 81 2 42 18 ... $ TOV : num 27 2 84 82 54 64 99 1 69 78 ... $ PF : num 103 2 223 142 134 139 151 1 147 175 ... $ PTS : num 307 7 636 909 352 ... Text A lot of times we’ll want to grab text as opposed to tables. See the API chapter for an example, and the html_text function in the rvest package. Images Images are fairly easy because they are simply files with specific extensions like svg, png, gif etc. In that sense, if one knows the actual location of the image already, a function like download.files can be used to grab it directly. Other times it may not be known, and so we can use a similar approach as before to grab the file. base_page = read_html(&#39;https://en.wikipedia.org/wiki/Main_Page&#39;) picofday_location = base_page %&gt;% # get the main page html_nodes(&#39;#mp-tfp&#39;) %&gt;% # extract the div with &#39;today&#39;s featured picture&#39;, i.e. tfp html_nodes(&#39;img&#39;) %&gt;% # extract the img html_attr(&#39;src&#39;) # grab the source location With location in hand we can now download the file, and even display it in R. The following requires the grid and jpeg packages. download.file(url=paste0(&#39;https:&#39;, picofday_location), destfile=&#39;img/picofday.jpg&#39;, mode=&#39;wb&#39;) picofday = jpeg::readJPEG(source=&#39;img/picofday.jpg&#39;) df = data.frame(x=rnorm(1), y=rnorm(1)) # note that any random df will suffice qplot(data=df, geom=&#39;blank&#39;) + annotation_custom(grid::rasterGrob(picofday)) + theme_void() Note that an alternative approach that might work on some websites would be to extract all the images and then the one with a relevant naming convention. This doesn’t work for the Wikipedia main page because there is nothing to identify which image is the featured picture by file name alone. Issues As mentioned, the ease with which you will be able to scrape a website will depend a lot on how well the page/site is put together. Many are cookie-cutter templates designed with no regard to data availability whatsoever, others are just the result of amateurs that do not do web design for a living, and still others are just poorly designed. In addition, some websites may have security or other server back end things to consider that require much of the content to not be made easily available, perhaps intentionally. On the other hand, other sites will make it as easy as a URL pointing to a csv file, or an API that is easily maneuverable. The main thing is that you must plan ahead for it to either not be easy to get, and/or to be ready for heavy post-processing even when you get the bulk of what you want. The goal should be to make the process automatic such that only a fundamental site or API change will cause you to have to change your code again. Called by various names: web harvesting, data scraping (presumptuous), etc.↩ Think about your R code, now remove the spaces, indent as irregularly as possible, do not comment anything, use reserved words for common names, don’t name important parts of your site, reject any known coding style, and use default settings. That would describe about 99% of the typical website design I come across.↩ There is subtle distinction between tags versus elements, but which won’t really matter to us.↩ APIs change so regularly that in some cases it might be easier to scrape as above, especially if the site itself changes relatively rarely and still allows direct access to the desired content.↩ I highly recommend Beautiful Soup also. Until rvest and related packages came along, I preferred using Beautiful Soup to what was available in R.↩ Think graphical models rather than spruce.↩ The openfda site was depicted using the Mozilla firebug extension.↩ Technically this data can be downloaded as a supplied csv.↩ "],
["apis.html", "APIs Raw Example R packages Issues", " APIs An application programming interface allows one to interact with a website. In the simplest situation, this is merely a url-based approach to grabbing what you need. As an example consider the following generic URL: http://somewebsite.com/key?par1;par2 The key ingredients are: http://somewebsite.com/ the base URL key some authorization component ?par1;par2 the parameters of interest that specify what you want to grab from that website. With R, once we have authorization we can then simply feed the parameters that tell the server what data to provide. We can do this in a raw fashion, where we make the URL, or web address9, that has the necessary specification, and then simply take what it provides. Alternatively, there are many R packages to make the process easier for things like Twitter, Qualtrics, and many other websites. Raw Example Basic functions of the raw approach are requests to a server things like GET and POST, commands that tell the server to provide something or perhaps provide data to it. In the following, api_key10 is an R object with a character string of the key provided to me by the website. I use the httr package for the web functionality to acquire the content. In particular, the GET function retrieves whatever information is noted by the request/url. Additional arguments or modifications to the base URL can also be provided, which is what the query part does. # raw approach library(httr) most_viewed_base = GET(&#39;https://api.nytimes.com/svc/mostpopular/v2/mostviewed/all-sections/30.json&#39;, query=list(`api-key`=api_key)) # see the url created # most_viewed_base$url # tacks on ?api-key=YOURAPI at the end At this point, most_viewed_base is a response class object, a list with several pieces of information including what we want, which is the content. str(most_viewed_base[-1], 1) List of 9 $ status_code: int 200 $ headers :List of 16 ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;insensitive&quot; &quot;list&quot; $ all_headers:List of 1 $ cookies :&#39;data.frame&#39;: 0 obs. of 7 variables: $ content : raw [1:54928] 7b 22 73 74 ... $ date : POSIXct[1:1], format: &quot;2016-10-12 19:31:40&quot; $ times : Named num [1:6] 0 0.031 0.078 0.125 0.187 0.218 ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;redirect&quot; &quot;namelookup&quot; &quot;connect&quot; &quot;pretransfer&quot; ... $ request :List of 7 ..- attr(*, &quot;class&quot;)= chr &quot;request&quot; $ handle :Class &#39;curl_handle&#39; &lt;externalptr&gt; However the content’s in binary, and to get it into a useful state we’ll use the content function. The rest of the code just takes the title and arranges it by date. most_viewed = content(most_viewed_base) lapply(most_viewed$results, function(x) data_frame(title=x$title, date=x$published_date)) %&gt;% bind_rows() %&gt;% arrange(desc(date)) # A tibble: 20 × 2 title date &lt;chr&gt; &lt;chr&gt; 1 Latest Election Polls 2016 2016-10-12 2 2016 Election Forecast: Who Will Be President? 2016-10-12 3 Paul Ryan Turns Focus From Donald Trump to House Races, Roiling G.O.P. 2016-10-11 4 Presidential Debate: Heres What You Missed 2016-10-10 5 Fact Checks of the Second Presidential Debate 2016-10-09 6 Donald Trump, Groper in Chief 2016-10-09 7 Donald Trump Apology Caps Day of Outrage Over Lewd Tape 2016-10-08 8 A Letter to the Doctors and Nurses Who Cared for My Wife 2016-10-06 9 Donald Trump Tax Records Show He Could Have Avoided Taxes for Nearly Two Decades, The Times Found 2016-10-02 10 After a Disappointing Debate, Donald Trump Goes on the Attack 2016-09-28 11 Did You Miss the Presidential Debate? Here Are the Highlights 2016-09-27 12 Our Fact Checks of the First Debate 2016-09-27 13 Why Donald Trump Should Not Be President 2016-09-26 14 Video by Wife of Keith Scott Shows Her Pleas to Police 2016-09-24 15 A Week of Whoppers From Donald Trump 2016-09-24 16 Angelina Jolie to Divorce Brad Pitt, Ending Brangelina 2016-09-22 17 Ahmad Khan Rahami Is Arrested in Manhattan and New Jersey Bombings 2016-09-20 18 Powerful Blast Injures at Least 29 in Manhattan; Second Device Found 2016-09-18 19 How the Sugar Industry Shifted Blame to Fat 2016-09-13 20 Hillary Clintons Doctor Says Pneumonia Led to Abrupt Exit From 9/11 Event 2016-09-12 The community API can get user comments and movie reviews. In the following case, it will need a specific date to retrieve comments. comments_base = GET(&#39;http://api.nytimes.com/svc/community/v3/user-content/by-date.json&#39;, query=list(date=Sys.Date())) comments = content(comments_base) sapply(comments$results$comments, function(x) x$commentBody)[1:5] [1] &quot;As ugly as I thought the \\&quot;republicans\\&quot; were or could be, has been dramatically eclipsed, by the vulgarity that is. &lt;br/&gt;&lt;br/&gt;What an embarrassment for our country.&lt;br/&gt;Talk about \\&quot;the ugly American\\&quot;&quot; [2] &quot;The Great Depression gave FDR the conditions to enable Social Security, Worker Rights and Unemployment Insurance to become a reality. If Trump \\&quot;enables\\&quot; the Democrats with filibuster-proof majorities in the Congress, they must treat it as a great chance for long-sought and long-needed projects like single payer national health care and income tax reforms that are truly progressive.&quot; [3] &quot;I heard, many people are saying, that Hillary is sending money to Donald so he can continue his meltdown. It&#39;s what people say...&quot; [4] &quot;Nominee Trump is a demagogue--one to be classed with the likes of Louisiana&#39;s Gov. Huey Long.&lt;br/&gt;&lt;br/&gt;It is ironic that the demagogic Trump has now turned on the demagogic party that spawned him--and that party leaders are waiting for further polls before deciding how to pay him back in kind.&lt;br/&gt;&lt;br/&gt;Perhaps, however, it is time to turn away from our fixation on an individual demagogue and focus on the far graver threat: the institutionalized demagoguery of the GOP itself.&lt;br/&gt;&lt;br/&gt;Demagoguery effectively shuts down rational discourse and reflective consideration of alternative solutions to our many problems.&lt;br/&gt;&lt;br/&gt;Isn&#39;t that what the Grand Obstructionist Party is all about?&lt;br/&gt;&lt;br/&gt;Among the GOP&#39;s demagogic strategies are appeals to divisive \\&quot;social/moral\\&quot; issues, identification with a shallow militaristic \\&quot;patriotism\\&quot; and a&lt;br/&gt;narrow Christian fundamentalism, obstructionist Hastert rules and Norquist pledges, etc.&lt;br/&gt;&lt;br/&gt;While pursuing these strategies, many GOP politicians and spokespersons--not just candidate Donald Trump--employ the demagogue&#39;s favored techniques on a daily basis: gross oversimplification, fear mongering, scapegoating, emotional appeals, accusations that opponents are disloyal or weak, attacks on the news media, obstructive refusals of all compromise--and, yes, bald faced lies.&lt;br/&gt;&lt;br/&gt;Donald Trump&#39;s candidacy is the symptom, not the disease.&lt;br/&gt;&lt;br/&gt;The voters can readily reject this individual buffoonish demagogue.&lt;br/&gt;&lt;br/&gt;But just who will reform this cadaverous Republican Party?&quot; [5] &quot;It looks like both parties&#39; pollsters seem aligned on the direction of the race, though they await a new round of polling to see where to allocate resources. Trump retains the support of his base but is losing soft supporters and independents while Hillary Clinton continues to gain among millennials, white females without college degrees and will likely have the largest margin among latino voters that we&#39;ve ever seen. The gap between the Democratic and Republican nominees among white voters is the lowest it has been since Bill Clinton won re-election in 1996. It stands to reason that Hillary Clinton will have a margin of victory that aligns to the results in 2008 or 1996.&quot; The API documentation doesn’t make obvious what the date format should be, how the comments are chosen (aside from being taken from some section of the paper), or what the limits are. As mentioned previously, this is typical API documentation in my experience. R packages Many R packages allow extremely easy access to various websites through their API. Usually all it takes is acquiring the authorization and the package will do the rest. It might only be marginally less effort than the raw approach we did before, but can make things more efficient in the long run (assuming the package is kept up to date). New York Times Article Search The following is an example of accessing the New York Times article search API11. Using the rtimes package, we can use a function similar to any other in R. In this case we need a query, beginning date and an end date to collect articles that contain the text of the query. Note that it still requires an key, but that’s not shown here. library(rtimes) article_search = as_search(q=&quot;bomb&quot;, begin_date = &#39;20160918&#39;, end_date = &#39;20160919&#39;)[-(1:2)] lapply(article_search$data, function(x) data_frame(snippet=x$snippet, date= x$pub_date)) %&gt;% bind_rows() %&gt;% arrange(desc(date)) # A tibble: 10 × 2 snippet &lt;chr&gt; 1 New York City police investigating a bombing in Manhattan over the weekend said on Monday they wanted to question two me 2 Long before Ahmad Rahami became notorious as the suspect in this weekend&#39;s bombings in and around New York, his family w 3 Syrian or Russian warplanes bombed the city of Aleppo and nearby villages on Monday, the Syrian Observatory for Human Ri 4 Democrat Hillary Clinton on Monday accused Republican Donald Trump of aiding Islamic State recruitment, while Trump said 5 The Latest on explosive devices being found in two states (all times local): 6 Mayor J. Christian Bollwage of Elizabeth, N.J., described how a bag containing pipe bombs was discovered near a train st 7 Amnesty International says it has evidence that a Saudi-led coalition battling Yemeni rebels dropped a U.S.-made bomb on 8 Saturday nights explosion in the Chelsea neighborhood of Manhattan occurred just outside a building that is home to man 9 The authorities used a total containment vessel to haul the device to the Bronx before passing it along to the Federal 10 A Somali general and at least seven of his bodyguards were killed on Sunday when their vehicle was rammed by a car bomb # ... with 1 more variables: date &lt;chr&gt; Wikipedia With the WikipediR package, getting the whole content of a page is just one possibility, and it feeds nicely into rvest functionality for more processing. Here the package’s page_content function will extract information from the page. The ‘text’ element is the html content, which we can then feed to the previously used rvest functions. I show only a snippet of the output. library(WikipediR) r_wikipedia = page_content(language=&#39;en&#39;, &#39;wikipedia&#39;, page_name = &#39;R_(programming_language)&#39;) str(r_wikipedia[[1]], 1) # inspect r_wikipedia$parse$text$`*` %&gt;% read_html %&gt;% html_text %&gt;% message Qualtrics Qualtrics is a survey software which one can use to create and disseminate surveys and the data from them, and is a very useful tool in this regard. The University of Michigan has a license, and so anyone at the university can use it, which is why I provide a generic example here. The qualtricsR12 package makes it easy to import data from and export surveys to Qualtrics, as well as functionality to create a standard survey within R. As before you’ll need proper authorization, but then it can be straightforward to grab your data from Qualtrics without having to go to the website. library(qualtricsR) mydata = importQualtricsData(username = &quot;qualtricsUser@email.address#brand&quot;, # example micl@umich.edu#umich token = &quot;tokenString&quot;, surveyID = &quot;idString&quot;) # your token from Qualtrics Others Note that there are many R packages that work with various APIs, so if you’re looking to work with data from a site that has an API, definitely see if something already exists. If it is a very popular website, you are probably not the first person using R that wants to access its data. Issues Documentation One issue I commonly find with APIs is that often they are generally poorly documented beyond a certain level of initial detail, e.g. often telling you what the parameters are but not the values they can take on. As an example, if something says it wants a date, you may be left to figure out what date format is expected. Sometimes I feel that developers spend so little effort it seems they don’t actually want people to use the API. Just be aware that you may still have some guesswork left even when a lot of information is provided initially. API Changes Many websites cannot leave the API alone very long. Sometimes this is due to actual content changes that require subsequent changes in the API, sometimes it might reflect a server side issue that needs to be addressed. Often a reason isn’t given. The gist is that you shouldn’t be surprised if an API package in R doesn’t work if it hasn’t been updated recently. As the code is often pretty straightforward, you may be able to easily tweak a function to work with the current API. Broken API Sometimes someone fiddles with things they shouldn’t and you start getting errors. As an example, while developing this document, NY times article search would start providing different errors depending on the combination of start and end date (forbidden, API rate exceeded, no content etc.). It is not necessarily the case that you are doing something incorrectly. Restrictions Note that there are almost always restrictions. For example, the New York Times API limits requests to 1000 calls per day, and five calls per second. Unfortunately these are typically not based on actual testing of what modern servers could handle and are often overly restrictive (and overly optimistic in estimates of web traffic) in my opinion. I mention it though because you’ll need to plan ahead. If you need a 1 million requests, but the API restricts you to 1000 a day, you’ll obviously need some other way to get the data you want. A simple request to the website developers for more flexibility might be all that is needed. Technically uniform resource locator.↩ The reason you don’t see the key is that, like passwords, you should not provide your keys to others. They provide a means for you to interact with the website, not other people.↩ There are several APIs for different types of content.↩ https://github.com/saberry/qualtricsR↩ "],
["visualization.html", "Visualization Systems htmlwidgets Shiny Dashboards", " Visualization For standard approaches, R already is one of the more powerful data visualization tools out there, to the point where the look of its graphics is being replicated across other platforms. However, R has come a long way very quickly with regard to interactive and web-based visualization, such that it is as easy to do an interactive, web-ready graph as it is the standard plot. The following provides an overview of tools to be aware of for web-based visualization in R. Systems This section notes a couple packages which I will call visualization systems. For example, for static plots ggplot2 and lattice are systems within which to produce visualizations. They have their own consistent style with which one may specify the details of a plot and are very flexible in what they can produce. In that sense I’ll note two interactive visualization systems, Plotly and Bokeh. plotly Plotly is a general visualization system with APIs for JavaScript, Python, Matlab, and of interest to us, R. It used to require an account, but now is used as any other package within R, and in this case the package is just called plotly. Like ggplot2, you start with a base and add layers via pipes. library(plotly) plot_ly(x=~x, y=~y, color=factor(g), type=&#39;scatter&#39;, mode=&#39;markers&#39;) %&gt;% # add_trace() %&gt;% add_lines(y=y, line=list(shape=&#39;spline&#39;)) %&gt;% lazerhawk::theme_plotly() iris %&gt;% plot_ly(x=~Petal.Length, y=~Petal.Width, color=~Species) By default the plot is interactive with hover-over information, but you can add a lot more too it (my own theme cleans up gridlines, zero reference line, etc.). One of the best things about plotly is it’s ability to make a ggplot graphic interactive. The resulting visual may need further tweaking as there is not a one-to-one mapping from ggplot2 to plotly, but it’s otherwise a very useful tool. ## ggplot(aes(x=Petal.Length, y=Petal.Width, color=Species), data=iris) + ## geom_point() g = ggplot(aes(x=Petal.Length, y=Petal.Width, color=Species), data=iris) + geom_point() ggplotly(g) Unfortunately the documentation for plotly is very poor in my opinion. Every page on the website requires loading images that still take time even on very fast connections, there is so much unused whitespace that it can be difficult to tell which part of the functionality you’re reading about, examples are not consistent from language to language even for things that should not require different demonstrations, there are almost no comments explaining the code, etc. In short it can be frustrating to learn, and often you may have to go to the Python or JavaScript examples to get more detail than the R example provides. And of course, there is always SO. rbokeh Bokeh is a Python interactive visualization library, and rbokeh is an attempt to port it to the R world. It works very similarly to plotly and other packages, and for easy comparison I iterate the iris example from above. The figure function serves to produce the base of the plot, with other elements added as layers (ly_*) via pipes. It doesn’t appear to mesh well with bookdown yet, so I simply show the image you’d see in your RStudio viewer. library(rbokeh) iris %&gt;% figure(width=500, height=500) %&gt;% ly_points(Petal.Length, Petal.Width, data = iris, color = Species, glyph = Species, hover = list(Petal.Length, Petal.Width)) More on rbokeh can be found here. Others I’ll note that ggvis was once considered to be the successor to ggplot2, providing all the same stuff but specifically to add interactivity and web-based visualization. Development has stalled, probably due to the fact that things like Plotly and Bokeh attempt the same thing, but are used in other programming languages as well, and may have more development behind them. I do not recommend spending any time learning ggvis until development picks back up, and even then, plotly and rbokeh will likely be more developed and still have the cross-language advantage for a time. htmlwidgets Many newer visualization packages take advantage of JavaScript for graphical web-based display, as well as the piping approach to add layers of visual information to a plot. htmlwidgets is a package that makes it easy to create JavaScript visualizations, similar to what you see everywhere on the web. The packages using it typically are pipe-oriented (%&gt;%) and specifically produce interactive plots, and include plotly and rbokeh. Leaflet example The following example utilizes the leaflet package. Aside from the icons, you can create this same interactive graph from essentially nothing. As with most graphing approaches in R, we start with a base, which may or may not contain additional layout or other information. From there we add subsequent layers of information/visuals until the final goal is reached. The final product is an interactive map with popups when the icons are clicked. Rlogo = file.path(R.home(&#39;doc&#39;), &#39;html&#39;, &#39;logo.jpg&#39;) points = cbind(lon=c(-83.738233,-83.7582789), lat=c(42.280799, 42.2668577)) # latitude and longitude locations library(leaflet) leaflet() %&gt;% # base plot nothing to see addTiles() %&gt;% # base plot, the whole world setView(mean(points[,1]), mean(points[,2]), zoom = 14) %&gt;% # set location addMarkers(points[,1], points[,2], # add points etc. icon=list(iconUrl=c(&#39;../img/Rlogo.png&#39;, &#39;img/UM.png&#39;), iconSize = c(75, 75)), popup = c(&#39;CSCAR&#39;, &#39;Big House&#39;)) See the htmlwidgets website for more examples and other packages. Shiny Shiny is a framework within which to use R to build interactive web-based applications. Essentially you can create a website13 that allows interactive data creation, analysis and visualization. While you can use it easily enough on your own machine, it will not work for the web unless you have access to a an actual machine that can serve as the Shiny server. shinyapps.io allows you to create 5 apps for free, with the possibility to pay for more. Server You write two parts of code to use Shiny, and neither is in a fashion in which one typically uses R. The server determines how inputs will produce outputs for the application. It actually may have some more or less standard R code, as one extracts parts of data, produces plots etc. However, most of it will be wrapped as an expression within another R function, which itself is wrapped in a main function called shinyServer. The following comes from one of the Shiny tutorials and demonstrates the server setup. Note that you can define the server and UI as separate R files that are then called by the runApp function. library(shiny) # Define server logic required to draw a histogram shinyServer(function(input, output) { # Expression that generates a histogram. The expression is # wrapped in a call to renderPlot to indicate that: # # 1) It is &quot;reactive&quot; and therefore should be automatically # re-executed when inputs change # 2) Its output type is a plot output$distPlot &lt;- renderPlot({ x &lt;- faithful[, 2] # Old Faithful Geyser data bins &lt;- seq(min(x), max(x), length.out = input$bins + 1) # draw the histogram with the specified number of bins hist(x, breaks = bins, col = &#39;darkgray&#39;, border = &#39;white&#39;) }) }) UI The UI stands for user interface and defines how someone will interact with the inputs and the look of the page. This is where you essentially create the webpage where the user can adjust settings or other aspects which might then change an analysis, visualization or whatever. This is also where you will likely find more of the R code used in a fashion unlike your normal usage. And I hope you like parentheses. In the following the shinyUI function, like the shinyServer function before, wraps the rest of the code. Additional functions are used to create sidebars, interactive inputs, titles etc. # Define UI for application that draws a histogram shinyUI(fluidPage( # Application title titlePanel(&quot;Hello Shiny!&quot;), # Sidebar with a slider input for the number of bins sidebarLayout( sidebarPanel( sliderInput(&quot;bins&quot;, &quot;Number of bins:&quot;, min = 1, max = 50, value = 30) ), # Show a plot of the generated distribution mainPanel( plotOutput(&quot;distPlot&quot;) ) ) )) To see the app in action, simply install the shiny package and run the following code: shiny::runExample(&#39;01_hello&#39;) The shiny framework is very powerful in terms of what it can do, but realize you are basically creating a web app using a language that wasn’t designed for general programming in that fashion and will require outside resources if you want to use it. It is very likely that other approaches would be more efficient and notably lighter weight14, resulting in a better user experience, which is what your number one priority should be if you are constructing an interactive app. That said, shiny can be a lot of fun, and useful for presentations too if running it from your own device. See the gallery for demos and more. Dashboards The flex_dashboard output format allows one to quickly create a dashboard type webpage complete with whatever interactivity you may want. In a sense it comes off as a Shiny shortcut, enabling you to create a webpage quickly and easily by letting the default mode do more of the work behind the scenes. Consider the following image. The only R code necessary to start building a dashboard with the layout on the right is not even seen, it’s just part of the underlying markdown that comes with it. Whatever you put in the R chunks will show up in the corresponding chart position. As an example, I’d never used flex_dashboard before and it only took part of an afternoon to get the data and produce the following dashboard that looks at tornados in the U.S. over time (click image to see the app). It’s still demonstrates the slowness that can come with a shiny app, but it was relatively easy to put together. All plots use plotly. A lot of folks these days seem to confuse what a software application is, and what a website is, to the point that many actually call what is akin to a URL shortcut an ‘app’ (e.g. the Netflix app just takes you to netflix.com, i.e. while you may be able to find such a thing to download for your phone, this does not constitute a software application: ). In general, software applications do not require the web, and a web app is basically a website that functions similar to a software application that might run on your desktop or other device. Shiny has many functions that essentially create the html, JavaScript etc. you would otherwise have written. Whether you want to call the result an app is up to you, but using Shiny won’t qualify you to call yourself a software developer. This point is also made because nowadays Shiny is actually unnecessary to create interactive webpages/sites (e.g. this document does not utilize Shiny). If all you want is some basic interactivity, you may be able to do that with the other packages noted.↩ Shiny apps take a while to load and may be slowly implemented.↩ "],
["publishing.html", "Publishing Publishing Languages Document Formats Presentations Other formats Other", " Publishing The publishing capabilities in R are phenomenal, and keep expanding all the time. From traditional document types (including pdf and MS Word) to building whole websites with loads of interactivity, R can generally take you where you want to go. The ability to embed the data in the document is clearly important in academia for the facilitation of ideas and discovery, as well as conducting more reproducible research. However, such capabilities can benefit anyone. Publishing Languages Markdown Markdown serves as the basis for much of the approach. Markdown provides an easy way to create html products without coding any raw html. It is extremely limited in this regard, but for most text it’s all you need. Despite it’s utility, there is no standard for markdown and it hasn’t been developed in years. Thus there are many flavors of markdown which do continue to be developed, of which R Markdown is one. R Markdown R Markdown is Markdown with some other stuff that allows you to work with R. Like shiny is for webpages, it’s a framework for authoring with data science in mind. You no longer ever need your documents and data to be separated15. The basic process is such that you write an R Markdown document and it is then converted into the desired format via other tools, e.g. html or pdf. Use File/New File/R Markdown... in RStudio to get started. The following shows what a markdown file might look like. You see YAML for the first few lines, basic text intermingled with standard markdown (e.g. ** adds bold, # are headers), and R chunks, where R code resides. The R code itself may remain hidden, exposed but not run, or run along with the rest of the output and the results shown in the text. Much of this process is made possible via the knitr package, which takes the .Rmd file and knits it into the desired format, using things like pandoc and other tools to convert what you to see into the final product. You can always change the output format, so it doesn’t really matter what you select at the initial point. It might be obvious, but you’ll need outside programs for some things. For example, you can’t knit a MS Word doc16 if MS Word is not installed on your computer. Likewise, you’ll need a \\(\\LaTeX\\) installation to create a pdf. You should also know that what a document renders to in html will not have the exact same look in pdf or MS Word, nor could it be expected to. If you select an inferior or less flexible format for your publication, don’t expect all the bells and whistles that worked in a better format to come along. HTML, CSS etc. HTML is what is behind most of the web that you actually see and interact with, allowing one to create a webpage in a mostly tabular format of some kind. CSS allows you have a consistent style across a collection of pages usually amalgamated to form an entire website. Javascript allows one to build applications that run within the browser, and a host of other languages process data on the server side. You don’t need to know these languages to produce documents with R, but the more you know of them the more you can enhance and customize your product. LaTeX It used to be the case that \\(\\LaTeX\\) served as the primary means for producing high-quality documents in academia, and with a lot of headache, it can make beautiful work. However, despite some impressions, these days, pdf and similar print-first output is neither necessary nor really should be the chief means of scientific communication, and so the need of \\(\\LaTeX\\) has been greatly minimized. It is still useful for formulas, but even that is translated via Mathjax, which is a JavaScript library that essentially reproduces the math functionality of \\(\\LaTeX\\). For example, inserting the following bit in an R Markdown document $$y = X\\beta + \\epsilon$$ will produce the following when the document is ‘knit’: \\[y = X\\beta + \\epsilon\\] One can also use various latex packages as well if needed. However, with R Markdown, using raw \\(\\LaTeX\\) is rarely needed outside of formal mathematical exposition. There are several packages such as xtable that may be able to create a \\(\\LaTeX\\) table for you, or perhaps there are other means to display things that might be better. Thinking beyond the restriction of page boundaries and printed work will take time to get used to, but is well worth it. YAML YAML (or YAML Aint Markup Language) serves as the means to configure your R Markdown files17. The syntax is very simple, and in the context of an R Markdown document, it allows you to specify things like the output type, title, other css files to use, and so forth. This will start any R Markdown file you use unless you have one main file that calls other Rmd files. Pandoc Pandoc is a universal converter18, allowing markup languages to flop from one format to another. Essentially a babel fish for web, it reads the markdown, HTML, \\(\\LaTeX\\), and everything else and converts it to an HTML, pdf etc. document. From the pandoc website: Markdown, CommonMark, PHP Markdown Extra, GitHub-Flavored Markdown, MultiMarkdown, and (subsets of) Textile, reStructuredText, HTML, LaTeX, MediaWiki markup, TWiki markup, Haddock markup, OPML, Emacs Org mode, DocBook, txt2tags, EPUB, ODT and Word docx; and it can write plain text, Markdown, CommonMark, PHP Markdown Extra, GitHub-Flavored Markdown, MultiMarkdown, reStructuredText, XHTML, HTML5, LaTeX (including beamer slide shows), ConTeXt, RTF, OPML, DocBook, OpenDocument, ODT, Word docx, GNU Texinfo, MediaWiki markup, DokuWiki markup, ZimWiki markup, Haddock markup, EPUB (v2 or v3), FictionBook2, Textile, groff man pages, Emacs Org mode, AsciiDoc, InDesign ICML, TEI Simple, and Slidy, Slideous, DZSlides, reveal.js or S5 HTML slide shows. It can also produce PDF output on systems where LaTeX, ConTeXt, or wkhtmltopdf is installed. Summary As mentioned you don’t have to know these languages to produce really interesting documents. However, being more cognizant of their capabilities means you’ll be able to be more creative with your products. Document Formats HTML The html_document is the default output for R Markdown documents, and really should be your default as well. It provides much more freedom and easier specificity in the way content is presented, and can get quite extensive. Notebooks I’ve been using notebooks since they were first advertised as part of an RStudio preview release. However, it is still not clear to me what their role is19, other than to allow your output to reside in your R markdown document rather than the console, something typically not desirable (at least with how I use R). I guess it’s useful if you want to share the script/document and let someone else play with the code, but technically they could do that with an R Markdown document anyway, or a basic R script for that matter. Having the output below or to the side of the document rather than a few inches from that point is not something I can get overly excited about, and you can do that with the standard html_document format. And sometimes there is a lot of output from a chunk that I definitely do not want clogging up the code or document, but still may want to regularly inspect. Code should have a flow just like text should, at least that’s my opinion. That said, notebooks do appear be highly useful for educational purposes, and more so if they make it such that the notebooks run in Jupyter similar to Python’s *.nb files. I also like Julia’s approach to it’s standard script, where you can simply hover/click for the output rather than have it automatically spat out, and maybe the R notebooks will go that direction. Stay tuned to see how this document format pans out. Print For more print-oriented approaches you have pdf, MS Word, RTF, ODF. As mentioned, you will need to have something else installed (e.g. a \\(\\TeX\\) installation for pdf) in order to use these. Furthermore, you should think very hard about whether you need them, as you will sacrifice your document’s capabilities and look in some fashion. Journals There is some template support for Elsevier journals (assuming you’re not boycotting them) and some specific ones that the R crowd would be interested in such as the Journal of Statistical Software. More may be available via packages or added over time. My impression is that it’s more likely the journals are catering to the output (e.g. some already accept markdown files), such that the templates may not be necessary. Presentations Some of the presentations for my CSCAR workshops use revealjs20 or similar for a web-based approach, though I find slides highly restrictive for conveying information that isn’t visual. For better or worse, at present there are multiple types of presentations one might use in R with varying degrees of functionality. ioslides: HTML presentation with ioslides reveal.js: HTML presentation with reveal.js (requires revealjs package/template) Slidy: HTML presentation with W3C Slidy Beamer: PDF presentation with \\(\\LaTeX\\) Beamer These days I cannot think of a reason to do a set of pdf slides as there is zero benefit and an automatic loss of functionality. The others are more configurable and can work on any device as easily. Given that you can run something like an interactive shiny app as part of a presentation, why would you give that up?21 As for the others, they will all serve you well but seem intermittently developed aside from the revealjs. Be aware that just because you did it with a standard html_document format, does not mean it will work just the same when trying to make a slide form of it22. Other formats bookdown HTML, PDF, ePub, and Kindle books Websites Multi-page websites. Tufte Handout Handouts in the style of Edward Tufte Package Vignette R package vignette (HTML) GitHub Document GitHub Flavored Markdown document. I’ll provide a few words about these formats but may expand later. This document was created with bookdown, so that should give you some idea about its capabilities. The website format is not shiny nor just the standard html doc output, but a format of its own. The Tufte handouts work very well for pdf, and perhaps for standard html versions, but I’ve had issues with bookdown and the Tufte output is not well documented for it. Package vignettes are very useful if you actually create an R package, but can otherwise be ignored. A GitHub doc isn’t much different from standard markdown, so may be of limited use, even assuming you are using GitHub in the first place. Other Customization For customization you’ll need to learn at least a little HTML and CSS, and possibly quite a bit once you go down the rabbit hole, along with other languages such as JavaScript. This goes for all R markdown based document formats. I find it necessary to get things how I want them, but you may be fine with default themes etc. In addition, user created formats with custom looks or functionality are now coming into play, and will provide other formats beyond what you can get from the RStudio crowd. Rpubs With an Rpubs account, you can publish your documents directly to the web for easy dissemination. This allows you to share any of your R creations with ease. For those in academia this is perhaps of limited use, as typically everyone is given their own web space already. However, even then it could potentially serve as an additional outlet. More I’ll add more stuff here as I come across it. Unfortunately, many journals still seem to think it’s 1985 where people mostly access them in print form in the library. These are also journals that aren’t being cited as much anymore. Accessibility and openness are the hallmarks of science, and any journal outlets that have not figured this out should not be allowed to ride the coattails of their past status.↩ I’ve yet to come across any reason to still be using MS Word nowadays. It is easier to create an MS Word document via markdown than it is to use the program itself.↩ There are other things that YAML is that I do not understand (e.g. How ‘Yet Another Markup Language’ now doesn’t want to be considered a markup language).↩ Actually a Haskell library.↩ Maybe I’m short-sighted but notebooks strike me as the phablet of R markdown output formats. As quoted from J.J. Allaire in response from someone who wanted a simple description that states the difference between notebooks and the standard html doc: “They are the same thing, r notebooks are just a way of working interactively with R Markdown and embedding the source code directly within the R Markdown output.”↩ revealjs is an R package that ports the reveal JavaScript library.↩ R.I.P. Beamer.↩ It’s not clear to me why anything shouldn’t work aside from bounding box issues, but you’ll have notable problems with some interactive graphics, while others will work great.↩ "],
["conclusion.html", "Conclusion", " Conclusion As mentioned at the beginning of this document, do check the CRAN Task View for Web Technologies. There’s a lot more out there that allows you to use R with the web, and the capabilities increase all the time. The purpose of this document was to merely give insight into some possibilities for how to use R to engage the web, with simple demonstrations or explanations of those approaches. However, any discussed here can be taken further, and there is a great deal more to explore. This document will be updated, and possibly expanded so check back for more. --> --> "],
["references.html", "References", " References "]
]
