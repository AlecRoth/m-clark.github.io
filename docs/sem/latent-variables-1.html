<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Graphical and Latent Variable Modeling</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Latent variable modeling.">
  <meta name="generator" content="bookdown 0.1.7 and GitBook 2.6.7">

  <meta property="og:title" content="Graphical and Latent Variable Modeling" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Latent variable modeling." />
  <meta name="github-repo" content="m-clark/Workshops" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Graphical and Latent Variable Modeling" />
  
  <meta name="twitter:description" content="Latent variable modeling." />
  


<meta name="date" content="2016-10-26">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="graphical-models-1.html">
<link rel="next" href="structural-equation-modeling.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.7/htmlwidgets.js"></script>
<script src="libs/viz-0.3/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-0.8.4/grViz.js"></script>
<script src="libs/d3-3.5.3/./d3.min.js"></script>
<link href="libs/d3heatmapcore-0.0.0/heatmapcore.css" rel="stylesheet" />
<script src="libs/d3heatmapcore-0.0.0/heatmapcore.js"></script>
<script src="libs/d3-tip-0.6.6/index.js"></script>
<script src="libs/d3heatmap-binding-0.6.1.1/d3heatmap.js"></script>
<link href="libs/vis-4.16.1/vis.min.css" rel="stylesheet" />
<script src="libs/vis-4.16.1/vis.min.js"></script>
<script src="libs/visNetwork-binding-1.0.2/visNetwork.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>
<link href="libs/plotlyjs-1.16.3/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.16.3/plotly-latest.min.js"></script>
<script src="libs/plotly-binding-4.5.2/plotly.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="latent.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://m-clark.github.io/docs/sem/"><span style="font-size:150%; font-variant:small-caps; font-style:italic; color:#ff5503">Latent Variable Models</span></a></li>

<li class="divider"></li>
<li><a href="index.html#section"></a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a><ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#statistical"><i class="fa fa-check"></i>Statistical</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#programming"><i class="fa fa-check"></i>Programming</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#outline"><i class="fa fa-check"></i>Outline</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#graphical-models"><i class="fa fa-check"></i>Graphical Models</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#latent-variables"><i class="fa fa-check"></i>Latent Variables</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#sem"><i class="fa fa-check"></i>SEM</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#others"><i class="fa fa-check"></i>Others</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#programming-language-choice"><i class="fa fa-check"></i>Programming Language Choice</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#setup"><i class="fa fa-check"></i>Setup</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html"><i class="fa fa-check"></i>Introduction to R</a><ul>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#getting-started"><i class="fa fa-check"></i>Getting Started</a><ul>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#installation"><i class="fa fa-check"></i>Installation</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#packages"><i class="fa fa-check"></i>Packages</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#rstudio"><i class="fa fa-check"></i>RStudio</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#key-things-to-know-about-r"><i class="fa fa-check"></i>Key things to know about R</a><ul>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#r-is-a-programming-language-not-a-stats-package"><i class="fa fa-check"></i>R is a programming language, not a ‘stats package’</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#never-ask-if-r-can-do-what-you-want.-it-can."><i class="fa fa-check"></i>Never ask if R can do what you want. It can.</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#main-components-script-console-graphics-device"><i class="fa fa-check"></i>Main components: script, console, graphics device</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#r-is-easy-to-use-but-difficult-to-master."><i class="fa fa-check"></i>R is easy to use, but difficult to master.</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#object-oriented"><i class="fa fa-check"></i>Object-oriented</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#case-sensitive"><i class="fa fa-check"></i>Case sensitive</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#the-lavaan-package"><i class="fa fa-check"></i>The lavaan package</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#getting-help"><i class="fa fa-check"></i>Getting help</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#moving-forward"><i class="fa fa-check"></i>Moving forward</a><ul>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#exercises"><i class="fa fa-check"></i>Exercises</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#summary"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="graphical-models-1.html"><a href="graphical-models-1.html"><i class="fa fa-check"></i>Graphical Models</a><ul>
<li class="chapter" data-level="" data-path="graphical-models-1.html"><a href="graphical-models-1.html#directed-graphs"><i class="fa fa-check"></i>Directed Graphs</a><ul>
<li class="chapter" data-level="" data-path="graphical-models-1.html"><a href="graphical-models-1.html#standard-linear-model"><i class="fa fa-check"></i>Standard linear model</a></li>
<li class="chapter" data-level="" data-path="graphical-models-1.html"><a href="graphical-models-1.html#path-analysis"><i class="fa fa-check"></i>Path Analysis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="graphical-models-1.html"><a href="graphical-models-1.html#bayesian-networks"><i class="fa fa-check"></i>Bayesian Networks</a></li>
<li class="chapter" data-level="" data-path="graphical-models-1.html"><a href="graphical-models-1.html#undirected-graphs"><i class="fa fa-check"></i>Undirected Graphs</a><ul>
<li class="chapter" data-level="" data-path="graphical-models-1.html"><a href="graphical-models-1.html#network-analysis"><i class="fa fa-check"></i>Network analysis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="graphical-models-1.html"><a href="graphical-models-1.html#nonrecursive-models"><i class="fa fa-check"></i>Nonrecursive Models</a></li>
<li class="chapter" data-level="" data-path="graphical-models-1.html"><a href="graphical-models-1.html#summary-1"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="graphical-models-1.html"><a href="graphical-models-1.html#r-packages-used"><i class="fa fa-check"></i>R packages used</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html"><i class="fa fa-check"></i>Latent Variables</a><ul>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html#dimension-reductioncompression"><i class="fa fa-check"></i>Dimension Reduction/Compression</a><ul>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html#principal-components-analysis"><i class="fa fa-check"></i>Principal Components Analysis</a></li>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html#other-matrix-factorization-techniques"><i class="fa fa-check"></i>Other Matrix Factorization Techniques</a></li>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html#factor-analysis"><i class="fa fa-check"></i>Factor Analysis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html#constructs-and-measurement-models"><i class="fa fa-check"></i>Constructs and Measurement models</a><ul>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html#scale-development"><i class="fa fa-check"></i>Scale development</a></li>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html#factor-scores"><i class="fa fa-check"></i>Factor Scores</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html#some-other-uses-of-latent-variables"><i class="fa fa-check"></i>Some Other Uses of Latent Variables</a></li>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html#summary-2"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html#r-packages-used-1"><i class="fa fa-check"></i>R packages used</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html"><i class="fa fa-check"></i>Structural Equation Modeling</a><ul>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#measurement-model"><i class="fa fa-check"></i>Measurement model</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#structural-model"><i class="fa fa-check"></i>Structural model</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#example-1"><i class="fa fa-check"></i>Example</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#issues-in-sem"><i class="fa fa-check"></i>Issues in SEM</a><ul>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#identification"><i class="fa fa-check"></i>Identification</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#fit"><i class="fa fa-check"></i>Fit</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#model-comparison"><i class="fa fa-check"></i>Model Comparison</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#prediction"><i class="fa fa-check"></i>Prediction</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#observed-covariates"><i class="fa fa-check"></i>Observed covariates</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#interactions"><i class="fa fa-check"></i>Interactions</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#estimation"><i class="fa fa-check"></i>Estimation</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#missing-data"><i class="fa fa-check"></i>Missing data</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#other-sem-approaches"><i class="fa fa-check"></i>Other SEM approaches</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#how-to-fool-yourself-with-sem"><i class="fa fa-check"></i>How to fool yourself with SEM</a><ul>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#sample-size"><i class="fa fa-check"></i>Sample size</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#poor-data"><i class="fa fa-check"></i>Poor data</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#naming-a-latent-variable-doesnt-mean-it-exists"><i class="fa fa-check"></i>Naming a latent variable doesn’t mean it exists</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#ignoring-diagnostics"><i class="fa fa-check"></i>Ignoring diagnostics</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#ignoring-performance"><i class="fa fa-check"></i>Ignoring performance</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#summary-3"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#r-packages-used-2"><i class="fa fa-check"></i>R packages used</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html"><i class="fa fa-check"></i>Latent Growth Curves</a><ul>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#random-effects"><i class="fa fa-check"></i>Random effects</a><ul>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#model-formality"><i class="fa fa-check"></i>Model formality</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#random-effects-in-sem"><i class="fa fa-check"></i>Random Effects in SEM</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#simulating-random-effects"><i class="fa fa-check"></i>Simulating Random Effects</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#running-a-growth-curve-model"><i class="fa fa-check"></i>Running a Growth Curve Model</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#thinking-more-generally-about-regression"><i class="fa fa-check"></i>Thinking more generally about regression</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#more-on-lgc"><i class="fa fa-check"></i>More on LGC</a><ul>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#lgc-are-non-standard-sem"><i class="fa fa-check"></i>LGC are non-standard SEM</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#residual-correlations"><i class="fa fa-check"></i>Residual correlations</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#nonlinear-time-effect"><i class="fa fa-check"></i>Nonlinear time effect</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#growth-mixture-models"><i class="fa fa-check"></i>Growth Mixture Models</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#other-covariates"><i class="fa fa-check"></i>Other covariates</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#some-differences-between-mixed-models-and-growth-curves"><i class="fa fa-check"></i>Some Differences between Mixed Models and Growth Curves</a><ul>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#random-slopes"><i class="fa fa-check"></i>Random slopes</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#wide-vs.long"><i class="fa fa-check"></i>Wide vs. long</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#sample-size-1"><i class="fa fa-check"></i>Sample size</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#number-of-time-points"><i class="fa fa-check"></i>Number of time points</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#balance"><i class="fa fa-check"></i>Balance</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#numbering-the-time-points"><i class="fa fa-check"></i>Numbering the time points</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#other-stuff"><i class="fa fa-check"></i>Other stuff</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#summary-4"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="mixture-models.html"><a href="mixture-models.html"><i class="fa fa-check"></i>Mixture Models</a><ul>
<li class="chapter" data-level="" data-path="mixture-models.html"><a href="mixture-models.html#a-motivating-example"><i class="fa fa-check"></i>A Motivating Example</a></li>
<li class="chapter" data-level="" data-path="mixture-models.html"><a href="mixture-models.html#create-clustered-data"><i class="fa fa-check"></i>Create Clustered Data</a></li>
<li class="chapter" data-level="" data-path="mixture-models.html"><a href="mixture-models.html#mixture-modeling-with-old-faithful"><i class="fa fa-check"></i>Mixture modeling with Old Faithful</a></li>
<li class="chapter" data-level="" data-path="mixture-models.html"><a href="mixture-models.html#sem-and-latent-categorical-variables"><i class="fa fa-check"></i>SEM and Latent Categorical Variables</a><ul>
<li class="chapter" data-level="" data-path="mixture-models.html"><a href="mixture-models.html#latent-categories-vs.multi-group-analysis"><i class="fa fa-check"></i>Latent Categories vs. Multi-group Analysis</a></li>
<li class="chapter" data-level="" data-path="mixture-models.html"><a href="mixture-models.html#latent-trajectories"><i class="fa fa-check"></i>Latent Trajectories</a></li>
<li class="chapter" data-level="" data-path="mixture-models.html"><a href="mixture-models.html#estimation-1"><i class="fa fa-check"></i>Estimation</a></li>
<li class="chapter" data-level="" data-path="mixture-models.html"><a href="mixture-models.html#terminology-in-sem"><i class="fa fa-check"></i>Terminology in SEM</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="mixture-models.html"><a href="mixture-models.html#r-packages-used-3"><i class="fa fa-check"></i>R packages used</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="other.html"><a href="other.html"><i class="fa fa-check"></i>Other</a></li>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i>Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#path-analysis-1"><i class="fa fa-check"></i>Path Analysis</a><ul>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#part-1"><i class="fa fa-check"></i>Part 1</a></li>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#part-2"><i class="fa fa-check"></i>Part 2</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#factor-analysis-1"><i class="fa fa-check"></i>Factor Analysis</a><ul>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#part-1-1"><i class="fa fa-check"></i>Part 1</a></li>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#part-2-1"><i class="fa fa-check"></i>Part 2</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#sem-1"><i class="fa fa-check"></i>SEM</a><ul>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#exercise-1"><i class="fa fa-check"></i>Exercise 1</a></li>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#exercise-2"><i class="fa fa-check"></i>Exercise 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#data-set-descriptions"><i class="fa fa-check"></i>Data set descriptions</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#mcclelland"><i class="fa fa-check"></i>McClelland</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#national-longitudinal-survey-of-youth-1997-nlsy97"><i class="fa fa-check"></i>National Longitudinal Survey of Youth (1997, NLSY97)</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#wheaton-1977-data"><i class="fa fa-check"></i>Wheaton 1977 data</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#old-faithful"><i class="fa fa-check"></i>Old Faithful</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#harman-1974"><i class="fa fa-check"></i>Harman 1974</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#terminology-in-sem-1"><i class="fa fa-check"></i>Terminology in SEM</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#parallel-process-example"><i class="fa fa-check"></i>Parallel Process Example</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#causal-bias"><i class="fa fa-check"></i>Causal Bias</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#prediction-1"><i class="fa fa-check"></i>Prediction</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#chance"><i class="fa fa-check"></i>Chance</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#other-1"><i class="fa fa-check"></i>Other</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#resources"><i class="fa fa-check"></i>Resources</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#graphical-models-2"><i class="fa fa-check"></i>Graphical Models</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#potential-outcomes"><i class="fa fa-check"></i>Potential Outcomes</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#measurement-models"><i class="fa fa-check"></i>Measurement Models</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#sem-2"><i class="fa fa-check"></i>SEM</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#lavaan"><i class="fa fa-check"></i>lavaan</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#other-sem"><i class="fa fa-check"></i>Other SEM</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://m-clark.github.io" target="blank" style="font-size:150%; font-variant:small-caps; color:#ff5500">Michael Clark</a></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank" style="font-size:75%;">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><span style="font-size:150%; font-variant:small-caps; font-style:italic; color:#1e90ff">Graphical and Latent Variable Modeling</span></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="latent-variables-1" class="section level1">
<h1>Latent Variables</h1>
<p>Not everything we want to measure comes with an obvious yardstick. If one wants to measure something like a person’s happiness, what would they have at their disposal?</p>
<ul>
<li>Are they smiling?</li>
<li>Did they just get a pay raise?</li>
<li>Are they interacting well with others?</li>
<li>Are they relatively healthy?</li>
</ul>
<p>Any of these might be useful as an <em>indicator</em> of their current state of happiness, but of course none of them would tell us whether they truly are happy or not. At best, they can be considered imperfect measures. If we consider those and other indicators collectively, perhaps we can get an underlying measure of something we might call happiness, contentment, or some other arbitrary but descriptive name.</p>
<p>Despite how they are typically used within psychology, education and related fields, the use of <span class="emph">latent variable</span> models are actually seen all over, and in ways that may have little to do with what we will be mostly focusing on here<a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a>. Broadly speaking, <span class="emph">factor analysis</span> can be seen as a dimension reduction technique, or as an approach to modeling measurement error and understanding underlying constructs. We will give some description of the former while focusing on the latter.</p>
<div id="dimension-reductioncompression" class="section level2">
<h2>Dimension Reduction/Compression</h2>
<p>Many times we simply have the goal of taking a whole lot of variables, reducing them to much fewer, but while retaining as much information about the originals as possible. For example, this is an extremely common goal in areas of image and audio compression. Statistical techniques amenable to these approaches are commonly referred to as <span class="emph">matrix factorization</span>.</p>
<div id="principal-components-analysis" class="section level3">
<h3>Principal Components Analysis</h3>
<p>Probably the most commonly used factor-analytic technique is <span class="emph">principal components analysis</span> (PCA). It seeks to extract <em>components</em> from a set of variables, with each component containing as much of the original variance as possible. Components can be seen as a linear combination of the original variables.</p>
<p>PCA works on a covariance/correlation matrix, and it will return as many components as there are variables that go into it, each subsequent component accounting for less variance than the previous component, and summing up to 100% of the total variance in the original data. With appropriate steps, the components can completely reproduce the original correlation matrix. However, as the goal is dimension reduction, we only want to retain some of these components, and so the reproduced matrix will not be exact. This however gives us some sense of a goal to shoot for, and the same idea is also employed in factor analysis/SEM, where we also work with the covariance matrix and prefer models that can closely reproduce the original correlations seen with the observed data.</p>
<p>Visually, we can display PCA as a graphical model. Here is one with four components/variables. The size of the components represents the amount of variance each accounts for.</p>
<div style="text-align:center">
<div style="width:50%; margin-left:auto;  margin-right:auto">
<div id="htmlwidget-8c582bcd4f721d850b73" style="width:250px;height:250px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-8c582bcd4f721d850b73">{"x":{"diagram":"\n  digraph DAG {\n    # Intialization of graph attributes\n    graph [overlap = false]\n  \n    # Initialization of node attributes\n    node [shape = circle,\n          fontname = Helvetica,\n          color = gray80,\n          type = box,\n          fixedsize = true, penwidth=2, fontcolor=gray50]\n  \n  \n    # Node statements\n    PC3 [width=.75, color=gray70];\n    PC4 [width=.5, color=gray70]; \n    PC1 [width=1.5, color=salmon2, fontcolor=salmon2]; \n    PC2 [width=1.25, color=gray70];    # reordered to deal with unusual ordering otherwise\n    node [width=1, shape=square, color=gray10]\n    I3; I4; I1; I2; \n  \n    # Initialization of edge attributes\n    edge [color = gray50, rel = yields]\n  \n    # Edge statements\n    edge [arrowhead=none, penwidth=2]\n    PC1 -> I1[color=salmon]; PC1 -> I2[color=salmon]; PC1 -> I3[color=salmon]; PC1 -> I4[color=salmon];\n    PC2 -> I1; PC2 -> I2; PC2 -> I3; PC2 -> I4;\n    PC3 -> I1; PC3 -> I2; PC3 -> I3; PC3 -> I4;\n    PC4 -> I1; PC4 -> I2; PC4 -> I3; PC4 -> I4;\n  \n    }\n  ","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
<p>Let’s see an example. The following regards a correlation matrix<a href="#fn16" class="footnoteRef" id="fnref16"><sup>16</sup></a> of 24 psychological tests given to 145 seventh and eight-grade children in a Chicago suburb by Holzinger and Swineford. We’ll use the <span class="pack">psych</span> package, and to use the <span class="func">principal</span> function, we provide the data (available via the psych package), specify the number of components/factors we want to retain, and other options (in this case, the <em>rotated</em> solution will be a little more interpretable<a href="#fn17" class="footnoteRef" id="fnref17"><sup>17</sup></a>). We will use the <span class="pack">psych</span> package here as it gives us a little more output than standard PCA packages and functions, and one that is more consistent with the factor analysis technique we’ll spend time with later. While we will use lavaan for factor analysis to be consistent with the SEM approach, the psych package is a great tool for standard factor analysis, assessing reliability and other fun stuff.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(psych)
pc =<span class="st"> </span><span class="kw">principal</span>(Harman74.cor$cov, <span class="dt">nfactors=</span><span class="dv">4</span>,  <span class="dt">rotate=</span><span class="st">&#39;varimax&#39;</span>)
pc</code></pre></div>
<pre><code>Principal Components Analysis
Call: principal(r = Harman74.cor$cov, nfactors = 4, rotate = &quot;varimax&quot;)
Standardized loadings (pattern matrix) based upon correlation matrix
                         RC1   RC3   RC2  RC4   h2   u2 com
VisualPerception        0.16  0.71  0.23 0.14 0.60 0.40 1.4
Cubes                   0.09  0.59  0.08 0.03 0.37 0.63 1.1
PaperFormBoard          0.14  0.66 -0.04 0.11 0.47 0.53 1.2
Flags                   0.25  0.62  0.09 0.03 0.45 0.55 1.4
GeneralInformation      0.79  0.15  0.22 0.11 0.70 0.30 1.3
PargraphComprehension   0.81  0.18  0.07 0.21 0.73 0.27 1.2
SentenceCompletion      0.85  0.15  0.15 0.06 0.77 0.23 1.1
WordClassification      0.64  0.31  0.24 0.11 0.57 0.43 1.8
WordMeaning             0.84  0.16  0.06 0.19 0.78 0.22 1.2
Addition                0.18 -0.13  0.83 0.12 0.76 0.24 1.2
Code                    0.18  0.05  0.63 0.37 0.57 0.43 1.8
CountingDots            0.02  0.17  0.80 0.05 0.67 0.33 1.1
StraightCurvedCapitals  0.18  0.41  0.62 0.03 0.59 0.41 1.9
WordRecognition         0.23 -0.01  0.06 0.68 0.52 0.48 1.2
NumberRecognition       0.12  0.08  0.05 0.67 0.48 0.52 1.1
FigureRecognition       0.06  0.46  0.05 0.58 0.55 0.45 1.9
ObjectNumber            0.14  0.01  0.24 0.68 0.54 0.46 1.4
NumberFigure           -0.02  0.32  0.40 0.50 0.51 0.49 2.7
FigureWord              0.14  0.25  0.20 0.42 0.30 0.70 2.4
Deduction               0.43  0.43  0.09 0.30 0.47 0.53 2.8
NumericalPuzzles        0.18  0.42  0.50 0.17 0.49 0.51 2.5
ProblemReasoning        0.42  0.41  0.13 0.29 0.45 0.55 3.0
SeriesCompletion        0.42  0.52  0.25 0.20 0.55 0.45 2.7
ArithmeticProblems      0.40  0.14  0.55 0.26 0.55 0.45 2.5

                       RC1  RC3  RC2  RC4
SS loadings           4.16 3.31 3.22 2.74
Proportion Var        0.17 0.14 0.13 0.11
Cumulative Var        0.17 0.31 0.45 0.56
Proportion Explained  0.31 0.25 0.24 0.20
Cumulative Proportion 0.31 0.56 0.80 1.00

Mean item complexity =  1.7
Test of the hypothesis that 4 components are sufficient.

The root mean square of the residuals (RMSR) is  0.06 

Fit based upon off diagonal values = 0.97</code></pre>
<p>First focus on the last portion of the output where it says <code>SS loadings</code> . The first line is the sum of the squared loadings for each component (in this case where we are using a correlation matrix, summing across all 24 components would equal the value of 24). The <code>Proportion Var</code> tells us how much of the overall variance the component accounts for out of all the variables (e.g. 4.16 / 24 = 0.17). The <code>Cumulative Var</code> tells us that all 4 components make up over half the variance. The others are the same thing just based on the four components rather than all 24 variables. We can see that each component accounts for a decreasing amount of variance.</p>
<p><span class="emph">Loadings</span> in this scenario represent the estimated correlation of an item with its component, and provide the key way in which we interpret the factors. However, we have a lot of them, and rather than interpret that mess in our output, we’ll look at it visually. In the following plot, stronger loadings are indicated by blue, and we can see the different variables associated with different components.</p>
<p>Interpretation is the fun but commonly difficult part. As an example, we can see PC2 as indicative of mathematical ability, but in general this isn’t the cleanest result.</p>
<div style="width:500px; margin-left:auto;  margin-right:auto">
<div id="htmlwidget-fcbe638409efbfd8d032" style="width:500px;height:500px;" class="d3heatmap html-widget"></div>
<script type="application/json" data-for="htmlwidget-fcbe638409efbfd8d032">{"x":{"rows":null,"cols":null,"matrix":{"data":["0.157","0.713","0.226","0.142","0.087","0.593","0.081","0.029","0.143","0.662","-0.042","0.108","0.248","0.617","0.089","0.026","0.786","0.153","0.218","0.112","0.806","0.178","0.069","0.206","0.85","0.15","0.15","0.056","0.64","0.311","0.239","0.106","0.842","0.165","0.056","0.194","0.179","-0.132","0.833","0.123","0.184","0.046","0.628","0.367","0.018","0.167","0.797","0.053","0.184","0.413","0.624","0.034","0.233","-0.011","0.058","0.68","0.116","0.079","0.048","0.674","0.061","0.457","0.048","0.582","0.144","0.01","0.242","0.676","-0.016","0.32","0.399","0.495","0.136","0.251","0.197","0.421","0.432","0.434","0.089","0.296","0.18","0.42","0.503","0.17","0.416","0.413","0.13","0.291","0.416","0.525","0.246","0.203","0.401","0.144","0.549","0.256"],"dim":[24,4],"rows":["VisualPerception","Cubes","PaperFormBoard","Flags","GeneralInformation","PargraphComprehension","SentenceCompletion","WordClassification","WordMeaning","Addition","Code","CountingDots","StraightCurvedCapitals","WordRecognition","NumberRecognition","FigureRecognition","ObjectNumber","NumberFigure","FigureWord","Deduction","NumericalPuzzles","ProblemReasoning","SeriesCompletion","ArithmeticProblems"],"cols":["RC1","RC3","RC2","RC4"]},"image":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABgAAAAECAYAAACUY/8YAAABOElEQVQYlWWPPUvDUBiFX4cMxS5Ft4JDRItDoZNbJ0Fa+gsylbpkcJFmFtwE9xLoVDpIltChg1l0sSWBIoZC4ArCzRC4XGKw134MLscpWTzb4TzPcMhjEgede9g+h8ckgjhDvT9FEGeIhILHJKrGEHK1QRBnsH0O3XSLvdy04DEJ2+cI4gxaowcnTBDEGZwwAWmNHjwm4TGJSCjopgtrsvwnRELBCRM4YYKf7Q65l2+66aI7XhRcuWkhEgrkhAlagxl000UkFGyfg6drREKhagyLV3mPhEJrMEMkFOr9KTwmwdM1bJ+jagzB0zWsyRLd8QLNhxfseUyCf++oUtJoNI8pz13njJ4/v+j4cJ9G85i221+6vjwtmJuLE3pLVvT6kVImN3TVrlGlpNHTUpBxfkS3j+901a7RHzFEAekPyh0zAAAAAElFTkSuQmCC","theme":null,"options":{"xaxis_height":80,"yaxis_width":120,"xaxis_font_size":null,"yaxis_font_size":null,"brush_color":"#0000FF","show_grid":true,"anim_duration":500}},"evals":[],"jsHooks":[]}</script>
</div>
<p>Some explanation of the other parts of the output:</p>
<ul>
<li><code>h2</code>: the amount of variance in the item explained by the (retained) components. It is the sum of the squared loadings (a.k.a. <span class="emph">communality</span>).</li>
<li><code>u2</code>: 1 - h2</li>
<li><code>com</code>: A measure of complexity. A value of 1 might be seen for something that loaded on only one component, and zero otherwise (a.k.a. perfect simple structure)</li>
</ul>
<p>We can get a quick graphical model display as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## fa.diagram(pc, cex=.5)</code></pre></div>
<div style="width:500px; height:500px; margin-left:auto;  margin-right:auto">
<div id="htmlwidget-1bcc8173ace94b215764" style="width:500px;height:500px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-1bcc8173ace94b215764">{"x":{"diagram":"digraph Factor  {\n  rankdir=RL;\n  size=\"8,6\";\n  node [fontname=\"Helvetica\" fontsize=10, shape=box, width=1.75, fixedsize=true];\n  edge [fontname=\"Helvetica\" fontsize=12];\nV1  [label = \"VisualPerception\"];\nV2  [label = \"Cubes\"];\nV3  [label = \"PaperFormBoard\"];\nV4  [label = \"Flags\"];\nV5  [label = \"GeneralInformation\"];\nV6  [label = \"PargraphComprehension\"];\nV7  [label = \"SentenceCompletion\"];\nV8  [label = \"WordClassification\"];\nV9  [label = \"WordMeaning\"];\nV10  [label = \"Addition\"];\nV11  [label = \"Code\"];\nV12  [label = \"CountingDots\"];\nV13  [label = \"StraightCurvedCapitals\"];\nV14  [label = \"WordRecognition\"];\nV15  [label = \"NumberRecognition\"];\nV16  [label = \"FigureRecognition\"];\nV17  [label = \"ObjectNumber\"];\nV18  [label = \"NumberFigure\"];\nV19  [label = \"FigureWord\"];\nV20  [label = \"Deduction\"];\nV21  [label = \"NumericalPuzzles\"];\nV22  [label = \"ProblemReasoning\"];\nV23  [label = \"SeriesCompletion\"];\nV24  [label = \"ArithmeticProblems\"];\nnode [shape=ellipse, width =\"1\"];\nRC1-> V5 [ label = 0.8 ];\nRC1-> V6 [ label = 0.8 ];\nRC1-> V7 [ label = 0.8 ];\nRC1-> V8 [ label = 0.6 ];\nRC1-> V9 [ label = 0.8 ];\nRC1-> V22 [ label = 0.4 ];\nRC3-> V1 [ label = 0.7 ];\nRC3-> V2 [ label = 0.6 ];\nRC3-> V3 [ label = 0.7 ];\nRC3-> V4 [ label = 0.6 ];\nRC3-> V20 [ label = 0.4 ];\nRC3-> V23 [ label = 0.5 ];\nRC2-> V10 [ label = 0.8 ];\nRC2-> V11 [ label = 0.6 ];\nRC2-> V12 [ label = 0.8 ];\nRC2-> V13 [ label = 0.6 ];\nRC2-> V21 [ label = 0.5 ];\nRC2-> V24 [ label = 0.5 ];\nRC4-> V14 [ label = 0.7 ];\nRC4-> V15 [ label = 0.7 ];\nRC4-> V16 [ label = 0.6 ];\nRC4-> V17 [ label = 0.7 ];\nRC4-> V18 [ label = 0.5 ];\nRC4-> V19 [ label = 0.4 ];\n{ rank=same;\nV1;V2;V3;V4;V5;V6;V7;V8;V9;V10;V11;V12;V13;V14;V15;V16;V17;V18;V19;V20;V21;V22;V23;V24;}{ rank=same;\nRC1;RC3;RC2;RC4;}}<div style=\"width:500px; height:500px; margin-left:auto;  margin-right:auto\">\n  <div id=\"htmlwidget-53743b74594c61941126\" style=\"width:500px;height:500px;\" class=\"grViz html-widget\">\u003c/div>\n  <script type=\"application/json\" data-for=\"htmlwidget-53743b74594c61941126\">{\"x\":{\"diagram\":\"digraph Factor  {\\n  rankdir=RL;\\n  size=\\\"8,6\\\";\\n  node [fontname=\\\"Helvetica\\\" fontsize=14 shape=box, width=2];\\n  edge [fontname=\\\"Helvetica\\\" fontsize=10];\\nV1  [label = \\\"VisualPerception\\\"];\\nV2  [label = \\\"Cubes\\\"];\\nV3  [label = \\\"PaperFormBoard\\\"];\\nV4  [label = \\\"Flags\\\"];\\nV5  [label = \\\"GeneralInformation\\\"];\\nV6  [label = \\\"PargraphComprehension\\\"];\\nV7  [label = \\\"SentenceCompletion\\\"];\\nV8  [label = \\\"WordClassification\\\"];\\nV9  [label = \\\"WordMeaning\\\"];\\nV10  [label = \\\"Addition\\\"];\\nV11  [label = \\\"Code\\\"];\\nV12  [label = \\\"CountingDots\\\"];\\nV13  [label = \\\"StraightCurvedCapitals\\\"];\\nV14  [label = \\\"WordRecognition\\\"];\\nV15  [label = \\\"NumberRecognition\\\"];\\nV16  [label = \\\"FigureRecognition\\\"];\\nV17  [label = \\\"ObjectNumber\\\"];\\nV18  [label = \\\"NumberFigure\\\"];\\nV19  [label = \\\"FigureWord\\\"];\\nV20  [label = \\\"Deduction\\\"];\\nV21  [label = \\\"NumericalPuzzles\\\"];\\nV22  [label = \\\"ProblemReasoning\\\"];\\nV23  [label = \\\"SeriesCompletion\\\"];\\nV24  [label = \\\"ArithmeticProblems\\\"];\\nnode [shape=ellipse, width =\\\"1\\\"];\\nRC1-> V5 [ label = 0.8 ];\\nRC1-> V6 [ label = 0.8 ];\\nRC1-> V7 [ label = 0.8 ];\\nRC1-> V8 [ label = 0.6 ];\\nRC1-> V9 [ label = 0.8 ];\\nRC1-> V22 [ label = 0.4 ];\\nRC3-> V1 [ label = 0.7 ];\\nRC3-> V2 [ label = 0.6 ];\\nRC3-> V3 [ label = 0.7 ];\\nRC3-> V4 [ label = 0.6 ];\\nRC3-> V20 [ label = 0.4 ];\\nRC3-> V23 [ label = 0.5 ];\\nRC2-> V10 [ label = 0.8 ];\\nRC2-> V11 [ label = 0.6 ];\\nRC2-> V12 [ label = 0.8 ];\\nRC2-> V13 [ label = 0.6 ];\\nRC2-> V21 [ label = 0.5 ];\\nRC2-> V24 [ label = 0.5 ];\\nRC4-> V14 [ label = 0.7 ];\\nRC4-> V15 [ label = 0.7 ];\\nRC4-> V16 [ label = 0.6 ];\\nRC4-> V17 [ label = 0.7 ];\\nRC4-> V18 [ label = 0.5 ];\\nRC4-> V19 [ label = 0.4 ];\\n{ rank=same;\\nV1;V2;V3;V4;V5;V6;V7;V8;V9;V10;V11;V12;V13;V14;V15;V16;V17;V18;V19;V20;V21;V22;V23;V24;}{ rank=same;\\nRC1;RC3;RC2;RC4;}}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\u003c/script>\n\u003c/div>\nlavaan (0.5-22) converged normally after  19 iterations\n\n                                                  Used       Total\n  Number of observations                          7183        8985\n\n  Estimator                                         ML\n  Minimum Function Test Statistic                0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Information                                 Expected\n  Standard Errors                             Standard\n\nLatent Variables:\n                   Estimate\n  depressed =~\n    FeltDown          1.000\n    BeenHappy        -0.732\n    DeprssdLstMnth    0.719\n  Std.Err  z-value  P(>|z|)\n\n\n    0.020  -37.329    0.000\n    0.019   37.992    0.000\n   Std.lv  Std.all\n\n    0.541    0.813\n   -0.396   -0.609\n    0.388    0.655\n\nVariances:\n                   Estimate\n   .FeltDown          0.150\n   .BeenHappy         0.266\n   .DeprssdLstMnth    0.201\n    depressed         0.292\n  Std.Err  z-value  P(>|z|)\n    0.007   20.853    0.000\n    0.006   46.489    0.000\n    0.005   41.606    0.000\n    0.010   30.221    0.000\n   Std.lv  Std.all\n    0.150    0.339\n    0.266    0.629\n    0.201    0.571\n    1.000    1.000\n\n   vars    n  mean   sd median trimmed  mad   min  max range  skew kurtosis   se\nX1    1 1000  0.05 1.44   0.05    0.05 1.42 -5.13 4.51  9.63  0.00     0.01 0.05\nX2    2 1000  0.00 1.08  -0.01    0.00 1.04 -3.34 3.25  6.59  0.00    -0.06 0.03\nX3    3 1000 -0.01 1.04   0.00   -0.01 1.01 -4.40 3.56  7.96 -0.07     0.27 0.03\nX4    4 1000  0.00 1.14  -0.03   -0.01 1.13 -3.85 3.98  7.83  0.10     0.16 0.04\nX5    5 1000  0.04 1.43   0.10    0.05 1.39 -4.43 5.21  9.63 -0.02     0.07 0.05\nX6    6 1000 -0.02 1.22  -0.01   -0.02 1.27 -3.35 4.68  8.03  0.04    -0.10 0.04\nX7    7 1000  0.02 1.06   0.01    0.01 1.05 -2.94 3.33  6.28  0.11    -0.09 0.03\nX8    8 1000  0.00 1.14  -0.01    0.01 1.12 -3.27 3.47  6.73 -0.05     0.07 0.04\n      [,1]   [,2]  [,3]  [,4]   [,5]  [,6]  [,7]  [,8]\n[1,] 1.000  0.291 0.233 0.352  0.104 0.078 0.049 0.050\n[2,] 0.291  1.000 0.145 0.228 -0.008 0.031 0.054 0.072\n[3,] 0.233  0.145 1.000 0.171  0.006 0.066 0.026 0.079\n[4,] 0.352  0.228 0.171 1.000  0.014 0.057 0.054 0.079\n[5,] 0.104 -0.008 0.006 0.014  1.000 0.417 0.259 0.300\n[6,] 0.078  0.031 0.066 0.057  0.417 1.000 0.201 0.273\n[7,] 0.049  0.054 0.026 0.054  0.259 0.201 1.000 0.142\n[8,] 0.050  0.072 0.079 0.079  0.300 0.273 0.142 1.000\n$raw\n$raw$loadings\n     f1    f2 Variances\n1 1.000 0.000     1.097\n2 0.465 0.000     0.955\n3 0.353 0.000     0.951\n4 0.588 0.000     0.948\n5 0.000 1.000     1.064\n6 0.000 0.744     0.942\n7 0.000 0.381     0.976\n8 0.000 0.507     1.038\n\n$raw$cov.fact\n      [,1]  [,2]\n[1,] 0.983 0.170\n[2,] 0.170 0.973\n\n\n$standardized\n$standardized$loadings\n     X1    X2 Variances   Rsq\n1 0.687 0.000     0.528 0.472\n2 0.426 0.000     0.818 0.182\n3 0.338 0.000     0.886 0.114\n4 0.514 0.000     0.736 0.264\n5 0.000 0.691     0.522 0.478\n6 0.000 0.603     0.636 0.364\n7 0.000 0.356     0.874 0.126\n8 0.000 0.441     0.806 0.194\n\n$standardized$cor.fact\n      [,1]  [,2]\n[1,] 1.000 0.174\n[2,] 0.174 1.000\n\n\n$fit\n         ll      AIC      BIC\n1 -12330.93 24711.86 24834.56\n\nlavaan (0.5-22) converged normally after  30 iterations\n\n  Number of observations                          1000\n\n  Number of missing patterns                         1\n\n  Estimator                                         ML\n  Minimum Function Test Statistic               25.437\n  Degrees of freedom                                19\n  P-value (Chi-square)                           0.147\n\nModel test baseline model:\n\n  Minimum Function Test Statistic              746.093\n  Degrees of freedom                                28\n  P-value                                        0.000\n\nUser model versus baseline model:\n\n  Comparative Fit Index (CFI)                    0.991\n  Tucker-Lewis Index (TLI)                       0.987\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -12330.931\n  Loglikelihood unrestricted model (H1)     -12318.212\n\n  Number of free parameters                         25\n  Akaike (AIC)                               24711.862\n  Bayesian (BIC)                             24834.555\n  Sample-size adjusted Bayesian (BIC)        24755.154\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.018\n  90 Percent Confidence Interval          0.000  0.035\n  P-value RMSEA <= 0.05                          1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.019\n\nParameter Estimates:\n\n  Information                                 Observed\n  Standard Errors                             Standard\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  F1 =~\n    X1                1.000\n    X2                0.465    0.058    7.955    0.000\n    X3                0.353    0.050    7.041    0.000\n    X4                0.588    0.070    8.387    0.000\n  F2 =~\n    X5                1.000\n    X6                0.744    0.074   10.075    0.000\n    X7                0.381    0.047    8.160    0.000\n    X8                0.507    0.055    9.144    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  F1 ~~\n    F2                0.170    0.050    3.367    0.001\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .X1                0.054    0.046    1.173    0.241\n   .X2               -0.004    0.034   -0.104    0.917\n   .X3               -0.012    0.033   -0.372    0.710\n   .X4                0.004    0.036    0.113    0.910\n   .X5                0.044    0.045    0.964    0.335\n   .X6               -0.019    0.038   -0.504    0.614\n   .X7                0.019    0.033    0.555    0.579\n   .X8                0.002    0.036    0.042    0.967\n    F1                0.000\n    F2                0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .X1                1.097    0.120    9.119    0.000\n   .X2                0.955    0.051   18.876    0.000\n   .X3                0.951    0.046   20.471    0.000\n   .X4                0.948    0.058   16.263    0.000\n   .X5                1.064    0.102   10.404    0.000\n   .X6                0.942    0.066   14.236    0.000\n   .X7                0.976    0.047   20.607    0.000\n   .X8                1.038    0.054   19.281    0.000\n    F1                0.983    0.135    7.264    0.000\n    F2                0.973    0.119    8.156    0.000\n\nlavaan (0.5-22) converged normally after  30 iterations\n\n  Number of observations                          1000\n\n  Number of missing patterns                         1\n\n  Estimator                                         ML\n  Minimum Function Test Statistic               25.437\n  Degrees of freedom                                19\n  P-value (Chi-square)                           0.147\n\nModel test baseline model:\n\n  Minimum Function Test Statistic              746.093\n  Degrees of freedom                                28\n  P-value                                        0.000\n\nUser model versus baseline model:\n\n  Comparative Fit Index (CFI)                    0.991\n  Tucker-Lewis Index (TLI)                       0.987\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -12330.931\n  Loglikelihood unrestricted model (H1)     -12318.212\n\n  Number of free parameters                         25\n  Akaike (AIC)                               24711.862\n  Bayesian (BIC)                             24834.555\n  Sample-size adjusted Bayesian (BIC)        24755.154\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.018\n  90 Percent Confidence Interval          0.000  0.035\n  P-value RMSEA <= 0.05                          1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.019\n\nParameter Estimates:\n\n  Information                                 Observed\n  Standard Errors                             Standard\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  F1 =~\n    X1                1.000\n    X2                0.465    0.058    7.955    0.000\n    X3                0.353    0.050    7.041    0.000\n    X4                0.588    0.070    8.387    0.000\n  F2 =~\n    X5                1.000\n    X6                0.744    0.074   10.075    0.000\n    X7                0.381    0.047    8.160    0.000\n    X8                0.507    0.055    9.144    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  F1 ~~\n    F2                0.170    0.050    3.367    0.001\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .X1                0.054    0.046    1.173    0.241\n   .X2               -0.004    0.034   -0.104    0.917\n   .X3               -0.012    0.033   -0.372    0.710\n   .X4                0.004    0.036    0.113    0.910\n   .X5                0.044    0.045    0.964    0.335\n   .X6               -0.019    0.038   -0.504    0.614\n   .X7                0.019    0.033    0.555    0.579\n   .X8                0.002    0.036    0.042    0.967\n    F1                0.000\n    F2                0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .X1                1.097    0.120    9.119    0.000\n   .X2                0.955    0.051   18.876    0.000\n   .X3                0.951    0.046   20.471    0.000\n   .X4                0.948    0.058   16.263    0.000\n   .X5                1.064    0.102   10.404    0.000\n   .X6                0.942    0.066   14.236    0.000\n   .X7                0.976    0.047   20.607    0.000\n   .X8                1.038    0.054   19.281    0.000\n    F1                0.983    0.135    7.264    0.000\n    F2                0.973    0.119    8.156    0.000\n\nlavaan (0.5-22) converged normally after  30 iterations\n\n  Number of observations                          1000\n\n  Number of missing patterns                         1\n\n  Estimator                                         ML\n  Minimum Function Test Statistic               25.437\n  Degrees of freedom                                19\n  P-value (Chi-square)                           0.147","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
</div>
<p>PCA is probably not the best choice in this scenario, nor likely, is a 4 factor solution. One of the primary reasons is that this graphical model assumes the observed variables are measured without error. In addition, the principal components do not correlate with one another, but it seems likely that we would want to allow the latent variables to do so (a different rotation would allow this). However, if our goal is merely to reduce the 24 items to a few that account for the most variance, this would be a standard technique.</p>
</div>
<div id="other-matrix-factorization-techniques" class="section level3">
<h3>Other Matrix Factorization Techniques</h3>
<p>Before leaving PCA for factor analysis of the sort we’ll mostly be concerned with, I’ll mention other matrix factorization techniques that might be of use depending on your data situation.</p>
<ul>
<li><strong>SVD</strong>: singular value decomposition. Works on a raw data matrix rather than covariance matrix, and is still a very viable technique that may perform better in a lot of situations relative to fancier latent variable models, and other more recently developed techniques. Variations on SVD are behind some recommender systems of the sort you come across at Amazon, Netflix etc.</li>
<li><strong>ICA</strong>: Independent components analysis. Extracts non-normal, independent components. The primary goal is to create independent latent variables.</li>
<li><strong>Generalized PCA</strong>: PCA techniques for different types of data, e.g. binary data situations.</li>
<li><strong>PC Regression</strong>: combining PCA with regression in one model.</li>
<li><strong>NMF</strong>: non-negative matrix factorization. Applied to positive valued matrices, produces positive valued factors. Useful, for example, when dealing with counts.</li>
<li><strong>LSI</strong>: Latent Semantic Indexing, an early form of topic modeling.</li>
<li>Many others.</li>
</ul>
</div>
<div id="factor-analysis" class="section level3">
<h3>Factor Analysis</h3>
<p><span class="emph">Factor analysis</span> is a general technique for uncovering latent variables within data. While initially one might think it similar to PCA<a href="#fn18" class="footnoteRef" id="fnref18"><sup>18</sup></a>, one difference from PCA is that the goal is not to recover maximum variance with each factor. In addition, we will move beyond factor analysis as a dimension reduction technique (and fully ‘exploratory’ technique, see below), and instead present it as an approach with a potentially strong theoretical underpinning, and one that can help us assess measurement error, ultimately even leading to regression models utilizing the latent variables themselves.</p>
<p>So let us turn to what are typically called <span class="emph">measurement models</span> within SEM. The underlying model can be thought of as a case in which the observed variables, in some disciplines referred to as <em>indicators</em> (or <em>manifest</em> variables) of the latent construct , are caused by the latent variable. The degree to which the observed variables correlate with one another depends in part on how much of the underlying (presumed) latent variable they reliably measure<a href="#fn19" class="footnoteRef" id="fnref19"><sup>19</sup></a>.</p>
<p>For each indicator we can think of a regression model as follows, where <span class="math inline">\(\beta_0\)</span> is the intercept and <span class="math inline">\(\lambda\)</span> the regression coefficient that expresses the effect of the latent variable <span class="math inline">\(F\)</span> on the observed variable <span class="math inline">\(X\)</span> (I do not note the error).</p>
<p><span class="math display">\[X = \beta_0 + \lambda F\]</span></p>
<p>We will almost always have multiple indicators, and often multiple latent variables. Some indicators may be associated with multiple factors.</p>
<p><span class="math display">\[\begin{aligned}
X_1 &amp;= \beta_{01} + \lambda_{11} F_1 + \lambda_{21} F_2 \\
X_2 &amp;= \beta_{02} + \lambda_{12} F_1 + \lambda_{22} F_2 \\
X_3 &amp;= \beta_{03} + \lambda_{13} F_1
\end{aligned}\]</span></p>
<p>It is important to understand this regression model, because many who engage in factor analysis seemingly do not, and often think of it the other way around, where the observed variables cause the latent. In factor analysis, the <span class="math inline">\(\lambda\)</span> coefficients are called <span class="emph">loadings</span> (as they were in PCA), but are interpreted as any other regression coefficient- a one unit change in the latent variable results in a <span class="math inline">\(\lambda\)</span> change in the observed variable. Most factor models assume that, controlling for the latent variable, the observed variables are independent (recall our previous discussion on conditional independence in graphical models), though this is sometimes relaxed. If only one factor is associated with an item and does not correlate with any other factors, then we have a simple regression setting where the standardized coefficient is equal to the correlation between the latent variable and the observed.</p>
<div id="exploratory-vs.confirmatory" class="section level4">
<h4>Exploratory vs. Confirmatory</h4>
<p>An unfortunate and unhelpful distinction in some disciplines (esp. psychology) is that of <span class="emph">exploratory</span> vs. <span class="emph">confirmatory</span> factor analysis (and even exploratory SEM). In any regression analysis, there is a non-zero correlation between <em>any</em> variable and some target variable. We don’t include everything for theoretical (and even practical) reasons, which is akin to fixing its coefficient to zero, and here it is no different. Furthermore, most modeling endeavors could be considered exploratory, regardless of how the model is specified. As such, this distinction doesn’t tell us anything about the model, and is thus unnecessary in my opinion.</p>
<p>As an example, in the above equations <span class="math inline">\(X_3\)</span> is not modeled by <span class="math inline">\(F_2\)</span>, which is the same as fixing the <span class="math inline">\(\lambda_{23}\)</span> coefficient for <span class="math inline">\(F2\)</span> to 0. However, that doesn’t tell me whether the model is exploratory or not, and yet that is all the distinction refers to, namely, whether we let all factors load with all indicators or not. An analysis doesn’t suddenly have more theoretical weight, validity, causal efficacy, etc. due to the paths specified.</p>
</div>
<div id="example" class="section level4">
<h4>Example</h4>
<p>Let’s see a factor analysis in action. The motivating example for this section comes from the National Longitudinal Survey of Youth (1997, NLSY97), which investigates the transition from youth to adulthood. For this example, we will investigate a series of questions asked to the participants in 2006 pertaining to the government’s role in promoting well-being. Questions regarded the government’s responsibility for the following: providing jobs for everyone, keeping prices under control, providing health care, providing for elderly, helping industry, providing for unemployed, reducing income differences, providing college financial aid, providing decent housing, protecting the environment. Each item has four values 1:4, which range from ‘definitely should be’ to ‘definitely should not be’<a href="#fn20" class="footnoteRef" id="fnref20"><sup>20</sup></a>. We’ll save this for the exercise.</p>
<p>There are also three items regarding their emotional well-being (depression)- how often the person felt down or blue, how often they’ve been a happy person, and how often they’ve been depressed in the last month. These are also four point scales and range from ‘all of the time’ to ‘none of the time’. We’ll use this here.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">depressed =<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&#39;data/nlsy97_depressedNumeric.csv&#39;</span>)

<span class="kw">library</span>(lavaan)
modelCode =<span class="st"> &quot;</span>
<span class="st">  depressed =~ FeltDown + BeenHappy + DepressedLastMonth</span>
<span class="st">&quot;</span>
famod =<span class="st"> </span><span class="kw">cfa</span>(modelCode, <span class="dt">data=</span>depressed)
<span class="kw">summary</span>(famod, <span class="dt">standardized=</span>T)</code></pre></div>
<pre><code>lavaan (0.5-22) converged normally after  19 iterations

                                                  Used       Total
  Number of observations                          7183        8985

  Estimator                                         ML
  Minimum Function Test Statistic                0.000
  Degrees of freedom                                 0

Parameter Estimates:

  Information                                 Expected
  Standard Errors                             Standard

Latent Variables:
                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all
  depressed =~                                                          
    FeltDown          1.000                               0.541    0.813
    BeenHappy        -0.732    0.020  -37.329    0.000   -0.396   -0.609
    DeprssdLstMnth    0.719    0.019   37.992    0.000    0.388    0.655

Variances:
                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all
   .FeltDown          0.150    0.007   20.853    0.000    0.150    0.339
   .BeenHappy         0.266    0.006   46.489    0.000    0.266    0.629
   .DeprssdLstMnth    0.201    0.005   41.606    0.000    0.201    0.571
    depressed         0.292    0.010   30.221    0.000    1.000    1.000</code></pre>
<div id="raw-results" class="section level5">
<h5>Raw results</h5>
<p>In a standard measurement model such as this we must <em>scale the factor</em> by fixing one of the indicator’s loadings to one. This is done for identification purposes, so that we can estimate the latent variable variance. Which variable is selected for scaling is arbitrary, but doing so means that the sum of the latent variable variance and the residual variance of the variable whose loading is fixed to one equals the variance of that observed variable<a href="#fn21" class="footnoteRef" id="fnref21"><sup>21</sup></a>.</p>
<p><img src="latent_files/figure-html/plotFARaw-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(depressed$FeltDown, <span class="dt">na.rm=</span>T)  <span class="co"># .29 + .15</span></code></pre></div>
<pre><code>[1] 0.441856</code></pre>
</div>
<div id="standardized-latent-variable" class="section level5">
<h5>Standardized latent variable</h5>
<p>An alternative way to scale the latent variable is to simply fix its variance to one (the <code>std.lv=TRUE</code> results). It does not need to be estimated, allowing us to obtain loadings for each observed variable. Again, think of the SLiM setting. The loadings would be standardized coefficients where the latent construct is the standardized covariate predicting the item of interest.</p>
</div>
<div id="standardized-latent-and-observed" class="section level5">
<h5>Standardized latent and observed</h5>
<p>With both standardized (using the <span class="func">summary</span> function, set <code>standardized=T</code>), these loadings represent correlations between the observed and latent variables. This is the default output in the factor analysis we’d get from non-SEM software (i.e. ‘exploratory’ FA). If one is just doing a factor-analytic model, these loadings are typically reported. Standardized coefficients in a CFA are computed by taking the unstandardized coefficient (loading) and multiplying it by the model implied standard deviation of the indicator then dividing by the latent variable’s standard deviation. Otherwise, one can simply use standardized variables in the analysis, or supply only the correlation matrix.</p>
<p><img src="latent_files/figure-html/plotFAStd-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>If you’d like to peel back the curtain and see maximum likelihood estimation based on the raw (covariance) and standardized (correlation) scales, with a comparison to lavaan output, <a href="https://github.com/mclark--/Miscellaneous-R-Code/blob/master/ModelFitting/cfa_ml.R">click here</a>.</p>
</div>
</div>
</div>
</div>
<div id="constructs-and-measurement-models" class="section level2">
<h2>Constructs and Measurement models</h2>
<div id="scale-development" class="section level3">
<h3>Scale development</h3>
<p>A good use of factor analysis regards scale development. If we come up with 10 items that reflect some underlying construct, factor analysis can provide a sense of how well the scale is put together. Recall that in path analysis, residual variance, sometimes called disturbance<a href="#fn22" class="footnoteRef" id="fnref22"><sup>22</sup></a>, reflects both omitted causes as well as measurement error. In this context, <span class="math inline">\(1-R^2_{item}\)</span> provides a sense of how unreliable the item is. A perfectly reliable item would be perfectly explained by the construct it is a measure of. Strong loadings indicate a strong relationship between the item and the underlying construct.</p>
<p>The following diagram shows how the variance breaks down (from Kline).</p>
<p><img src="img/uniquevcommon.png" style="display:block; margin: 0 auto; width:50%"></p>
</div>
<div id="factor-scores" class="section level3">
<h3>Factor Scores</h3>
<p>In factor analysis, we can obtain estimated factor scores for each observation, possibly to be used in additional analyses or examined in their own right. One common way is to simply use the loadings as one would regression weights/coefficients (actually scaled versions of them), and create a ‘predicted’ factor score as the linear combination of the indicator variables, just as we would in regression.</p>
<div id="vs.meanssums" class="section level4">
<h4>vs. Means/Sums</h4>
<p>In many occasions, people reduce the number of variables in a model by using a mean or sum score. These actually can be seen to reflect an underlying factor analysis where all loadings are fixed to be equal and the residual variance of the observed variables is fixed to zero, i.e. perfect measurement. If you really think the items reflect a particular construct, you’d probably be better off using a score that comes from a model that doesn’t assume perfect measurement.</p>
</div>
<div id="vs.composites" class="section level4">
<h4>vs. Composites</h4>
<p><span class="emph">Composites</span> scores are what we’d have if we turned the arrows around, and allowed different weights for the different variables, which may not be similar too similar in nature or necessarily correlated (e.g. think of how one might construct a measure of socioeconomic status). Unlike a simple mean, these would have different weights associated with the items. PCA is one way one could create such a composite. Sometimes people just make up weights to use based on what they think they <em>should</em> be (especially in the sporting world). This is silly in my opinion, as I can’t think of any reasonable justification for such an approach over the many available.</p>
<p><img src="img/composite.png" style="display:block; margin: 0 auto;"></img></p>
</div>
</div>
</div>
<div id="some-other-uses-of-latent-variables" class="section level2">
<h2>Some Other Uses of Latent Variables</h2>
<ul>
<li><p><strong>EM algorithm</strong>: A very common technique to estimate model parameters for a variety of model situations, it incorporates a latent variable approach where parameters of interest are treated as a latent variable (e.g. probability of belonging to some cluster).</p></li>
<li><p><strong>Item Response Theory</strong>: uses latent variables, especially in test situations (though is much broader), to assess things like item discrimination, student ability etc.</p></li>
<li><p><strong>Hidden Markov Model</strong>: A latent variable model approach commonly used for time series.</p></li>
<li><p><strong>Topic Model</strong>: In the analysis of text, one can discover latent ‘topics’ based on the frequency of words.</p></li>
<li><p><strong>Collaborative Filtering</strong>: For example, in recommender systems for movies or music, the latent variable might represent genre or demographic subsets.</p></li>
</ul>
</div>
<div id="summary-2" class="section level2">
<h2>Summary</h2>
<p>Latent variable approaches are a necessary tool to have in your statistical toolbox. Whether your goal is to compress data or explore underlying theoretically motivated constructs, ‘factor-analysis’ will serve you well.</p>
</div>
<div id="r-packages-used-1" class="section level2">
<h2>R packages used</h2>
<ul>
<li>psych</li>
<li>lavaan</li>
</ul>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="15">
<li id="fn15"><p>Some even use use a factor analytic approach to estimating correlations among parameters in models (e.g. I’ve seen this with gaussian processes and multinomial regression).<a href="latent-variables-1.html#fnref15">↩</a></p></li>
<li id="fn16"><p>Principal components, standard factor analysis and SEM can work on covariance/correlation matrices even without the raw data, this will be perhaps demonstrated in a later version of this doc.<a href="latent-variables-1.html#fnref16">↩</a></p></li>
<li id="fn17"><p>I don’t think it necessary to get into rotation here, though will likely add a bit in the future. If you’re doing PCA, you’re likely not really concerned about interpretation of loadings, as you are going to use the components for other means. It might help with standard factor analysis, but this workshop will spend time on more focused approaches where one would have some idea of the underlying structure rather than looking to uncover the structure. Rotation doesn’t change anything about the fundamental model result, so one just uses whatever leads to the clearest interpretation.<a href="latent-variables-1.html#fnref17">↩</a></p></li>
<li id="fn18"><p>One version of factor analysis is nearly identical to PCA in terms of mechanics, save for what are on the diagonals of the correlation matrix (1s vs. ‘communalities’).<a href="latent-variables-1.html#fnref18">↩</a></p></li>
<li id="fn19"><p>There are actually deep philosophical underpinnings to this approach, going at least as far back as the notion of the Platonic forms, and continuing on through philosophical debates about what mental aspects can be measured scientifically. However, even when it became a more quantitative discipline, the philosophy was not far behind. See, for example, <a href="https://archive.org/details/vectorsofmindmul010122mbp">The Vectors of Mind</a> by L.L. Thurstone, one of the pioneers of measurement theory (1935). As a philosophy major from back in the day, latent variable modeling has always had great appeal to me.<a href="latent-variables-1.html#fnref19">↩</a></p></li>
<li id="fn20"><p>For your own sake, if you develop a questionnaire, make higher numeric values correspond to meaning ‘more of’ something, rather than in this backward fashion.<a href="latent-variables-1.html#fnref20">↩</a></p></li>
<li id="fn21"><p>Note that this is actually done for all disturbance/residual terms, as there is an underlying latent variable there which represents measurement error and the effect of unexplained causes. The path of that latent variable is fixed to 1, and its variance is the residual variance in the SEM output.<a href="latent-variables-1.html#fnref21">↩</a></p></li>
<li id="fn22"><p>Kline distinguishes between the residuals in standard regression and disturbance in SEM (p. 131 4th ed.), but the distinction there appears to conflate the estimated variance as a parameter/construct and the actual residuals (<span class="math inline">\(y - \hat{y}\)</span>) you’d get after model estimation. A standard regression as typically estimated is no different than the same model in the graphical modeling context. Calling path analysis a causal model makes it no more causal than any other regression model, and the remaining variance is the effect of many things not in the model, and they are causal, regardless of estimation technique. I think we care more deeply about it in the SEM context, and perhaps that necessitates another name, and anything would be better than ‘error’.<a href="latent-variables-1.html#fnref22">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="graphical-models-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="structural-equation-modeling.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": false,
"twitter": false,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": false,
"download": null,
"toc": {
"collapse": "subsection"
},
"highlight": "pygments",
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
